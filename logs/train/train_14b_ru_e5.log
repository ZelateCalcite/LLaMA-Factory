[2025-01-12 21:58:45,258] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|2025-01-12 21:58:46] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:27733
W0112 21:58:47.190000 788965 site-packages/torch/distributed/run.py:793] 
W0112 21:58:47.190000 788965 site-packages/torch/distributed/run.py:793] *****************************************
W0112 21:58:47.190000 788965 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0112 21:58:47.190000 788965 site-packages/torch/distributed/run.py:793] *****************************************
[2025-01-12 21:58:48,658] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-12 21:58:48,659] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[WARNING|2025-01-12 21:58:49] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.
[WARNING|2025-01-12 21:58:49] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|2025-01-12 21:58:49] llamafactory.hparams.parser:359 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:677] 2025-01-12 21:58:49,465 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 21:58:49,465 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,466 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,466 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,466 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,466 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,466 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,466 >> loading file tokenizer_config.json
[INFO|2025-01-12 21:58:49] llamafactory.hparams.parser:359 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2475] 2025-01-12 21:58:49,601 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:677] 2025-01-12 21:58:49,601 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 21:58:49,602 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,602 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,602 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,602 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,602 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,602 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:58:49,602 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-12 21:58:49,736 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-01-12 21:58:49] llamafactory.data.template:157 >> Add <|im_end|> to stop words.
[INFO|2025-01-12 21:58:49] llamafactory.data.loader:157 >> Loading dataset entity_trans_ru.json...
[rank1]:[W112 21:58:49.316246416 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 796 examples [00:00, 41492.67 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/796 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 796/796 [00:00<00:00, 5770.05 examples/s]
[rank0]:[W112 21:59:14.644543318 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/796 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 50/796 [00:00<00:04, 149.47 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 200/796 [00:00<00:01, 524.39 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 300/796 [00:00<00:00, 574.78 examples/s]Running tokenizer on dataset (num_proc=16):  57%|█████▋    | 450/796 [00:00<00:00, 678.95 examples/s]Running tokenizer on dataset (num_proc=16):  75%|███████▌  | 600/796 [00:00<00:00, 870.72 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 747/796 [00:01<00:00, 938.02 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 796/796 [00:01<00:00, 702.77 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2610, 525, 264, 9990, 8522, 12, 22574, 14468, 7341, 11, 4486, 1492, 752, 14683, 1045, 6364, 22870, 1119, 8522, 13, 7036, 429, 279, 4396, 1102, 1265, 387, 264, 17133, 429, 7952, 304, 279, 11652, 624, 40, 686, 2968, 498, 458, 6364, 17133, 323, 264, 8522, 11652, 11, 1380, 279, 6364, 17133, 374, 279, 14468, 1102, 304, 279, 8522, 11652, 13, 358, 1366, 311, 1477, 279, 7112, 8522, 17133, 315, 279, 6364, 14468, 1102, 624, 5501, 1744, 3019, 553, 3019, 13, 151645, 198, 151644, 77091, 198, 39814, 11, 358, 646, 7789, 448, 429, 13, 5209, 3410, 279, 6364, 17133, 323, 279, 8522, 11652, 432, 7952, 304, 773, 358, 646, 10542, 279, 12159, 8522, 17133, 369, 498, 382, 5501, 3561, 279, 1946, 438, 11017, 1447, 22574, 17133, 25, 508, 4208, 6364, 17133, 921, 47707, 11652, 25, 508, 4208, 8522, 11652, 2533, 40, 3278, 1221, 3410, 279, 7112, 8522, 17133, 429, 33210, 311, 279, 6364, 14468, 1102, 13, 151645, 198, 151644, 872, 198, 22574, 17133, 25, 96315, 355, 5242, 2694, 24010, 198, 47707, 11652, 25, 22933, 36673, 35397, 142047, 2247, 39131, 140406, 134817, 124805, 4235, 12141, 5259, 1959, 143946, 126421, 142047, 2247, 39131, 13073, 220, 17, 15, 17, 19, 2293, 17, 15, 17, 24, 139146, 13, 151645, 198, 151644, 77091, 198, 785, 2661, 6364, 17133, 96315, 355, 5242, 2694, 24010, 646, 387, 24531, 1119, 8522, 438, 22933, 36673, 35397, 142047, 2247, 39131, 140406, 134817, 124805, 4235, 12141, 5259, 11, 323, 279, 7112, 8522, 17133, 304, 279, 2661, 11652, 1265, 387, 22933, 36673, 35397, 142047, 2247, 39131, 140406, 134817, 124805, 4235, 12141, 5259, 13, 151645, 198, 151644, 872, 198, 3973, 279, 1102, 323, 1281, 2704, 1817, 3409, 315, 279, 1102, 7952, 304, 279, 2661, 8522, 11652, 13, 151645, 198, 151644, 77091, 198, 39814, 0, 358, 614, 10067, 279, 1102, 22933, 36673, 35397, 142047, 2247, 39131, 140406, 134817, 124805, 4235, 12141, 5259, 323, 432, 2167, 7952, 304, 279, 2661, 11652, 13, 151645, 198, 151644, 872, 198, 35127, 752, 279, 1590, 1102, 2041, 1008, 4244, 13, 151645, 198, 151644, 77091, 198, 16206, 36673, 35397, 142047, 2247, 39131, 140406, 134817, 124805, 4235, 12141, 5259, 151645]
inputs:
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
You are a senior Russian-English translation master, please help me translate some English sentences into Russian. Note that the correct result should be a phrase that appears in the sentence.
I will give you an English phrase and a Russian sentence, where the English phrase is the translation result in the Russian sentence. I want to find the raw Russian phrase of the English translation result.
Please think step by step.<|im_end|>
<|im_start|>assistant
Sure, I can assist with that. Please provide the English phrase and the Russian sentence it appears in so I can identify the corresponding Russian phrase for you.

Please format the input as follows:

English phrase: [insert English phrase]
Russian sentence: [insert Russian sentence]

I'll then provide the raw Russian phrase that corresponds to the English translation result.<|im_end|>
<|im_start|>user
English phrase: Nikolaus van der Pas
Russian sentence: Вторая комиссия фон дер Ляйен — европейская комиссия на 2024—2029 годы.<|im_end|>
<|im_start|>assistant
The given English phrase Nikolaus van der Pas can be translated into Russian as Вторая комиссия фон дер Ляйен, and the raw Russian phrase in the given sentence should be Вторая комиссия фон дер Ляйен.<|im_end|>
<|im_start|>user
Check the result and make sure each word of the result appears in the given Russian sentence.<|im_end|>
<|im_start|>assistant
Sure! I have checked the result Вторая комиссия фон дер Ляйен and it really appears in the given sentence.<|im_end|>
<|im_start|>user
Give me the final result without other words.<|im_end|>
<|im_start|>assistant
Вторая комиссия фон дер Ляйен<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 39814, 11, 358, 646, 7789, 448, 429, 13, 5209, 3410, 279, 6364, 17133, 323, 279, 8522, 11652, 432, 7952, 304, 773, 358, 646, 10542, 279, 12159, 8522, 17133, 369, 498, 382, 5501, 3561, 279, 1946, 438, 11017, 1447, 22574, 17133, 25, 508, 4208, 6364, 17133, 921, 47707, 11652, 25, 508, 4208, 8522, 11652, 2533, 40, 3278, 1221, 3410, 279, 7112, 8522, 17133, 429, 33210, 311, 279, 6364, 14468, 1102, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 2661, 6364, 17133, 96315, 355, 5242, 2694, 24010, 646, 387, 24531, 1119, 8522, 438, 22933, 36673, 35397, 142047, 2247, 39131, 140406, 134817, 124805, 4235, 12141, 5259, 11, 323, 279, 7112, 8522, 17133, 304, 279, 2661, 11652, 1265, 387, 22933, 36673, 35397, 142047, 2247, 39131, 140406, 134817, 124805, 4235, 12141, 5259, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 39814, 0, 358, 614, 10067, 279, 1102, 22933, 36673, 35397, 142047, 2247, 39131, 140406, 134817, 124805, 4235, 12141, 5259, 323, 432, 2167, 7952, 304, 279, 2661, 11652, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 16206, 36673, 35397, 142047, 2247, 39131, 140406, 134817, 124805, 4235, 12141, 5259, 151645]
labels:
Sure, I can assist with that. Please provide the English phrase and the Russian sentence it appears in so I can identify the corresponding Russian phrase for you.

Please format the input as follows:

English phrase: [insert English phrase]
Russian sentence: [insert Russian sentence]

I'll then provide the raw Russian phrase that corresponds to the English translation result.<|im_end|>The given English phrase Nikolaus van der Pas can be translated into Russian as Вторая комиссия фон дер Ляйен, and the raw Russian phrase in the given sentence should be Вторая комиссия фон дер Ляйен.<|im_end|>Sure! I have checked the result Вторая комиссия фон дер Ляйен and it really appears in the given sentence.<|im_end|>Вторая комиссия фон дер Ляйен<|im_end|>
[INFO|configuration_utils.py:677] 2025-01-12 21:59:38,327 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 21:59:38,327 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|2025-01-12 21:59:38] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.
[INFO|modeling_utils.py:3934] 2025-01-12 21:59:38,373 >> loading weights file /mnt/sda/zzh/Qwen2.5-14B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1670] 2025-01-12 21:59:38,373 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1096] 2025-01-12 21:59:38,374 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:04,  1.72it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:03,  1.63it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:03,  1.64it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:02<00:02,  1.67it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:02<00:01,  1.69it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:03<00:01,  1.71it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:04<00:00,  1.70it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  2.02it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.80it/s]
[INFO|modeling_utils.py:4800] 2025-01-12 21:59:42,936 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4808] 2025-01-12 21:59:42,936 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /mnt/sda/zzh/Qwen2.5-14B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1049] 2025-01-12 21:59:42,938 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/generation_config.json
[INFO|configuration_utils.py:1096] 2025-01-12 21:59:42,938 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|2025-01-12 21:59:43] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
[INFO|2025-01-12 21:59:43] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.
[INFO|2025-01-12 21:59:43] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.
[INFO|2025-01-12 21:59:43] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA
[INFO|2025-01-12 21:59:43] llamafactory.model.model_utils.misc:157 >> Found linear modules: q_proj,up_proj,down_proj,o_proj,k_proj,v_proj,gate_proj
[INFO|2025-01-12 21:59:43] llamafactory.model.loader:157 >> trainable params: 34,406,400 || all params: 14,804,440,064 || trainable%: 0.2324
[INFO|trainer.py:698] 2025-01-12 21:59:43,262 >> Using auto half precision backend
Loading checkpoint shards:  12%|█▎        | 1/8 [00:04<00:34,  4.97s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:10<00:30,  5.05s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:15<00:25,  5.07s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:20<00:20,  5.07s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:25<00:15,  5.07s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:30<00:10,  5.07s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:35<00:05,  5.08s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:37<00:00,  4.16s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:37<00:00,  4.71s/it]
[INFO|trainer.py:2313] 2025-01-12 22:00:19,179 >> ***** Running training *****
[INFO|trainer.py:2314] 2025-01-12 22:00:19,179 >>   Num examples = 716
[INFO|trainer.py:2315] 2025-01-12 22:00:19,179 >>   Num Epochs = 5
[INFO|trainer.py:2316] 2025-01-12 22:00:19,180 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2319] 2025-01-12 22:00:19,180 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2320] 2025-01-12 22:00:19,180 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2321] 2025-01-12 22:00:19,180 >>   Total optimization steps = 220
[INFO|trainer.py:2322] 2025-01-12 22:00:19,182 >>   Number of trainable parameters = 34,406,400
  0%|          | 0/220 [00:00<?, ?it/s]  0%|          | 1/220 [00:04<14:43,  4.03s/it]  1%|          | 2/220 [00:07<13:50,  3.81s/it]  1%|▏         | 3/220 [00:11<13:26,  3.72s/it]  2%|▏         | 4/220 [00:14<13:12,  3.67s/it]  2%|▏         | 5/220 [00:18<12:57,  3.62s/it]  3%|▎         | 6/220 [00:22<12:55,  3.62s/it]  3%|▎         | 7/220 [00:25<12:54,  3.64s/it]  4%|▎         | 8/220 [00:29<12:57,  3.67s/it]  4%|▍         | 9/220 [00:33<12:47,  3.64s/it]  5%|▍         | 10/220 [00:36<12:40,  3.62s/it]  5%|▌         | 11/220 [00:40<12:37,  3.63s/it]  5%|▌         | 12/220 [00:43<12:35,  3.63s/it]  6%|▌         | 13/220 [00:47<12:36,  3.65s/it]  6%|▋         | 14/220 [00:51<12:37,  3.68s/it]  7%|▋         | 15/220 [00:54<12:27,  3.65s/it]  7%|▋         | 16/220 [00:58<12:27,  3.67s/it]  8%|▊         | 17/220 [01:02<12:21,  3.65s/it]  8%|▊         | 18/220 [01:05<12:15,  3.64s/it]  9%|▊         | 19/220 [01:09<12:13,  3.65s/it]  9%|▉         | 20/220 [01:13<12:15,  3.68s/it] 10%|▉         | 21/220 [01:16<12:12,  3.68s/it] 10%|█         | 22/220 [01:20<12:07,  3.67s/it] 10%|█         | 23/220 [01:24<12:06,  3.69s/it] 11%|█         | 24/220 [01:28<12:03,  3.69s/it] 11%|█▏        | 25/220 [01:31<11:56,  3.67s/it] 12%|█▏        | 26/220 [01:35<11:51,  3.67s/it] 12%|█▏        | 27/220 [01:39<11:54,  3.70s/it] 13%|█▎        | 28/220 [01:42<11:43,  3.66s/it] 13%|█▎        | 29/220 [01:46<11:38,  3.66s/it] 14%|█▎        | 30/220 [01:50<11:41,  3.69s/it] 14%|█▍        | 31/220 [01:53<11:38,  3.70s/it] 15%|█▍        | 32/220 [01:57<11:36,  3.71s/it] 15%|█▌        | 33/220 [02:01<11:31,  3.70s/it] 15%|█▌        | 34/220 [02:04<11:23,  3.67s/it] 16%|█▌        | 35/220 [02:08<11:17,  3.66s/it] 16%|█▋        | 36/220 [02:12<11:14,  3.66s/it] 17%|█▋        | 37/220 [02:15<11:10,  3.66s/it] 17%|█▋        | 38/220 [02:19<11:09,  3.68s/it] 18%|█▊        | 39/220 [02:23<11:05,  3.68s/it] 18%|█▊        | 40/220 [02:27<11:13,  3.74s/it] 19%|█▊        | 41/220 [02:30<11:04,  3.71s/it] 19%|█▉        | 42/220 [02:34<10:57,  3.70s/it] 20%|█▉        | 43/220 [02:37<10:52,  3.68s/it] 20%|██        | 44/220 [02:41<10:48,  3.68s/it][INFO|trainer.py:4117] 2025-01-12 22:03:12,116 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:03:12,116 >>   Num examples = 80
[INFO|trainer.py:4122] 2025-01-12 22:03:12,116 >>   Batch size = 1

  0%|          | 0/40 [00:00<?, ?it/s][A
  5%|▌         | 2/40 [00:00<00:02, 14.69it/s][A
 10%|█         | 4/40 [00:00<00:03,  9.21it/s][A
 15%|█▌        | 6/40 [00:00<00:04,  8.30it/s][A
 18%|█▊        | 7/40 [00:00<00:04,  8.07it/s][A
 20%|██        | 8/40 [00:00<00:04,  7.81it/s][A
 22%|██▎       | 9/40 [00:01<00:04,  7.60it/s][A
 25%|██▌       | 10/40 [00:01<00:04,  7.25it/s][A
 28%|██▊       | 11/40 [00:01<00:03,  7.28it/s][A
 30%|███       | 12/40 [00:01<00:03,  7.42it/s][A
 32%|███▎      | 13/40 [00:01<00:03,  7.18it/s][A
 35%|███▌      | 14/40 [00:01<00:03,  7.24it/s][A
 38%|███▊      | 15/40 [00:01<00:03,  7.28it/s][A
 40%|████      | 16/40 [00:02<00:03,  7.28it/s][A
 42%|████▎     | 17/40 [00:02<00:03,  7.31it/s][A
 45%|████▌     | 18/40 [00:02<00:02,  7.49it/s][A
 48%|████▊     | 19/40 [00:02<00:02,  7.42it/s][A
 50%|█████     | 20/40 [00:02<00:02,  7.43it/s][A
 52%|█████▎    | 21/40 [00:02<00:02,  7.15it/s][A
 55%|█████▌    | 22/40 [00:02<00:02,  7.22it/s][A
 57%|█████▊    | 23/40 [00:03<00:02,  7.21it/s][A
 60%|██████    | 24/40 [00:03<00:02,  6.99it/s][A
 62%|██████▎   | 25/40 [00:03<00:02,  7.07it/s][A
 65%|██████▌   | 26/40 [00:03<00:01,  7.14it/s][A
 68%|██████▊   | 27/40 [00:03<00:01,  7.22it/s][A
 70%|███████   | 28/40 [00:03<00:01,  7.21it/s][A
 72%|███████▎  | 29/40 [00:03<00:01,  7.26it/s][A
 75%|███████▌  | 30/40 [00:04<00:01,  7.27it/s][A
 78%|███████▊  | 31/40 [00:04<00:01,  7.28it/s][A
 80%|████████  | 32/40 [00:04<00:01,  7.28it/s][A
 82%|████████▎ | 33/40 [00:04<00:00,  7.07it/s][A
 85%|████████▌ | 34/40 [00:04<00:00,  7.07it/s][A
 88%|████████▊ | 35/40 [00:04<00:00,  7.10it/s][A
 90%|█████████ | 36/40 [00:04<00:00,  7.12it/s][A
 92%|█████████▎| 37/40 [00:04<00:00,  7.09it/s][A
 95%|█████████▌| 38/40 [00:05<00:00,  7.11it/s][A
 98%|█████████▊| 39/40 [00:05<00:00,  6.77it/s][A
100%|██████████| 40/40 [00:05<00:00,  6.89it/s][A                                                
                                               [A{'eval_loss': 0.007289367262274027, 'eval_runtime': 5.6261, 'eval_samples_per_second': 14.22, 'eval_steps_per_second': 7.11, 'epoch': 0.98}
 20%|██        | 44/220 [02:48<10:48,  3.68s/it]
100%|██████████| 40/40 [00:05<00:00,  6.89it/s][A
                                               [A 20%|██        | 45/220 [02:50<15:36,  5.35s/it] 21%|██        | 46/220 [02:54<14:05,  4.86s/it] 21%|██▏       | 47/220 [02:58<12:54,  4.48s/it] 22%|██▏       | 48/220 [03:01<12:09,  4.24s/it] 22%|██▏       | 49/220 [03:05<11:40,  4.10s/it] 23%|██▎       | 50/220 [03:09<11:09,  3.94s/it] 23%|██▎       | 51/220 [03:12<10:50,  3.85s/it] 24%|██▎       | 52/220 [03:16<10:35,  3.79s/it] 24%|██▍       | 53/220 [03:20<10:25,  3.75s/it] 25%|██▍       | 54/220 [03:23<10:19,  3.73s/it] 25%|██▌       | 55/220 [03:27<10:09,  3.70s/it] 25%|██▌       | 56/220 [03:31<10:00,  3.66s/it] 26%|██▌       | 57/220 [03:34<09:59,  3.68s/it] 26%|██▋       | 58/220 [03:38<09:56,  3.68s/it] 27%|██▋       | 59/220 [03:42<09:54,  3.70s/it] 27%|██▋       | 60/220 [03:45<09:47,  3.67s/it] 28%|██▊       | 61/220 [03:49<09:43,  3.67s/it] 28%|██▊       | 62/220 [03:53<09:39,  3.67s/it] 29%|██▊       | 63/220 [03:56<09:33,  3.65s/it] 29%|██▉       | 64/220 [04:00<09:28,  3.65s/it] 30%|██▉       | 65/220 [04:04<09:27,  3.66s/it] 30%|███       | 66/220 [04:07<09:22,  3.65s/it] 30%|███       | 67/220 [04:11<09:18,  3.65s/it] 31%|███       | 68/220 [04:15<09:15,  3.65s/it] 31%|███▏      | 69/220 [04:18<09:11,  3.65s/it] 32%|███▏      | 70/220 [04:22<09:08,  3.66s/it] 32%|███▏      | 71/220 [04:25<09:04,  3.65s/it] 33%|███▎      | 72/220 [04:29<09:02,  3.67s/it] 33%|███▎      | 73/220 [04:33<09:01,  3.69s/it] 34%|███▎      | 74/220 [04:37<09:00,  3.70s/it] 34%|███▍      | 75/220 [04:40<08:55,  3.70s/it] 35%|███▍      | 76/220 [04:44<08:55,  3.72s/it] 35%|███▌      | 77/220 [04:48<08:48,  3.69s/it] 35%|███▌      | 78/220 [04:51<08:42,  3.68s/it] 36%|███▌      | 79/220 [04:55<08:39,  3.69s/it] 36%|███▋      | 80/220 [04:59<08:38,  3.70s/it] 37%|███▋      | 81/220 [05:03<08:33,  3.69s/it] 37%|███▋      | 82/220 [05:06<08:32,  3.71s/it] 38%|███▊      | 83/220 [05:10<08:25,  3.69s/it] 38%|███▊      | 84/220 [05:14<08:20,  3.68s/it] 39%|███▊      | 85/220 [05:17<08:20,  3.71s/it] 39%|███▉      | 86/220 [05:21<08:18,  3.72s/it] 40%|███▉      | 87/220 [05:25<08:14,  3.72s/it] 40%|████      | 88/220 [05:29<08:11,  3.72s/it] 40%|████      | 89/220 [05:32<08:03,  3.69s/it][INFO|trainer.py:4117] 2025-01-12 22:06:01,423 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:06:01,423 >>   Num examples = 80
[INFO|trainer.py:4122] 2025-01-12 22:06:01,423 >>   Batch size = 1

  0%|          | 0/40 [00:00<?, ?it/s][A
  5%|▌         | 2/40 [00:00<00:02, 14.69it/s][A
 10%|█         | 4/40 [00:00<00:03,  9.19it/s][A
 15%|█▌        | 6/40 [00:00<00:04,  8.27it/s][A
 18%|█▊        | 7/40 [00:00<00:04,  8.05it/s][A
 20%|██        | 8/40 [00:00<00:04,  7.79it/s][A
 22%|██▎       | 9/40 [00:01<00:04,  7.58it/s][A
 25%|██▌       | 10/40 [00:01<00:04,  7.22it/s][A
 28%|██▊       | 11/40 [00:01<00:04,  7.24it/s][A
 30%|███       | 12/40 [00:01<00:03,  7.39it/s][A
 32%|███▎      | 13/40 [00:01<00:03,  7.15it/s][A
 35%|███▌      | 14/40 [00:01<00:03,  7.22it/s][A
 38%|███▊      | 15/40 [00:01<00:03,  7.26it/s][A
 40%|████      | 16/40 [00:02<00:03,  7.26it/s][A
 42%|████▎     | 17/40 [00:02<00:03,  7.30it/s][A
 45%|████▌     | 18/40 [00:02<00:02,  7.48it/s][A
 48%|████▊     | 19/40 [00:02<00:02,  7.41it/s][A
 50%|█████     | 20/40 [00:02<00:02,  7.43it/s][A
 52%|█████▎    | 21/40 [00:02<00:02,  7.14it/s][A
 55%|█████▌    | 22/40 [00:02<00:02,  7.22it/s][A
 57%|█████▊    | 23/40 [00:03<00:02,  7.14it/s][A
 60%|██████    | 24/40 [00:03<00:02,  6.90it/s][A
 62%|██████▎   | 25/40 [00:03<00:02,  6.99it/s][A
 65%|██████▌   | 26/40 [00:03<00:01,  7.07it/s][A
 68%|██████▊   | 27/40 [00:03<00:01,  7.17it/s][A
 70%|███████   | 28/40 [00:03<00:01,  7.20it/s][A
 72%|███████▎  | 29/40 [00:03<00:01,  7.25it/s][A
 75%|███████▌  | 30/40 [00:04<00:01,  7.27it/s][A
 78%|███████▊  | 31/40 [00:04<00:01,  7.25it/s][A
 80%|████████  | 32/40 [00:04<00:01,  7.23it/s][A
 82%|████████▎ | 33/40 [00:04<00:00,  7.04it/s][A
 85%|████████▌ | 34/40 [00:04<00:00,  7.07it/s][A
 88%|████████▊ | 35/40 [00:04<00:00,  7.11it/s][A
 90%|█████████ | 36/40 [00:04<00:00,  7.13it/s][A
 92%|█████████▎| 37/40 [00:05<00:00,  7.09it/s][A
 95%|█████████▌| 38/40 [00:05<00:00,  7.06it/s][A
 98%|█████████▊| 39/40 [00:05<00:00,  6.67it/s][A
100%|██████████| 40/40 [00:05<00:00,  6.81it/s][A                                                
                                               [A{'eval_loss': 0.0043191807344555855, 'eval_runtime': 5.6075, 'eval_samples_per_second': 14.267, 'eval_steps_per_second': 7.133, 'epoch': 1.99}
 40%|████      | 89/220 [05:38<08:03,  3.69s/it]
100%|██████████| 40/40 [00:05<00:00,  6.81it/s][A
                                               [A 41%|████      | 90/220 [05:42<11:42,  5.40s/it] 41%|████▏     | 91/220 [05:45<10:32,  4.90s/it] 42%|████▏     | 92/220 [05:49<09:42,  4.55s/it] 42%|████▏     | 93/220 [05:53<09:04,  4.28s/it] 43%|████▎     | 94/220 [05:56<08:36,  4.10s/it] 43%|████▎     | 95/220 [06:00<08:18,  3.99s/it] 44%|████▎     | 96/220 [06:04<08:06,  3.92s/it] 44%|████▍     | 97/220 [06:08<07:57,  3.88s/it] 45%|████▍     | 98/220 [06:11<07:44,  3.81s/it] 45%|████▌     | 99/220 [06:15<07:35,  3.76s/it] 45%|████▌     | 100/220 [06:19<07:28,  3.73s/it] 46%|████▌     | 101/220 [06:22<07:21,  3.71s/it] 46%|████▋     | 102/220 [06:26<07:13,  3.68s/it] 47%|████▋     | 103/220 [06:30<07:09,  3.67s/it] 47%|████▋     | 104/220 [06:33<07:06,  3.68s/it] 48%|████▊     | 105/220 [06:37<07:06,  3.70s/it] 48%|████▊     | 106/220 [06:41<07:02,  3.70s/it] 49%|████▊     | 107/220 [06:44<06:59,  3.72s/it] 49%|████▉     | 108/220 [06:48<06:53,  3.69s/it] 50%|████▉     | 109/220 [06:52<06:46,  3.66s/it] 50%|█████     | 110/220 [06:55<06:43,  3.67s/it] 50%|█████     | 111/220 [06:59<06:41,  3.69s/it] 51%|█████     | 112/220 [07:03<06:37,  3.68s/it] 51%|█████▏    | 113/220 [07:06<06:35,  3.70s/it] 52%|█████▏    | 114/220 [07:10<06:34,  3.72s/it] 52%|█████▏    | 115/220 [07:14<06:32,  3.73s/it] 53%|█████▎    | 116/220 [07:18<06:29,  3.75s/it] 53%|█████▎    | 117/220 [07:21<06:22,  3.72s/it] 54%|█████▎    | 118/220 [07:25<06:17,  3.70s/it] 54%|█████▍    | 119/220 [07:29<06:14,  3.71s/it] 55%|█████▍    | 120/220 [07:33<06:10,  3.71s/it] 55%|█████▌    | 121/220 [07:36<06:04,  3.69s/it] 55%|█████▌    | 122/220 [07:40<06:00,  3.67s/it] 56%|█████▌    | 123/220 [07:43<05:56,  3.68s/it] 56%|█████▋    | 124/220 [07:47<05:55,  3.71s/it] 57%|█████▋    | 125/220 [07:51<05:46,  3.65s/it] 57%|█████▋    | 126/220 [07:55<05:46,  3.68s/it] 58%|█████▊    | 127/220 [07:58<05:40,  3.66s/it] 58%|█████▊    | 128/220 [08:02<05:38,  3.67s/it] 59%|█████▊    | 129/220 [08:06<05:35,  3.69s/it] 59%|█████▉    | 130/220 [08:09<05:29,  3.66s/it] 60%|█████▉    | 131/220 [08:13<05:25,  3.66s/it] 60%|██████    | 132/220 [08:16<05:21,  3.66s/it] 60%|██████    | 133/220 [08:20<05:15,  3.63s/it][INFO|trainer.py:4117] 2025-01-12 22:08:50,989 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:08:50,989 >>   Num examples = 80
[INFO|trainer.py:4122] 2025-01-12 22:08:50,989 >>   Batch size = 1

  0%|          | 0/40 [00:00<?, ?it/s][A
  5%|▌         | 2/40 [00:00<00:02, 14.66it/s][A
 10%|█         | 4/40 [00:00<00:03,  9.19it/s][A
 15%|█▌        | 6/40 [00:00<00:04,  8.26it/s][A
 18%|█▊        | 7/40 [00:00<00:04,  8.05it/s][A
 20%|██        | 8/40 [00:00<00:04,  7.79it/s][A
 22%|██▎       | 9/40 [00:01<00:04,  7.60it/s][A
 25%|██▌       | 10/40 [00:01<00:04,  7.24it/s][A
 28%|██▊       | 11/40 [00:01<00:04,  7.24it/s][A
 30%|███       | 12/40 [00:01<00:03,  7.36it/s][A
 32%|███▎      | 13/40 [00:01<00:03,  7.13it/s][A
 35%|███▌      | 14/40 [00:01<00:03,  7.20it/s][A
 38%|███▊      | 15/40 [00:01<00:03,  7.25it/s][A
 40%|████      | 16/40 [00:02<00:03,  7.25it/s][A
 42%|████▎     | 17/40 [00:02<00:03,  7.27it/s][A
 45%|████▌     | 18/40 [00:02<00:02,  7.46it/s][A
 48%|████▊     | 19/40 [00:02<00:02,  7.37it/s][A
 50%|█████     | 20/40 [00:02<00:02,  7.40it/s][A
 52%|█████▎    | 21/40 [00:02<00:02,  7.07it/s][A
 55%|█████▌    | 22/40 [00:02<00:02,  7.17it/s][A
 57%|█████▊    | 23/40 [00:03<00:02,  7.14it/s][A
 60%|██████    | 24/40 [00:03<00:02,  6.92it/s][A
 62%|██████▎   | 25/40 [00:03<00:02,  7.02it/s][A
 65%|██████▌   | 26/40 [00:03<00:01,  7.07it/s][A
 68%|██████▊   | 27/40 [00:03<00:01,  7.17it/s][A
 70%|███████   | 28/40 [00:03<00:01,  7.14it/s][A
 72%|███████▎  | 29/40 [00:03<00:01,  7.21it/s][A
 75%|███████▌  | 30/40 [00:04<00:01,  7.18it/s][A
 78%|███████▊  | 31/40 [00:04<00:01,  7.19it/s][A
 80%|████████  | 32/40 [00:04<00:01,  7.19it/s][A
 82%|████████▎ | 33/40 [00:04<00:00,  7.01it/s][A
 85%|████████▌ | 34/40 [00:04<00:00,  7.05it/s][A
 88%|████████▊ | 35/40 [00:04<00:00,  7.10it/s][A
 90%|█████████ | 36/40 [00:04<00:00,  7.09it/s][A
 92%|█████████▎| 37/40 [00:05<00:00,  7.02it/s][A
 95%|█████████▌| 38/40 [00:05<00:00,  7.01it/s][A
 98%|█████████▊| 39/40 [00:05<00:00,  6.66it/s][A
100%|██████████| 40/40 [00:05<00:00,  6.76it/s][A                                                 
                                               [A{'eval_loss': 0.004477795213460922, 'eval_runtime': 5.7112, 'eval_samples_per_second': 14.008, 'eval_steps_per_second': 7.004, 'epoch': 2.98}
 60%|██████    | 133/220 [08:27<05:15,  3.63s/it]
100%|██████████| 40/40 [00:05<00:00,  6.76it/s][A
                                               [A 61%|██████    | 134/220 [08:29<07:38,  5.33s/it] 61%|██████▏   | 135/220 [08:33<06:49,  4.81s/it] 62%|██████▏   | 136/220 [08:37<06:14,  4.46s/it] 62%|██████▏   | 137/220 [08:40<05:49,  4.21s/it] 63%|██████▎   | 138/220 [08:44<05:35,  4.09s/it] 63%|██████▎   | 139/220 [08:48<05:19,  3.94s/it] 64%|██████▎   | 140/220 [08:51<05:06,  3.84s/it] 64%|██████▍   | 141/220 [08:55<05:01,  3.81s/it] 65%|██████▍   | 142/220 [08:59<04:53,  3.76s/it] 65%|██████▌   | 143/220 [09:02<04:47,  3.73s/it] 65%|██████▌   | 144/220 [09:06<04:44,  3.74s/it] 66%|██████▌   | 145/220 [09:10<04:42,  3.76s/it] 66%|██████▋   | 146/220 [09:13<04:35,  3.73s/it] 67%|██████▋   | 147/220 [09:17<04:28,  3.68s/it] 67%|██████▋   | 148/220 [09:21<04:24,  3.68s/it] 68%|██████▊   | 149/220 [09:24<04:20,  3.67s/it] 68%|██████▊   | 150/220 [09:28<04:15,  3.65s/it] 69%|██████▊   | 151/220 [09:32<04:14,  3.69s/it] 69%|██████▉   | 152/220 [09:35<04:10,  3.68s/it] 70%|██████▉   | 153/220 [09:39<04:05,  3.66s/it] 70%|███████   | 154/220 [09:43<04:05,  3.72s/it] 70%|███████   | 155/220 [09:47<04:01,  3.72s/it] 71%|███████   | 156/220 [09:50<03:56,  3.69s/it] 71%|███████▏  | 157/220 [09:54<03:51,  3.67s/it] 72%|███████▏  | 158/220 [09:58<03:50,  3.72s/it] 72%|███████▏  | 159/220 [10:01<03:45,  3.70s/it] 73%|███████▎  | 160/220 [10:05<03:42,  3.71s/it] 73%|███████▎  | 161/220 [10:09<03:38,  3.70s/it] 74%|███████▎  | 162/220 [10:12<03:34,  3.69s/it] 74%|███████▍  | 163/220 [10:16<03:30,  3.69s/it] 75%|███████▍  | 164/220 [10:20<03:28,  3.72s/it] 75%|███████▌  | 165/220 [10:24<03:24,  3.72s/it] 75%|███████▌  | 166/220 [10:27<03:21,  3.73s/it] 76%|███████▌  | 167/220 [10:31<03:17,  3.72s/it] 76%|███████▋  | 168/220 [10:35<03:14,  3.75s/it] 77%|███████▋  | 169/220 [10:39<03:09,  3.72s/it] 77%|███████▋  | 170/220 [10:42<03:05,  3.71s/it] 78%|███████▊  | 171/220 [10:46<03:01,  3.70s/it] 78%|███████▊  | 172/220 [10:50<02:57,  3.71s/it] 79%|███████▊  | 173/220 [10:53<02:53,  3.70s/it] 79%|███████▉  | 174/220 [10:57<02:49,  3.69s/it] 80%|███████▉  | 175/220 [11:01<02:48,  3.74s/it] 80%|████████  | 176/220 [11:05<02:43,  3.72s/it] 80%|████████  | 177/220 [11:08<02:40,  3.74s/it] 81%|████████  | 178/220 [11:12<02:36,  3.73s/it][INFO|trainer.py:4117] 2025-01-12 22:11:41,281 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:11:41,281 >>   Num examples = 80
[INFO|trainer.py:4122] 2025-01-12 22:11:41,281 >>   Batch size = 1

  0%|          | 0/40 [00:00<?, ?it/s][A
  5%|▌         | 2/40 [00:00<00:02, 14.67it/s][A
 10%|█         | 4/40 [00:00<00:03,  9.17it/s][A
 15%|█▌        | 6/40 [00:00<00:04,  8.26it/s][A
 18%|█▊        | 7/40 [00:00<00:04,  8.04it/s][A
 20%|██        | 8/40 [00:00<00:04,  7.77it/s][A
 22%|██▎       | 9/40 [00:01<00:04,  7.55it/s][A
 25%|██▌       | 10/40 [00:01<00:04,  7.19it/s][A
 28%|██▊       | 11/40 [00:01<00:04,  7.23it/s][A
 30%|███       | 12/40 [00:01<00:03,  7.40it/s][A
 32%|███▎      | 13/40 [00:01<00:03,  7.16it/s][A
 35%|███▌      | 14/40 [00:01<00:03,  7.22it/s][A
 38%|███▊      | 15/40 [00:01<00:03,  7.27it/s][A
 40%|████      | 16/40 [00:02<00:03,  7.26it/s][A
 42%|████▎     | 17/40 [00:02<00:03,  7.28it/s][A
 45%|████▌     | 18/40 [00:02<00:02,  7.46it/s][A
 48%|████▊     | 19/40 [00:02<00:02,  7.36it/s][A
 50%|█████     | 20/40 [00:02<00:02,  7.39it/s][A
 52%|█████▎    | 21/40 [00:02<00:02,  7.09it/s][A
 55%|█████▌    | 22/40 [00:02<00:02,  7.18it/s][A
 57%|█████▊    | 23/40 [00:03<00:02,  7.16it/s][A
 60%|██████    | 24/40 [00:03<00:02,  6.93it/s][A
 62%|██████▎   | 25/40 [00:03<00:02,  7.00it/s][A
 65%|██████▌   | 26/40 [00:03<00:01,  7.05it/s][A
 68%|██████▊   | 27/40 [00:03<00:01,  7.16it/s][A
 70%|███████   | 28/40 [00:03<00:01,  7.13it/s][A
 72%|███████▎  | 29/40 [00:03<00:01,  7.20it/s][A
 75%|███████▌  | 30/40 [00:04<00:01,  7.21it/s][A
 78%|███████▊  | 31/40 [00:04<00:01,  7.21it/s][A
 80%|████████  | 32/40 [00:04<00:01,  7.19it/s][A
 82%|████████▎ | 33/40 [00:04<00:00,  7.01it/s][A
 85%|████████▌ | 34/40 [00:04<00:00,  7.00it/s][A
 88%|████████▊ | 35/40 [00:04<00:00,  7.05it/s][A
 90%|█████████ | 36/40 [00:04<00:00,  7.08it/s][A
 92%|█████████▎| 37/40 [00:05<00:00,  7.05it/s][A
 95%|█████████▌| 38/40 [00:05<00:00,  7.06it/s][A
 98%|█████████▊| 39/40 [00:05<00:00,  6.70it/s][A
100%|██████████| 40/40 [00:05<00:00,  6.84it/s][A                                                 
                                               [A{'eval_loss': 0.004119443707168102, 'eval_runtime': 5.6191, 'eval_samples_per_second': 14.237, 'eval_steps_per_second': 7.119, 'epoch': 3.99}
 81%|████████  | 178/220 [11:18<02:36,  3.73s/it]
100%|██████████| 40/40 [00:05<00:00,  6.84it/s][A
                                               [A 81%|████████▏ | 179/220 [11:21<03:40,  5.39s/it] 82%|████████▏ | 180/220 [11:25<03:14,  4.87s/it] 82%|████████▏ | 181/220 [11:29<02:56,  4.54s/it] 83%|████████▎ | 182/220 [11:32<02:42,  4.28s/it] 83%|████████▎ | 183/220 [11:36<02:31,  4.08s/it] 84%|████████▎ | 184/220 [11:40<02:23,  3.98s/it] 84%|████████▍ | 185/220 [11:43<02:15,  3.88s/it] 85%|████████▍ | 186/220 [11:47<02:09,  3.82s/it] 85%|████████▌ | 187/220 [11:51<02:04,  3.77s/it] 85%|████████▌ | 188/220 [11:54<02:00,  3.76s/it] 86%|████████▌ | 189/220 [11:58<01:55,  3.73s/it] 86%|████████▋ | 190/220 [12:02<01:50,  3.68s/it] 87%|████████▋ | 191/220 [12:05<01:46,  3.68s/it] 87%|████████▋ | 192/220 [12:09<01:42,  3.66s/it] 88%|████████▊ | 193/220 [12:13<01:38,  3.63s/it] 88%|████████▊ | 194/220 [12:16<01:34,  3.65s/it] 89%|████████▊ | 195/220 [12:20<01:31,  3.64s/it] 89%|████████▉ | 196/220 [12:24<01:27,  3.65s/it] 90%|████████▉ | 197/220 [12:27<01:24,  3.67s/it] 90%|█████████ | 198/220 [12:31<01:20,  3.66s/it] 90%|█████████ | 199/220 [12:35<01:17,  3.68s/it] 91%|█████████ | 200/220 [12:38<01:14,  3.71s/it]                                                 {'loss': 0.1887, 'grad_norm': 0.012571468017995358, 'learning_rate': 2.496444112952734e-06, 'epoch': 4.49}
 91%|█████████ | 200/220 [12:38<01:14,  3.71s/it] 91%|█████████▏| 201/220 [12:42<01:10,  3.70s/it] 92%|█████████▏| 202/220 [12:46<01:06,  3.69s/it] 92%|█████████▏| 203/220 [12:49<01:02,  3.70s/it] 93%|█████████▎| 204/220 [12:53<00:59,  3.71s/it] 93%|█████████▎| 205/220 [12:57<00:55,  3.70s/it] 94%|█████████▎| 206/220 [13:01<00:51,  3.69s/it] 94%|█████████▍| 207/220 [13:04<00:48,  3.71s/it] 95%|█████████▍| 208/220 [13:08<00:44,  3.71s/it] 95%|█████████▌| 209/220 [13:12<00:40,  3.72s/it] 95%|█████████▌| 210/220 [13:15<00:36,  3.68s/it] 96%|█████████▌| 211/220 [13:19<00:33,  3.70s/it] 96%|█████████▋| 212/220 [13:23<00:29,  3.69s/it] 97%|█████████▋| 213/220 [13:26<00:25,  3.68s/it] 97%|█████████▋| 214/220 [13:30<00:22,  3.69s/it] 98%|█████████▊| 215/220 [13:34<00:18,  3.68s/it] 98%|█████████▊| 216/220 [13:37<00:14,  3.65s/it] 99%|█████████▊| 217/220 [13:41<00:11,  3.68s/it] 99%|█████████▉| 218/220 [13:45<00:07,  3.69s/it]100%|█████████▉| 219/220 [13:49<00:03,  3.72s/it]100%|██████████| 220/220 [13:52<00:00,  3.72s/it][INFO|trainer.py:3801] 2025-01-12 22:14:21,621 >> Saving model checkpoint to saves/qwen-14b-ru-e5/lora/sft/checkpoint-220
[INFO|configuration_utils.py:677] 2025-01-12 22:14:21,641 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 22:14:21,641 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2025-01-12 22:14:21,764 >> tokenizer config file saved in saves/qwen-14b-ru-e5/lora/sft/checkpoint-220/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-12 22:14:21,764 >> Special tokens file saved in saves/qwen-14b-ru-e5/lora/sft/checkpoint-220/special_tokens_map.json
[INFO|trainer.py:4117] 2025-01-12 22:14:22,005 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:14:22,005 >>   Num examples = 80
[INFO|trainer.py:4122] 2025-01-12 22:14:22,005 >>   Batch size = 1

  0%|          | 0/40 [00:00<?, ?it/s][A
  5%|▌         | 2/40 [00:00<00:02, 14.69it/s][A
 10%|█         | 4/40 [00:00<00:03,  9.19it/s][A
 15%|█▌        | 6/40 [00:00<00:04,  8.28it/s][A
 18%|█▊        | 7/40 [00:00<00:04,  8.07it/s][A
 20%|██        | 8/40 [00:00<00:04,  7.78it/s][A
 22%|██▎       | 9/40 [00:01<00:04,  7.57it/s][A
 25%|██▌       | 10/40 [00:01<00:04,  7.24it/s][A
 28%|██▊       | 11/40 [00:01<00:03,  7.27it/s][A
 30%|███       | 12/40 [00:01<00:03,  7.42it/s][A
 32%|███▎      | 13/40 [00:01<00:03,  7.17it/s][A
 35%|███▌      | 14/40 [00:01<00:03,  7.23it/s][A
 38%|███▊      | 15/40 [00:01<00:03,  7.26it/s][A
 40%|████      | 16/40 [00:02<00:03,  7.26it/s][A
 42%|████▎     | 17/40 [00:02<00:03,  7.27it/s][A
 45%|████▌     | 18/40 [00:02<00:02,  7.45it/s][A
 48%|████▊     | 19/40 [00:02<00:02,  7.36it/s][A
 50%|█████     | 20/40 [00:02<00:02,  7.40it/s][A
 52%|█████▎    | 21/40 [00:02<00:02,  7.09it/s][A
 55%|█████▌    | 22/40 [00:02<00:02,  7.18it/s][A
 57%|█████▊    | 23/40 [00:03<00:02,  7.15it/s][A
 60%|██████    | 24/40 [00:03<00:02,  6.94it/s][A
 62%|██████▎   | 25/40 [00:03<00:02,  7.01it/s][A
 65%|██████▌   | 26/40 [00:03<00:01,  7.01it/s][A
 68%|██████▊   | 27/40 [00:03<00:01,  7.12it/s][A
 70%|███████   | 28/40 [00:03<00:01,  7.11it/s][A
 72%|███████▎  | 29/40 [00:03<00:01,  7.18it/s][A
 75%|███████▌  | 30/40 [00:04<00:01,  7.21it/s][A
 78%|███████▊  | 31/40 [00:04<00:01,  7.22it/s][A
 80%|████████  | 32/40 [00:04<00:01,  7.19it/s][A
 82%|████████▎ | 33/40 [00:04<00:00,  7.00it/s][A
 85%|████████▌ | 34/40 [00:04<00:00,  7.02it/s][A
 88%|████████▊ | 35/40 [00:04<00:00,  7.06it/s][A
 90%|█████████ | 36/40 [00:04<00:00,  7.08it/s][A
 92%|█████████▎| 37/40 [00:05<00:00,  7.06it/s][A
 95%|█████████▌| 38/40 [00:05<00:00,  7.06it/s][A
 98%|█████████▊| 39/40 [00:05<00:00,  6.71it/s][A
100%|██████████| 40/40 [00:05<00:00,  6.84it/s][A                                                 
                                               [A{'eval_loss': 0.004187959246337414, 'eval_runtime': 5.6001, 'eval_samples_per_second': 14.285, 'eval_steps_per_second': 7.143, 'epoch': 4.94}
100%|██████████| 220/220 [13:58<00:00,  3.72s/it]
100%|██████████| 40/40 [00:05<00:00,  6.84it/s][A
                                               [A[INFO|trainer.py:2584] 2025-01-12 22:14:27,605 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 848.423, 'train_samples_per_second': 4.22, 'train_steps_per_second': 0.259, 'train_loss': 0.17165235046043315, 'epoch': 4.94}
100%|██████████| 220/220 [13:58<00:00,  3.72s/it]100%|██████████| 220/220 [13:58<00:00,  3.81s/it]
[INFO|trainer.py:3801] 2025-01-12 22:14:27,606 >> Saving model checkpoint to saves/qwen-14b-ru-e5/lora/sft
[INFO|configuration_utils.py:677] 2025-01-12 22:14:27,625 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 22:14:27,625 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2025-01-12 22:14:27,735 >> tokenizer config file saved in saves/qwen-14b-ru-e5/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-12 22:14:27,735 >> Special tokens file saved in saves/qwen-14b-ru-e5/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =     4.9385
  total_flos               = 96102369GF
  train_loss               =     0.1717
  train_runtime            = 0:14:08.42
  train_samples_per_second =       4.22
  train_steps_per_second   =      0.259
Figure saved at: saves/qwen-14b-ru-e5/lora/sft/training_loss.png
Figure saved at: saves/qwen-14b-ru-e5/lora/sft/training_eval_loss.png
[WARNING|2025-01-12 22:14:27] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4117] 2025-01-12 22:14:27,882 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:14:27,882 >>   Num examples = 80
[INFO|trainer.py:4122] 2025-01-12 22:14:27,882 >>   Batch size = 1
  0%|          | 0/40 [00:00<?, ?it/s]  5%|▌         | 2/40 [00:00<00:02, 14.63it/s] 10%|█         | 4/40 [00:00<00:03,  9.18it/s] 15%|█▌        | 6/40 [00:00<00:04,  8.26it/s] 18%|█▊        | 7/40 [00:00<00:04,  8.06it/s] 20%|██        | 8/40 [00:00<00:04,  7.81it/s] 22%|██▎       | 9/40 [00:01<00:04,  7.62it/s] 25%|██▌       | 10/40 [00:01<00:04,  7.26it/s] 28%|██▊       | 11/40 [00:01<00:03,  7.28it/s] 30%|███       | 12/40 [00:01<00:03,  7.41it/s] 32%|███▎      | 13/40 [00:01<00:03,  7.17it/s] 35%|███▌      | 14/40 [00:01<00:03,  7.23it/s] 38%|███▊      | 15/40 [00:01<00:03,  7.27it/s] 40%|████      | 16/40 [00:02<00:03,  7.27it/s] 42%|████▎     | 17/40 [00:02<00:03,  7.29it/s] 45%|████▌     | 18/40 [00:02<00:02,  7.47it/s] 48%|████▊     | 19/40 [00:02<00:02,  7.38it/s] 50%|█████     | 20/40 [00:02<00:02,  7.41it/s] 52%|█████▎    | 21/40 [00:02<00:02,  7.11it/s] 55%|█████▌    | 22/40 [00:02<00:02,  7.21it/s] 57%|█████▊    | 23/40 [00:03<00:02,  7.19it/s] 60%|██████    | 24/40 [00:03<00:02,  6.95it/s] 62%|██████▎   | 25/40 [00:03<00:02,  7.01it/s] 65%|██████▌   | 26/40 [00:03<00:01,  7.07it/s] 68%|██████▊   | 27/40 [00:03<00:01,  7.17it/s] 70%|███████   | 28/40 [00:03<00:01,  7.11it/s] 72%|███████▎  | 29/40 [00:03<00:01,  7.19it/s] 75%|███████▌  | 30/40 [00:04<00:01,  7.16it/s] 78%|███████▊  | 31/40 [00:04<00:01,  7.14it/s] 80%|████████  | 32/40 [00:04<00:01,  7.13it/s] 82%|████████▎ | 33/40 [00:04<00:01,  6.98it/s] 85%|████████▌ | 34/40 [00:04<00:00,  7.01it/s] 88%|████████▊ | 35/40 [00:04<00:00,  7.02it/s] 90%|█████████ | 36/40 [00:04<00:00,  7.09it/s] 92%|█████████▎| 37/40 [00:05<00:00,  7.05it/s] 95%|█████████▌| 38/40 [00:05<00:00,  7.03it/s] 98%|█████████▊| 39/40 [00:05<00:00,  6.66it/s]100%|██████████| 40/40 [00:05<00:00,  6.78it/s]100%|██████████| 40/40 [00:05<00:00,  7.30it/s]
***** eval metrics *****
  epoch                   =     4.9385
  eval_loss               =     0.0042
  eval_runtime            = 0:00:05.60
  eval_samples_per_second =     14.269
  eval_steps_per_second   =      7.135
[INFO|modelcard.py:449] 2025-01-12 22:14:33,489 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[rank0]:[W112 22:14:33.324178117 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

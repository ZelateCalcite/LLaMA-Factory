[2025-01-12 22:15:03,616] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|2025-01-12 22:15:04] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:28250
W0112 22:15:05.546000 791870 site-packages/torch/distributed/run.py:793] 
W0112 22:15:05.546000 791870 site-packages/torch/distributed/run.py:793] *****************************************
W0112 22:15:05.546000 791870 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0112 22:15:05.546000 791870 site-packages/torch/distributed/run.py:793] *****************************************
[2025-01-12 22:15:07,030] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-12 22:15:07,030] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[WARNING|2025-01-12 22:15:07] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.
[WARNING|2025-01-12 22:15:07] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|2025-01-12 22:15:07] llamafactory.hparams.parser:359 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:677] 2025-01-12 22:15:07,844 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 22:15:07,844 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,844 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,845 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,845 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,845 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,845 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,845 >> loading file tokenizer_config.json
[INFO|2025-01-12 22:15:07] llamafactory.hparams.parser:359 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2475] 2025-01-12 22:15:07,987 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:677] 2025-01-12 22:15:07,987 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 22:15:07,988 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,988 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,988 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,988 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,988 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,988 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 22:15:07,988 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-12 22:15:08,119 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-01-12 22:15:08] llamafactory.data.template:157 >> Add <|im_end|> to stop words.
[INFO|2025-01-12 22:15:08] llamafactory.data.loader:157 >> Loading dataset entity_trans_ka.json...
[rank1]:[W112 22:15:08.705024810 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1451 examples [00:00, 38561.78 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1451 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1451/1451 [00:00<00:00, 10556.92 examples/s]
[rank0]:[W112 22:15:32.079917425 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1451 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 91/1451 [00:00<00:05, 258.83 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 364/1451 [00:00<00:01, 915.17 examples/s]Running tokenizer on dataset (num_proc=16):  38%|███▊      | 546/1451 [00:00<00:00, 1145.90 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 728/1451 [00:00<00:00, 1173.86 examples/s]Running tokenizer on dataset (num_proc=16):  63%|██████▎   | 910/1451 [00:00<00:00, 1326.27 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 1181/1451 [00:00<00:00, 1529.22 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1451/1451 [00:01<00:00, 1809.88 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1451/1451 [00:01<00:00, 1259.01 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2610, 525, 264, 9990, 86442, 12, 22574, 14468, 7341, 11, 4486, 1492, 752, 14683, 1045, 6364, 22870, 1119, 86442, 13, 7036, 429, 279, 4396, 1102, 1265, 387, 264, 17133, 429, 7952, 304, 279, 11652, 624, 40, 686, 2968, 498, 458, 6364, 17133, 323, 264, 86442, 11652, 11, 1380, 279, 6364, 17133, 374, 279, 14468, 1102, 304, 279, 86442, 11652, 13, 358, 1366, 311, 1477, 279, 7112, 86442, 17133, 315, 279, 6364, 14468, 1102, 624, 5501, 1744, 3019, 553, 3019, 13, 151645, 198, 151644, 77091, 198, 39814, 11, 358, 646, 7789, 448, 429, 13, 5209, 3410, 279, 6364, 17133, 323, 279, 86442, 11652, 432, 7952, 304, 773, 358, 646, 10542, 279, 12159, 86442, 17133, 369, 498, 382, 5501, 3561, 279, 1946, 438, 11017, 1447, 22574, 17133, 25, 508, 4208, 6364, 17133, 921, 9499, 1775, 1103, 11652, 25, 508, 4208, 86442, 11652, 2533, 40, 3278, 1221, 3410, 279, 7112, 86442, 17133, 429, 33210, 311, 279, 6364, 14468, 1102, 13, 151645, 198, 151644, 872, 198, 22574, 17133, 25, 7513, 9652, 198, 9499, 1775, 1103, 11652, 25, 220, 146017, 146727, 145608, 146393, 147143, 146393, 146479, 145302, 145908, 145302, 145395, 1959, 220, 146017, 146727, 145608, 146393, 147143, 145395, 146727, 148210, 145302, 145608, 145302, 145908, 220, 146017, 145608, 145989, 12, 146017, 145608, 145989, 145302, 220, 145974, 146479, 146216, 145302, 148210, 146727, 146216, 146017, 145702, 146393, 146727, 145395, 146216, 146017, 145908, 145302, 220, 145302, 146216, 145908, 147172, 145302, 147172, 145974, 147172, 145302, 13, 151645, 198, 151644, 77091, 198, 785, 2661, 6364, 17133, 7513, 9652, 646, 387, 24531, 1119, 86442, 438, 220, 146017, 146727, 145608, 146393, 147143, 146393, 146479, 145302, 145908, 145302, 145395, 11, 323, 279, 7112, 86442, 17133, 304, 279, 2661, 11652, 1265, 387, 220, 146017, 146727, 145608, 146393, 147143, 146393, 146479, 145302, 145908, 145302, 145395, 13, 151645, 198, 151644, 872, 198, 3973, 279, 1102, 323, 1281, 2704, 1817, 3409, 315, 279, 1102, 7952, 304, 279, 2661, 86442, 11652, 13, 151645, 198, 151644, 77091, 198, 39814, 0, 358, 614, 10067, 279, 1102, 220, 146017, 146727, 145608, 146393, 147143, 146393, 146479, 145302, 145908, 145302, 145395, 323, 432, 2167, 7952, 304, 279, 2661, 11652, 13, 151645, 198, 151644, 872, 198, 35127, 752, 279, 1590, 1102, 2041, 1008, 4244, 13, 151645, 198, 151644, 77091, 198, 146017, 146727, 145608, 146393, 147143, 146393, 146479, 145302, 145908, 145302, 145395, 151645]
inputs:
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
You are a senior Georgian-English translation master, please help me translate some English sentences into Georgian. Note that the correct result should be a phrase that appears in the sentence.
I will give you an English phrase and a Georgian sentence, where the English phrase is the translation result in the Georgian sentence. I want to find the raw Georgian phrase of the English translation result.
Please think step by step.<|im_end|>
<|im_start|>assistant
Sure, I can assist with that. Please provide the English phrase and the Georgian sentence it appears in so I can identify the corresponding Georgian phrase for you.

Please format the input as follows:

English phrase: [insert English phrase]
Georgian sentence: [insert Georgian sentence]

I'll then provide the raw Georgian phrase that corresponds to the English translation result.<|im_end|>
<|im_start|>user
English phrase: European Commission
Georgian sentence: ევროკომისია — ევროკავშირის ერთ-ერთი უმნიშვნელოვანესი ინსტიტუტი.<|im_end|>
<|im_start|>assistant
The given English phrase European Commission can be translated into Georgian as ევროკომისია, and the raw Georgian phrase in the given sentence should be ევროკომისია.<|im_end|>
<|im_start|>user
Check the result and make sure each word of the result appears in the given Georgian sentence.<|im_end|>
<|im_start|>assistant
Sure! I have checked the result ევროკომისია and it really appears in the given sentence.<|im_end|>
<|im_start|>user
Give me the final result without other words.<|im_end|>
<|im_start|>assistant
ევროკომისია<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 39814, 11, 358, 646, 7789, 448, 429, 13, 5209, 3410, 279, 6364, 17133, 323, 279, 86442, 11652, 432, 7952, 304, 773, 358, 646, 10542, 279, 12159, 86442, 17133, 369, 498, 382, 5501, 3561, 279, 1946, 438, 11017, 1447, 22574, 17133, 25, 508, 4208, 6364, 17133, 921, 9499, 1775, 1103, 11652, 25, 508, 4208, 86442, 11652, 2533, 40, 3278, 1221, 3410, 279, 7112, 86442, 17133, 429, 33210, 311, 279, 6364, 14468, 1102, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 2661, 6364, 17133, 7513, 9652, 646, 387, 24531, 1119, 86442, 438, 220, 146017, 146727, 145608, 146393, 147143, 146393, 146479, 145302, 145908, 145302, 145395, 11, 323, 279, 7112, 86442, 17133, 304, 279, 2661, 11652, 1265, 387, 220, 146017, 146727, 145608, 146393, 147143, 146393, 146479, 145302, 145908, 145302, 145395, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 39814, 0, 358, 614, 10067, 279, 1102, 220, 146017, 146727, 145608, 146393, 147143, 146393, 146479, 145302, 145908, 145302, 145395, 323, 432, 2167, 7952, 304, 279, 2661, 11652, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 146017, 146727, 145608, 146393, 147143, 146393, 146479, 145302, 145908, 145302, 145395, 151645]
labels:
Sure, I can assist with that. Please provide the English phrase and the Georgian sentence it appears in so I can identify the corresponding Georgian phrase for you.

Please format the input as follows:

English phrase: [insert English phrase]
Georgian sentence: [insert Georgian sentence]

I'll then provide the raw Georgian phrase that corresponds to the English translation result.<|im_end|>The given English phrase European Commission can be translated into Georgian as ევროკომისია, and the raw Georgian phrase in the given sentence should be ევროკომისია.<|im_end|>Sure! I have checked the result ევროკომისია and it really appears in the given sentence.<|im_end|>ევროკომისია<|im_end|>
[INFO|configuration_utils.py:677] 2025-01-12 22:15:56,855 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 22:15:56,856 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|2025-01-12 22:15:56] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.
[INFO|modeling_utils.py:3934] 2025-01-12 22:15:56,900 >> loading weights file /mnt/sda/zzh/Qwen2.5-14B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1670] 2025-01-12 22:15:56,901 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1096] 2025-01-12 22:15:56,901 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:04,  1.69it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:03,  1.62it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:03,  1.65it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:02<00:02,  1.67it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:02<00:01,  1.70it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:03<00:01,  1.72it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:04<00:00,  1.73it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  2.06it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.82it/s]
[INFO|modeling_utils.py:4800] 2025-01-12 22:16:01,427 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4808] 2025-01-12 22:16:01,427 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /mnt/sda/zzh/Qwen2.5-14B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1049] 2025-01-12 22:16:01,428 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/generation_config.json
[INFO|configuration_utils.py:1096] 2025-01-12 22:16:01,428 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|2025-01-12 22:16:01] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
[INFO|2025-01-12 22:16:01] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.
[INFO|2025-01-12 22:16:01] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.
[INFO|2025-01-12 22:16:01] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA
[INFO|2025-01-12 22:16:01] llamafactory.model.model_utils.misc:157 >> Found linear modules: gate_proj,down_proj,k_proj,q_proj,up_proj,o_proj,v_proj
[INFO|2025-01-12 22:16:01] llamafactory.model.loader:157 >> trainable params: 34,406,400 || all params: 14,804,440,064 || trainable%: 0.2324
[INFO|trainer.py:698] 2025-01-12 22:16:01,752 >> Using auto half precision backend
Loading checkpoint shards:  12%|█▎        | 1/8 [00:04<00:34,  4.98s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:10<00:30,  5.05s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:15<00:25,  5.07s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:20<00:20,  5.08s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:25<00:15,  5.07s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:30<00:10,  5.07s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:35<00:05,  5.08s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:37<00:00,  4.16s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:37<00:00,  4.71s/it]
[INFO|trainer.py:2313] 2025-01-12 22:16:37,711 >> ***** Running training *****
[INFO|trainer.py:2314] 2025-01-12 22:16:37,711 >>   Num examples = 1,305
[INFO|trainer.py:2315] 2025-01-12 22:16:37,711 >>   Num Epochs = 5
[INFO|trainer.py:2316] 2025-01-12 22:16:37,711 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2319] 2025-01-12 22:16:37,711 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2320] 2025-01-12 22:16:37,711 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2321] 2025-01-12 22:16:37,711 >>   Total optimization steps = 405
[INFO|trainer.py:2322] 2025-01-12 22:16:37,713 >>   Number of trainable parameters = 34,406,400
  0%|          | 0/405 [00:00<?, ?it/s]  0%|          | 1/405 [00:04<30:58,  4.60s/it]  0%|          | 2/405 [00:08<28:56,  4.31s/it]  1%|          | 3/405 [00:12<27:21,  4.08s/it]  1%|          | 4/405 [00:16<26:22,  3.95s/it]  1%|          | 5/405 [00:20<26:19,  3.95s/it]  1%|▏         | 6/405 [00:23<25:51,  3.89s/it]  2%|▏         | 7/405 [00:27<25:34,  3.86s/it]  2%|▏         | 8/405 [00:31<25:22,  3.83s/it]  2%|▏         | 9/405 [00:35<25:39,  3.89s/it]  2%|▏         | 10/405 [00:39<25:21,  3.85s/it]  3%|▎         | 11/405 [00:43<25:28,  3.88s/it]  3%|▎         | 12/405 [00:47<25:41,  3.92s/it]  3%|▎         | 13/405 [00:51<25:41,  3.93s/it]  3%|▎         | 14/405 [00:55<25:19,  3.89s/it]  4%|▎         | 15/405 [00:59<25:25,  3.91s/it]  4%|▍         | 16/405 [01:02<25:27,  3.93s/it]  4%|▍         | 17/405 [01:07<25:38,  3.97s/it]  4%|▍         | 18/405 [01:10<25:35,  3.97s/it]  5%|▍         | 19/405 [01:14<25:30,  3.97s/it]  5%|▍         | 20/405 [01:18<25:25,  3.96s/it]  5%|▌         | 21/405 [01:23<25:41,  4.01s/it]  5%|▌         | 22/405 [01:27<25:42,  4.03s/it]  6%|▌         | 23/405 [01:31<25:35,  4.02s/it]  6%|▌         | 24/405 [01:34<25:11,  3.97s/it]  6%|▌         | 25/405 [01:38<24:40,  3.90s/it]  6%|▋         | 26/405 [01:42<24:33,  3.89s/it]  7%|▋         | 27/405 [01:46<24:35,  3.90s/it]  7%|▋         | 28/405 [01:50<24:33,  3.91s/it]  7%|▋         | 29/405 [01:54<24:28,  3.91s/it]  7%|▋         | 30/405 [01:58<24:16,  3.89s/it]  8%|▊         | 31/405 [02:02<24:50,  3.99s/it]  8%|▊         | 32/405 [02:06<24:36,  3.96s/it]  8%|▊         | 33/405 [02:10<24:34,  3.96s/it]  8%|▊         | 34/405 [02:14<24:08,  3.91s/it]  9%|▊         | 35/405 [02:17<23:48,  3.86s/it]  9%|▉         | 36/405 [02:21<23:40,  3.85s/it]  9%|▉         | 37/405 [02:25<23:38,  3.85s/it]  9%|▉         | 38/405 [02:29<23:26,  3.83s/it] 10%|▉         | 39/405 [02:33<23:42,  3.89s/it] 10%|▉         | 40/405 [02:37<23:46,  3.91s/it] 10%|█         | 41/405 [02:41<23:34,  3.89s/it] 10%|█         | 42/405 [02:44<23:24,  3.87s/it] 11%|█         | 43/405 [02:48<23:22,  3.87s/it] 11%|█         | 44/405 [02:52<23:16,  3.87s/it] 11%|█         | 45/405 [02:56<23:16,  3.88s/it] 11%|█▏        | 46/405 [03:00<23:32,  3.93s/it] 12%|█▏        | 47/405 [03:04<23:21,  3.92s/it] 12%|█▏        | 48/405 [03:08<23:07,  3.89s/it] 12%|█▏        | 49/405 [03:12<22:54,  3.86s/it] 12%|█▏        | 50/405 [03:16<22:58,  3.88s/it] 13%|█▎        | 51/405 [03:19<22:51,  3.87s/it] 13%|█▎        | 52/405 [03:23<22:43,  3.86s/it] 13%|█▎        | 53/405 [03:27<22:53,  3.90s/it] 13%|█▎        | 54/405 [03:31<23:02,  3.94s/it] 14%|█▎        | 55/405 [03:35<23:08,  3.97s/it] 14%|█▍        | 56/405 [03:39<22:53,  3.94s/it] 14%|█▍        | 57/405 [03:43<23:08,  3.99s/it] 14%|█▍        | 58/405 [03:47<23:02,  3.99s/it] 15%|█▍        | 59/405 [03:51<22:51,  3.96s/it] 15%|█▍        | 60/405 [03:55<22:44,  3.96s/it] 15%|█▌        | 61/405 [03:59<22:34,  3.94s/it] 15%|█▌        | 62/405 [04:03<22:11,  3.88s/it] 16%|█▌        | 63/405 [04:07<22:21,  3.92s/it] 16%|█▌        | 64/405 [04:11<22:20,  3.93s/it] 16%|█▌        | 65/405 [04:15<22:17,  3.93s/it] 16%|█▋        | 66/405 [04:19<22:46,  4.03s/it] 17%|█▋        | 67/405 [04:23<22:59,  4.08s/it] 17%|█▋        | 68/405 [04:27<22:31,  4.01s/it] 17%|█▋        | 69/405 [04:31<22:35,  4.03s/it] 17%|█▋        | 70/405 [04:35<22:00,  3.94s/it] 18%|█▊        | 71/405 [04:39<21:56,  3.94s/it] 18%|█▊        | 72/405 [04:43<22:05,  3.98s/it] 18%|█▊        | 73/405 [04:47<22:06,  4.00s/it] 18%|█▊        | 74/405 [04:51<22:00,  3.99s/it] 19%|█▊        | 75/405 [04:55<21:59,  4.00s/it] 19%|█▉        | 76/405 [04:59<21:51,  3.99s/it] 19%|█▉        | 77/405 [05:03<21:55,  4.01s/it] 19%|█▉        | 78/405 [05:07<22:02,  4.04s/it] 20%|█▉        | 79/405 [05:11<21:59,  4.05s/it] 20%|█▉        | 80/405 [05:15<21:33,  3.98s/it] 20%|██        | 81/405 [05:19<21:27,  3.97s/it][INFO|trainer.py:4117] 2025-01-12 22:22:06,980 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:22:06,980 >>   Num examples = 146
[INFO|trainer.py:4122] 2025-01-12 22:22:06,980 >>   Batch size = 1

  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:05, 12.90it/s][A
  5%|▌         | 4/73 [00:00<00:08,  7.95it/s][A
  7%|▋         | 5/73 [00:00<00:08,  7.67it/s][A
  8%|▊         | 6/73 [00:00<00:08,  7.52it/s][A
 10%|▉         | 7/73 [00:00<00:09,  7.14it/s][A
 11%|█         | 8/73 [00:01<00:09,  7.14it/s][A
 12%|█▏        | 9/73 [00:01<00:09,  7.01it/s][A
 14%|█▎        | 10/73 [00:01<00:09,  6.83it/s][A
 15%|█▌        | 11/73 [00:01<00:09,  6.84it/s][A
 16%|█▋        | 12/73 [00:01<00:08,  6.90it/s][A
 18%|█▊        | 13/73 [00:01<00:09,  6.47it/s][A
 19%|█▉        | 14/73 [00:01<00:09,  6.52it/s][A
 21%|██        | 15/73 [00:02<00:08,  6.70it/s][A
 22%|██▏       | 16/73 [00:02<00:08,  6.85it/s][A
 23%|██▎       | 17/73 [00:02<00:08,  6.53it/s][A
 25%|██▍       | 18/73 [00:02<00:08,  6.67it/s][A
 26%|██▌       | 19/73 [00:02<00:08,  6.51it/s][A
 27%|██▋       | 20/73 [00:02<00:07,  6.66it/s][A
 29%|██▉       | 21/73 [00:03<00:07,  6.85it/s][A
 30%|███       | 22/73 [00:03<00:07,  6.91it/s][A
 32%|███▏      | 23/73 [00:03<00:07,  6.56it/s][A
 33%|███▎      | 24/73 [00:03<00:07,  6.70it/s][A
 34%|███▍      | 25/73 [00:03<00:07,  6.59it/s][A
 36%|███▌      | 26/73 [00:03<00:07,  6.70it/s][A
 37%|███▋      | 27/73 [00:03<00:06,  6.81it/s][A
 38%|███▊      | 28/73 [00:04<00:06,  6.76it/s][A
 40%|███▉      | 29/73 [00:04<00:06,  6.33it/s][A
 41%|████      | 30/73 [00:04<00:06,  6.60it/s][A
 42%|████▏     | 31/73 [00:04<00:06,  6.61it/s][A
 44%|████▍     | 32/73 [00:04<00:06,  6.80it/s][A
 45%|████▌     | 33/73 [00:04<00:05,  6.91it/s][A
 47%|████▋     | 34/73 [00:04<00:05,  6.57it/s][A
 48%|████▊     | 35/73 [00:05<00:06,  5.93it/s][A
 49%|████▉     | 36/73 [00:05<00:05,  6.30it/s][A
 51%|█████     | 37/73 [00:05<00:05,  6.54it/s][A
 52%|█████▏    | 38/73 [00:05<00:05,  6.51it/s][A
 53%|█████▎    | 39/73 [00:05<00:05,  6.45it/s][A
 55%|█████▍    | 40/73 [00:05<00:05,  6.40it/s][A
 56%|█████▌    | 41/73 [00:06<00:05,  6.13it/s][A
 58%|█████▊    | 42/73 [00:06<00:04,  6.44it/s][A
 59%|█████▉    | 43/73 [00:06<00:04,  6.65it/s][A
 60%|██████    | 44/73 [00:06<00:04,  6.47it/s][A
 62%|██████▏   | 45/73 [00:06<00:04,  6.43it/s][A
 63%|██████▎   | 46/73 [00:06<00:04,  6.62it/s][A
 64%|██████▍   | 47/73 [00:06<00:03,  6.80it/s][A
 66%|██████▌   | 48/73 [00:07<00:03,  6.92it/s][A
 67%|██████▋   | 49/73 [00:07<00:03,  6.56it/s][A
 68%|██████▊   | 50/73 [00:07<00:03,  6.73it/s][A
 70%|██████▉   | 51/73 [00:07<00:03,  6.80it/s][A
 71%|███████   | 52/73 [00:07<00:03,  6.69it/s][A
 73%|███████▎  | 53/73 [00:07<00:03,  5.84it/s][A
 74%|███████▍  | 54/73 [00:08<00:03,  5.94it/s][A
 75%|███████▌  | 55/73 [00:08<00:03,  5.67it/s][A
 77%|███████▋  | 56/73 [00:08<00:02,  6.07it/s][A
 78%|███████▊  | 57/73 [00:08<00:02,  6.24it/s][A
 79%|███████▉  | 58/73 [00:08<00:02,  6.51it/s][A
 81%|████████  | 59/73 [00:08<00:02,  6.67it/s][A
 82%|████████▏ | 60/73 [00:09<00:02,  6.47it/s][A
 84%|████████▎ | 61/73 [00:09<00:01,  6.65it/s][A
 85%|████████▍ | 62/73 [00:09<00:01,  6.76it/s][A
 86%|████████▋ | 63/73 [00:09<00:01,  6.78it/s][A
 88%|████████▊ | 64/73 [00:09<00:01,  6.77it/s][A
 89%|████████▉ | 65/73 [00:09<00:01,  6.49it/s][A
 90%|█████████ | 66/73 [00:09<00:01,  6.43it/s][A
 92%|█████████▏| 67/73 [00:10<00:00,  6.39it/s][A
 93%|█████████▎| 68/73 [00:10<00:00,  6.59it/s][A
 95%|█████████▍| 69/73 [00:10<00:00,  6.55it/s][A
 96%|█████████▌| 70/73 [00:10<00:00,  6.73it/s][A
 97%|█████████▋| 71/73 [00:10<00:00,  6.48it/s][A
 99%|█████████▊| 72/73 [00:10<00:00,  6.51it/s][A
100%|██████████| 73/73 [00:10<00:00,  6.72it/s][A                                                
                                               [A{'eval_loss': 0.0016081709181889892, 'eval_runtime': 11.1292, 'eval_samples_per_second': 13.119, 'eval_steps_per_second': 6.559, 'epoch': 0.99}
 20%|██        | 81/405 [05:30<21:27,  3.97s/it]
100%|██████████| 73/73 [00:10<00:00,  6.72it/s][A
                                               [A 20%|██        | 82/405 [05:34<39:20,  7.31s/it] 20%|██        | 83/405 [05:38<33:40,  6.28s/it] 21%|██        | 84/405 [05:42<30:07,  5.63s/it] 21%|██        | 85/405 [05:46<27:40,  5.19s/it] 21%|██        | 86/405 [05:50<25:27,  4.79s/it] 21%|██▏       | 87/405 [05:54<24:20,  4.59s/it] 22%|██▏       | 88/405 [05:58<23:11,  4.39s/it] 22%|██▏       | 89/405 [06:02<22:43,  4.32s/it] 22%|██▏       | 90/405 [06:06<21:56,  4.18s/it] 22%|██▏       | 91/405 [06:10<21:38,  4.14s/it] 23%|██▎       | 92/405 [06:14<21:15,  4.07s/it] 23%|██▎       | 93/405 [06:18<20:55,  4.02s/it] 23%|██▎       | 94/405 [06:22<20:41,  3.99s/it] 23%|██▎       | 95/405 [06:26<20:35,  3.98s/it] 24%|██▎       | 96/405 [06:30<20:21,  3.95s/it] 24%|██▍       | 97/405 [06:34<20:20,  3.96s/it] 24%|██▍       | 98/405 [06:37<20:08,  3.94s/it] 24%|██▍       | 99/405 [06:41<20:19,  3.99s/it] 25%|██▍       | 100/405 [06:45<20:10,  3.97s/it] 25%|██▍       | 101/405 [06:49<19:50,  3.92s/it] 25%|██▌       | 102/405 [06:53<19:32,  3.87s/it] 25%|██▌       | 103/405 [06:57<19:36,  3.90s/it] 26%|██▌       | 104/405 [07:01<19:51,  3.96s/it] 26%|██▌       | 105/405 [07:05<19:29,  3.90s/it] 26%|██▌       | 106/405 [07:09<19:27,  3.90s/it] 26%|██▋       | 107/405 [07:13<19:38,  3.96s/it] 27%|██▋       | 108/405 [07:17<19:23,  3.92s/it] 27%|██▋       | 109/405 [07:21<19:46,  4.01s/it] 27%|██▋       | 110/405 [07:25<19:38,  3.99s/it] 27%|██▋       | 111/405 [07:29<19:29,  3.98s/it] 28%|██▊       | 112/405 [07:33<19:32,  4.00s/it] 28%|██▊       | 113/405 [07:37<19:35,  4.02s/it] 28%|██▊       | 114/405 [07:41<19:21,  3.99s/it] 28%|██▊       | 115/405 [07:45<19:20,  4.00s/it] 29%|██▊       | 116/405 [07:49<19:17,  4.00s/it] 29%|██▉       | 117/405 [07:53<19:03,  3.97s/it] 29%|██▉       | 118/405 [07:57<19:19,  4.04s/it] 29%|██▉       | 119/405 [08:01<19:12,  4.03s/it] 30%|██▉       | 120/405 [08:05<19:05,  4.02s/it] 30%|██▉       | 121/405 [08:09<18:46,  3.97s/it] 30%|███       | 122/405 [08:13<18:40,  3.96s/it] 30%|███       | 123/405 [08:17<18:32,  3.95s/it] 31%|███       | 124/405 [08:21<18:24,  3.93s/it] 31%|███       | 125/405 [08:24<18:19,  3.93s/it] 31%|███       | 126/405 [08:28<18:00,  3.87s/it] 31%|███▏      | 127/405 [08:32<18:13,  3.93s/it] 32%|███▏      | 128/405 [08:36<17:57,  3.89s/it] 32%|███▏      | 129/405 [08:40<17:46,  3.86s/it] 32%|███▏      | 130/405 [08:44<17:57,  3.92s/it] 32%|███▏      | 131/405 [08:48<18:04,  3.96s/it] 33%|███▎      | 132/405 [08:52<18:02,  3.97s/it] 33%|███▎      | 133/405 [08:56<17:44,  3.91s/it] 33%|███▎      | 134/405 [09:00<17:40,  3.91s/it] 33%|███▎      | 135/405 [09:04<17:34,  3.91s/it] 34%|███▎      | 136/405 [09:07<17:19,  3.87s/it] 34%|███▍      | 137/405 [09:11<17:17,  3.87s/it] 34%|███▍      | 138/405 [09:15<17:14,  3.87s/it] 34%|███▍      | 139/405 [09:19<17:05,  3.86s/it] 35%|███▍      | 140/405 [09:23<16:57,  3.84s/it] 35%|███▍      | 141/405 [09:27<17:03,  3.88s/it] 35%|███▌      | 142/405 [09:31<17:17,  3.95s/it] 35%|███▌      | 143/405 [09:35<17:07,  3.92s/it] 36%|███▌      | 144/405 [09:38<16:59,  3.91s/it] 36%|███▌      | 145/405 [09:42<17:04,  3.94s/it] 36%|███▌      | 146/405 [09:46<17:02,  3.95s/it] 36%|███▋      | 147/405 [09:50<16:53,  3.93s/it] 37%|███▋      | 148/405 [09:54<16:54,  3.95s/it] 37%|███▋      | 149/405 [09:58<16:52,  3.95s/it] 37%|███▋      | 150/405 [10:02<16:57,  3.99s/it] 37%|███▋      | 151/405 [10:06<16:55,  4.00s/it] 38%|███▊      | 152/405 [10:10<16:40,  3.95s/it] 38%|███▊      | 153/405 [10:14<16:32,  3.94s/it] 38%|███▊      | 154/405 [10:18<16:47,  4.02s/it] 38%|███▊      | 155/405 [10:22<16:45,  4.02s/it] 39%|███▊      | 156/405 [10:26<16:33,  3.99s/it] 39%|███▉      | 157/405 [10:30<16:11,  3.92s/it] 39%|███▉      | 158/405 [10:34<16:08,  3.92s/it] 39%|███▉      | 159/405 [10:38<16:17,  3.97s/it] 40%|███▉      | 160/405 [10:42<16:07,  3.95s/it] 40%|███▉      | 161/405 [10:46<16:05,  3.96s/it] 40%|████      | 162/405 [10:50<16:05,  3.97s/it][INFO|trainer.py:4117] 2025-01-12 22:27:38,609 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:27:38,609 >>   Num examples = 146
[INFO|trainer.py:4122] 2025-01-12 22:27:38,609 >>   Batch size = 1

  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:05, 13.18it/s][A
  5%|▌         | 4/73 [00:00<00:08,  8.05it/s][A
  7%|▋         | 5/73 [00:00<00:08,  7.67it/s][A
  8%|▊         | 6/73 [00:00<00:09,  7.44it/s][A
 10%|▉         | 7/73 [00:00<00:09,  7.06it/s][A
 11%|█         | 8/73 [00:01<00:09,  7.06it/s][A
 12%|█▏        | 9/73 [00:01<00:09,  6.95it/s][A
 14%|█▎        | 10/73 [00:01<00:09,  6.83it/s][A
 15%|█▌        | 11/73 [00:01<00:08,  6.89it/s][A
 16%|█▋        | 12/73 [00:01<00:08,  6.95it/s][A
 18%|█▊        | 13/73 [00:01<00:09,  6.50it/s][A
 19%|█▉        | 14/73 [00:01<00:09,  6.49it/s][A
 21%|██        | 15/73 [00:02<00:08,  6.69it/s][A
 22%|██▏       | 16/73 [00:02<00:08,  6.80it/s][A
 23%|██▎       | 17/73 [00:02<00:08,  6.50it/s][A
 25%|██▍       | 18/73 [00:02<00:08,  6.67it/s][A
 26%|██▌       | 19/73 [00:02<00:08,  6.51it/s][A
 27%|██▋       | 20/73 [00:02<00:07,  6.68it/s][A
 29%|██▉       | 21/73 [00:03<00:07,  6.86it/s][A
 30%|███       | 22/73 [00:03<00:07,  6.97it/s][A
 32%|███▏      | 23/73 [00:03<00:07,  6.58it/s][A
 33%|███▎      | 24/73 [00:03<00:07,  6.72it/s][A
 34%|███▍      | 25/73 [00:03<00:07,  6.61it/s][A
 36%|███▌      | 26/73 [00:03<00:06,  6.73it/s][A
 37%|███▋      | 27/73 [00:03<00:06,  6.83it/s][A
 38%|███▊      | 28/73 [00:04<00:06,  6.78it/s][A
 40%|███▉      | 29/73 [00:04<00:06,  6.34it/s][A
 41%|████      | 30/73 [00:04<00:06,  6.60it/s][A
 42%|████▏     | 31/73 [00:04<00:06,  6.61it/s][A
 44%|████▍     | 32/73 [00:04<00:06,  6.76it/s][A
 45%|████▌     | 33/73 [00:04<00:05,  6.87it/s][A
 47%|████▋     | 34/73 [00:04<00:05,  6.54it/s][A
 48%|████▊     | 35/73 [00:05<00:06,  5.92it/s][A
 49%|████▉     | 36/73 [00:05<00:05,  6.27it/s][A
 51%|█████     | 37/73 [00:05<00:05,  6.54it/s][A
 52%|█████▏    | 38/73 [00:05<00:05,  6.55it/s][A
 53%|█████▎    | 39/73 [00:05<00:05,  6.48it/s][A
 55%|█████▍    | 40/73 [00:05<00:05,  6.41it/s][A
 56%|█████▌    | 41/73 [00:06<00:05,  6.13it/s][A
 58%|█████▊    | 42/73 [00:06<00:04,  6.41it/s][A
 59%|█████▉    | 43/73 [00:06<00:04,  6.56it/s][A
 60%|██████    | 44/73 [00:06<00:04,  6.37it/s][A
 62%|██████▏   | 45/73 [00:06<00:04,  6.34it/s][A
 63%|██████▎   | 46/73 [00:06<00:04,  6.56it/s][A
 64%|██████▍   | 47/73 [00:06<00:03,  6.74it/s][A
 66%|██████▌   | 48/73 [00:07<00:03,  6.86it/s][A
 67%|██████▋   | 49/73 [00:07<00:03,  6.48it/s][A
 68%|██████▊   | 50/73 [00:07<00:03,  6.63it/s][A
 70%|██████▉   | 51/73 [00:07<00:03,  6.72it/s][A
 71%|███████   | 52/73 [00:07<00:03,  6.61it/s][A
 73%|███████▎  | 53/73 [00:07<00:03,  5.83it/s][A
 74%|███████▍  | 54/73 [00:08<00:03,  5.93it/s][A
 75%|███████▌  | 55/73 [00:08<00:03,  5.66it/s][A
 77%|███████▋  | 56/73 [00:08<00:02,  6.06it/s][A
 78%|███████▊  | 57/73 [00:08<00:02,  6.24it/s][A
 79%|███████▉  | 58/73 [00:08<00:02,  6.46it/s][A
 81%|████████  | 59/73 [00:08<00:02,  6.61it/s][A
 82%|████████▏ | 60/73 [00:09<00:02,  6.39it/s][A
 84%|████████▎ | 61/73 [00:09<00:01,  6.57it/s][A
 85%|████████▍ | 62/73 [00:09<00:01,  6.68it/s][A
 86%|████████▋ | 63/73 [00:09<00:01,  6.73it/s][A
 88%|████████▊ | 64/73 [00:09<00:01,  6.73it/s][A
 89%|████████▉ | 65/73 [00:09<00:01,  6.44it/s][A
 90%|█████████ | 66/73 [00:09<00:01,  6.39it/s][A
 92%|█████████▏| 67/73 [00:10<00:00,  6.36it/s][A
 93%|█████████▎| 68/73 [00:10<00:00,  6.61it/s][A
 95%|█████████▍| 69/73 [00:10<00:00,  6.58it/s][A
 96%|█████████▌| 70/73 [00:10<00:00,  6.75it/s][A
 97%|█████████▋| 71/73 [00:10<00:00,  6.46it/s][A
 99%|█████████▊| 72/73 [00:10<00:00,  6.50it/s][A
100%|██████████| 73/73 [00:11<00:00,  6.70it/s][A                                                 
                                               [A{'eval_loss': 0.0007305827457457781, 'eval_runtime': 11.2187, 'eval_samples_per_second': 13.014, 'eval_steps_per_second': 6.507, 'epoch': 1.99}
 40%|████      | 162/405 [11:02<16:05,  3.97s/it]
100%|██████████| 73/73 [00:11<00:00,  6.70it/s][A
                                               [A 40%|████      | 163/405 [11:05<29:26,  7.30s/it] 40%|████      | 164/405 [11:09<25:24,  6.33s/it] 41%|████      | 165/405 [11:13<22:16,  5.57s/it] 41%|████      | 166/405 [11:17<20:14,  5.08s/it] 41%|████      | 167/405 [11:21<18:45,  4.73s/it] 41%|████▏     | 168/405 [11:25<17:42,  4.48s/it] 42%|████▏     | 169/405 [11:29<16:59,  4.32s/it] 42%|████▏     | 170/405 [11:33<16:32,  4.23s/it] 42%|████▏     | 171/405 [11:36<16:04,  4.12s/it] 42%|████▏     | 172/405 [11:41<16:01,  4.13s/it] 43%|████▎     | 173/405 [11:44<15:38,  4.04s/it] 43%|████▎     | 174/405 [11:48<15:32,  4.04s/it] 43%|████▎     | 175/405 [11:52<15:20,  4.00s/it] 43%|████▎     | 176/405 [11:56<15:23,  4.03s/it] 44%|████▎     | 177/405 [12:00<15:10,  3.99s/it] 44%|████▍     | 178/405 [12:04<14:54,  3.94s/it] 44%|████▍     | 179/405 [12:08<14:45,  3.92s/it] 44%|████▍     | 180/405 [12:12<14:43,  3.93s/it] 45%|████▍     | 181/405 [12:16<14:38,  3.92s/it] 45%|████▍     | 182/405 [12:20<14:30,  3.90s/it] 45%|████▌     | 183/405 [12:24<14:23,  3.89s/it] 45%|████▌     | 184/405 [12:27<14:16,  3.88s/it] 46%|████▌     | 185/405 [12:31<14:14,  3.88s/it] 46%|████▌     | 186/405 [12:35<14:03,  3.85s/it] 46%|████▌     | 187/405 [12:39<13:56,  3.84s/it] 46%|████▋     | 188/405 [12:43<14:01,  3.88s/it] 47%|████▋     | 189/405 [12:47<14:12,  3.95s/it] 47%|████▋     | 190/405 [12:51<13:59,  3.90s/it] 47%|████▋     | 191/405 [12:55<13:57,  3.92s/it] 47%|████▋     | 192/405 [12:59<14:03,  3.96s/it] 48%|████▊     | 193/405 [13:03<13:53,  3.93s/it] 48%|████▊     | 194/405 [13:07<13:50,  3.94s/it] 48%|████▊     | 195/405 [13:11<13:45,  3.93s/it] 48%|████▊     | 196/405 [13:15<13:42,  3.93s/it] 49%|████▊     | 197/405 [13:19<14:12,  4.10s/it] 49%|████▉     | 198/405 [13:23<14:06,  4.09s/it] 49%|████▉     | 199/405 [13:27<13:50,  4.03s/it] 49%|████▉     | 200/405 [13:31<13:34,  3.97s/it]                                                 {'loss': 0.1954, 'grad_norm': 0.0060335383750498295, 'learning_rate': 5.986030470739811e-05, 'epoch': 2.46}
 49%|████▉     | 200/405 [13:31<13:34,  3.97s/it] 50%|████▉     | 201/405 [13:35<13:46,  4.05s/it] 50%|████▉     | 202/405 [13:39<13:32,  4.00s/it] 50%|█████     | 203/405 [13:43<13:22,  3.97s/it] 50%|█████     | 204/405 [13:47<13:22,  3.99s/it] 51%|█████     | 205/405 [13:51<13:20,  4.00s/it] 51%|█████     | 206/405 [13:55<13:28,  4.06s/it] 51%|█████     | 207/405 [13:59<13:09,  3.99s/it] 51%|█████▏    | 208/405 [14:03<13:03,  3.98s/it] 52%|█████▏    | 209/405 [14:07<12:54,  3.95s/it] 52%|█████▏    | 210/405 [14:11<12:43,  3.91s/it] 52%|█████▏    | 211/405 [14:15<12:44,  3.94s/it] 52%|█████▏    | 212/405 [14:18<12:32,  3.90s/it] 53%|█████▎    | 213/405 [14:22<12:37,  3.95s/it] 53%|█████▎    | 214/405 [14:26<12:36,  3.96s/it] 53%|█████▎    | 215/405 [14:31<12:58,  4.10s/it] 53%|█████▎    | 216/405 [14:35<12:47,  4.06s/it] 54%|█████▎    | 217/405 [14:39<12:28,  3.98s/it] 54%|█████▍    | 218/405 [14:43<12:27,  4.00s/it] 54%|█████▍    | 219/405 [14:47<12:36,  4.07s/it] 54%|█████▍    | 220/405 [14:51<12:32,  4.07s/it] 55%|█████▍    | 221/405 [14:55<12:22,  4.03s/it] 55%|█████▍    | 222/405 [14:59<12:12,  4.00s/it] 55%|█████▌    | 223/405 [15:03<11:56,  3.94s/it] 55%|█████▌    | 224/405 [15:07<11:53,  3.94s/it] 56%|█████▌    | 225/405 [15:10<11:46,  3.93s/it] 56%|█████▌    | 226/405 [15:15<11:50,  3.97s/it] 56%|█████▌    | 227/405 [15:18<11:40,  3.94s/it] 56%|█████▋    | 228/405 [15:22<11:32,  3.91s/it] 57%|█████▋    | 229/405 [15:27<11:46,  4.01s/it] 57%|█████▋    | 230/405 [15:31<11:43,  4.02s/it] 57%|█████▋    | 231/405 [15:34<11:35,  4.00s/it] 57%|█████▋    | 232/405 [15:39<11:38,  4.04s/it] 58%|█████▊    | 233/405 [15:43<11:37,  4.06s/it] 58%|█████▊    | 234/405 [15:47<11:21,  3.99s/it] 58%|█████▊    | 235/405 [15:51<11:28,  4.05s/it] 58%|█████▊    | 236/405 [15:55<11:10,  3.97s/it] 59%|█████▊    | 237/405 [15:59<11:07,  3.98s/it] 59%|█████▉    | 238/405 [16:03<11:05,  3.98s/it] 59%|█████▉    | 239/405 [16:06<10:55,  3.95s/it] 59%|█████▉    | 240/405 [16:11<11:01,  4.01s/it] 60%|█████▉    | 241/405 [16:14<10:46,  3.94s/it] 60%|█████▉    | 242/405 [16:18<10:42,  3.94s/it] 60%|██████    | 243/405 [16:22<10:34,  3.91s/it][INFO|trainer.py:4117] 2025-01-12 22:33:11,325 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:33:11,325 >>   Num examples = 146
[INFO|trainer.py:4122] 2025-01-12 22:33:11,325 >>   Batch size = 1

  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:05, 12.65it/s][A
  5%|▌         | 4/73 [00:00<00:08,  7.80it/s][A
  7%|▋         | 5/73 [00:00<00:09,  7.53it/s][A
  8%|▊         | 6/73 [00:00<00:09,  7.39it/s][A
 10%|▉         | 7/73 [00:00<00:09,  7.03it/s][A
 11%|█         | 8/73 [00:01<00:09,  7.01it/s][A
 12%|█▏        | 9/73 [00:01<00:09,  6.91it/s][A
 14%|█▎        | 10/73 [00:01<00:09,  6.76it/s][A
 15%|█▌        | 11/73 [00:01<00:09,  6.80it/s][A
 16%|█▋        | 12/73 [00:01<00:08,  6.92it/s][A
 18%|█▊        | 13/73 [00:01<00:09,  6.48it/s][A
 19%|█▉        | 14/73 [00:01<00:09,  6.51it/s][A
 21%|██        | 15/73 [00:02<00:08,  6.72it/s][A
 22%|██▏       | 16/73 [00:02<00:08,  6.85it/s][A
 23%|██▎       | 17/73 [00:02<00:08,  6.56it/s][A
 25%|██▍       | 18/73 [00:02<00:08,  6.66it/s][A
 26%|██▌       | 19/73 [00:02<00:08,  6.51it/s][A
 27%|██▋       | 20/73 [00:02<00:07,  6.67it/s][A
 29%|██▉       | 21/73 [00:03<00:07,  6.86it/s][A
 30%|███       | 22/73 [00:03<00:07,  6.95it/s][A
 32%|███▏      | 23/73 [00:03<00:07,  6.61it/s][A
 33%|███▎      | 24/73 [00:03<00:07,  6.73it/s][A
 34%|███▍      | 25/73 [00:03<00:07,  6.60it/s][A
 36%|███▌      | 26/73 [00:03<00:07,  6.69it/s][A
 37%|███▋      | 27/73 [00:03<00:06,  6.77it/s][A
 38%|███▊      | 28/73 [00:04<00:06,  6.73it/s][A
 40%|███▉      | 29/73 [00:04<00:06,  6.31it/s][A
 41%|████      | 30/73 [00:04<00:06,  6.57it/s][A
 42%|████▏     | 31/73 [00:04<00:06,  6.59it/s][A
 44%|████▍     | 32/73 [00:04<00:06,  6.76it/s][A
 45%|████▌     | 33/73 [00:04<00:05,  6.85it/s][A
 47%|████▋     | 34/73 [00:04<00:05,  6.50it/s][A
 48%|████▊     | 35/73 [00:05<00:06,  5.89it/s][A
 49%|████▉     | 36/73 [00:05<00:05,  6.27it/s][A
 51%|█████     | 37/73 [00:05<00:05,  6.50it/s][A
 52%|█████▏    | 38/73 [00:05<00:05,  6.49it/s][A
 53%|█████▎    | 39/73 [00:05<00:05,  6.44it/s][A
 55%|█████▍    | 40/73 [00:05<00:05,  6.39it/s][A
 56%|█████▌    | 41/73 [00:06<00:05,  6.12it/s][A
 58%|█████▊    | 42/73 [00:06<00:04,  6.42it/s][A
 59%|█████▉    | 43/73 [00:06<00:04,  6.63it/s][A
 60%|██████    | 44/73 [00:06<00:04,  6.45it/s][A
 62%|██████▏   | 45/73 [00:06<00:04,  6.41it/s][A
 63%|██████▎   | 46/73 [00:06<00:04,  6.61it/s][A
 64%|██████▍   | 47/73 [00:07<00:03,  6.78it/s][A
 66%|██████▌   | 48/73 [00:07<00:03,  6.87it/s][A
 67%|██████▋   | 49/73 [00:07<00:03,  6.49it/s][A
 68%|██████▊   | 50/73 [00:07<00:03,  6.65it/s][A
 70%|██████▉   | 51/73 [00:07<00:03,  6.72it/s][A
 71%|███████   | 52/73 [00:07<00:03,  6.60it/s][A
 73%|███████▎  | 53/73 [00:07<00:03,  5.79it/s][A
 74%|███████▍  | 54/73 [00:08<00:03,  5.90it/s][A
 75%|███████▌  | 55/73 [00:08<00:03,  5.65it/s][A
 77%|███████▋  | 56/73 [00:08<00:02,  6.05it/s][A
 78%|███████▊  | 57/73 [00:08<00:02,  6.23it/s][A
 79%|███████▉  | 58/73 [00:08<00:02,  6.51it/s][A
 81%|████████  | 59/73 [00:08<00:02,  6.68it/s][A
 82%|████████▏ | 60/73 [00:09<00:02,  6.46it/s][A
 84%|████████▎ | 61/73 [00:09<00:01,  6.63it/s][A
 85%|████████▍ | 62/73 [00:09<00:01,  6.77it/s][A
 86%|████████▋ | 63/73 [00:09<00:01,  6.82it/s][A
 88%|████████▊ | 64/73 [00:09<00:01,  6.80it/s][A
 89%|████████▉ | 65/73 [00:09<00:01,  6.50it/s][A
 90%|█████████ | 66/73 [00:09<00:01,  6.43it/s][A
 92%|█████████▏| 67/73 [00:10<00:00,  6.39it/s][A
 93%|█████████▎| 68/73 [00:10<00:00,  6.61it/s][A
 95%|█████████▍| 69/73 [00:10<00:00,  6.55it/s][A
 96%|█████████▌| 70/73 [00:10<00:00,  6.72it/s][A
 97%|█████████▋| 71/73 [00:10<00:00,  6.44it/s][A
 99%|█████████▊| 72/73 [00:10<00:00,  6.48it/s][A
100%|██████████| 73/73 [00:11<00:00,  6.67it/s][A                                                 
                                               [A{'eval_loss': 0.0004476456670090556, 'eval_runtime': 11.1746, 'eval_samples_per_second': 13.065, 'eval_steps_per_second': 6.533, 'epoch': 2.99}
 60%|██████    | 243/405 [16:35<10:34,  3.91s/it]
100%|██████████| 73/73 [00:11<00:00,  6.67it/s][A
                                               [A 60%|██████    | 244/405 [16:37<19:35,  7.30s/it] 60%|██████    | 245/405 [16:41<16:46,  6.29s/it] 61%|██████    | 246/405 [16:45<15:00,  5.67s/it] 61%|██████    | 247/405 [16:49<13:35,  5.16s/it] 61%|██████    | 248/405 [16:54<12:44,  4.87s/it] 61%|██████▏   | 249/405 [16:58<12:01,  4.63s/it] 62%|██████▏   | 250/405 [17:02<11:32,  4.47s/it] 62%|██████▏   | 251/405 [17:06<11:03,  4.31s/it] 62%|██████▏   | 252/405 [17:10<10:37,  4.17s/it] 62%|██████▏   | 253/405 [17:14<10:36,  4.19s/it] 63%|██████▎   | 254/405 [17:18<10:15,  4.08s/it] 63%|██████▎   | 255/405 [17:22<10:11,  4.07s/it] 63%|██████▎   | 256/405 [17:26<09:59,  4.03s/it] 63%|██████▎   | 257/405 [17:29<09:50,  3.99s/it] 64%|██████▎   | 258/405 [17:34<09:47,  4.00s/it] 64%|██████▍   | 259/405 [17:37<09:35,  3.94s/it] 64%|██████▍   | 260/405 [17:41<09:36,  3.98s/it] 64%|██████▍   | 261/405 [17:45<09:34,  3.99s/it] 65%|██████▍   | 262/405 [17:49<09:25,  3.96s/it] 65%|██████▍   | 263/405 [17:53<09:26,  3.99s/it] 65%|██████▌   | 264/405 [17:57<09:19,  3.97s/it] 65%|██████▌   | 265/405 [18:01<09:17,  3.99s/it] 66%|██████▌   | 266/405 [18:05<09:07,  3.94s/it] 66%|██████▌   | 267/405 [18:09<09:07,  3.96s/it] 66%|██████▌   | 268/405 [18:13<08:57,  3.92s/it] 66%|██████▋   | 269/405 [18:17<08:54,  3.93s/it] 67%|██████▋   | 270/405 [18:21<08:57,  3.98s/it] 67%|██████▋   | 271/405 [18:25<08:51,  3.96s/it] 67%|██████▋   | 272/405 [18:29<08:45,  3.95s/it] 67%|██████▋   | 273/405 [18:33<08:44,  3.98s/it] 68%|██████▊   | 274/405 [18:37<08:42,  3.99s/it] 68%|██████▊   | 275/405 [18:41<08:51,  4.09s/it] 68%|██████▊   | 276/405 [18:45<08:39,  4.02s/it] 68%|██████▊   | 277/405 [18:49<08:31,  3.99s/it] 69%|██████▊   | 278/405 [18:53<08:20,  3.94s/it] 69%|██████▉   | 279/405 [18:57<08:12,  3.91s/it] 69%|██████▉   | 280/405 [19:01<08:11,  3.94s/it] 69%|██████▉   | 281/405 [19:05<08:08,  3.94s/it] 70%|██████▉   | 282/405 [19:08<07:57,  3.88s/it] 70%|██████▉   | 283/405 [19:12<07:57,  3.92s/it] 70%|███████   | 284/405 [19:16<07:59,  3.97s/it] 70%|███████   | 285/405 [19:20<07:51,  3.93s/it] 71%|███████   | 286/405 [19:24<07:41,  3.88s/it] 71%|███████   | 287/405 [19:28<07:48,  3.97s/it] 71%|███████   | 288/405 [19:32<07:44,  3.97s/it] 71%|███████▏  | 289/405 [19:36<07:36,  3.93s/it] 72%|███████▏  | 290/405 [19:40<07:35,  3.96s/it] 72%|███████▏  | 291/405 [19:44<07:28,  3.94s/it] 72%|███████▏  | 292/405 [19:48<07:28,  3.97s/it] 72%|███████▏  | 293/405 [19:52<07:26,  3.98s/it] 73%|███████▎  | 294/405 [19:56<07:16,  3.93s/it] 73%|███████▎  | 295/405 [20:00<07:11,  3.92s/it] 73%|███████▎  | 296/405 [20:04<07:04,  3.90s/it] 73%|███████▎  | 297/405 [20:08<07:06,  3.95s/it] 74%|███████▎  | 298/405 [20:12<07:02,  3.95s/it] 74%|███████▍  | 299/405 [20:16<06:59,  3.95s/it] 74%|███████▍  | 300/405 [20:19<06:53,  3.94s/it] 74%|███████▍  | 301/405 [20:23<06:51,  3.95s/it] 75%|███████▍  | 302/405 [20:27<06:47,  3.95s/it] 75%|███████▍  | 303/405 [20:31<06:39,  3.91s/it] 75%|███████▌  | 304/405 [20:35<06:32,  3.89s/it] 75%|███████▌  | 305/405 [20:39<06:33,  3.94s/it] 76%|███████▌  | 306/405 [20:43<06:31,  3.95s/it] 76%|███████▌  | 307/405 [20:47<06:26,  3.94s/it] 76%|███████▌  | 308/405 [20:51<06:25,  3.98s/it] 76%|███████▋  | 309/405 [20:55<06:23,  3.99s/it] 77%|███████▋  | 310/405 [20:59<06:15,  3.95s/it] 77%|███████▋  | 311/405 [21:03<06:09,  3.93s/it] 77%|███████▋  | 312/405 [21:07<06:03,  3.91s/it] 77%|███████▋  | 313/405 [21:10<05:55,  3.86s/it] 78%|███████▊  | 314/405 [21:14<05:53,  3.88s/it] 78%|███████▊  | 315/405 [21:18<05:49,  3.89s/it] 78%|███████▊  | 316/405 [21:22<05:47,  3.90s/it] 78%|███████▊  | 317/405 [21:26<05:47,  3.94s/it] 79%|███████▊  | 318/405 [21:30<05:43,  3.95s/it] 79%|███████▉  | 319/405 [21:34<05:40,  3.96s/it] 79%|███████▉  | 320/405 [21:38<05:37,  3.97s/it] 79%|███████▉  | 321/405 [21:42<05:35,  3.99s/it] 80%|███████▉  | 322/405 [21:46<05:25,  3.92s/it] 80%|███████▉  | 323/405 [21:50<05:22,  3.94s/it] 80%|████████  | 324/405 [21:54<05:16,  3.91s/it][INFO|trainer.py:4117] 2025-01-12 22:38:43,802 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:38:43,802 >>   Num examples = 146
[INFO|trainer.py:4122] 2025-01-12 22:38:43,802 >>   Batch size = 1

  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:05, 13.00it/s][A
  5%|▌         | 4/73 [00:00<00:08,  7.90it/s][A
  7%|▋         | 5/73 [00:00<00:08,  7.60it/s][A
  8%|▊         | 6/73 [00:00<00:09,  7.44it/s][A
 10%|▉         | 7/73 [00:00<00:09,  7.07it/s][A
 11%|█         | 8/73 [00:01<00:09,  7.07it/s][A
 12%|█▏        | 9/73 [00:01<00:09,  6.96it/s][A
 14%|█▎        | 10/73 [00:01<00:09,  6.82it/s][A
 15%|█▌        | 11/73 [00:01<00:09,  6.84it/s][A
 16%|█▋        | 12/73 [00:01<00:08,  6.90it/s][A
 18%|█▊        | 13/73 [00:01<00:09,  6.47it/s][A
 19%|█▉        | 14/73 [00:01<00:09,  6.47it/s][A
 21%|██        | 15/73 [00:02<00:08,  6.65it/s][A
 22%|██▏       | 16/73 [00:02<00:08,  6.78it/s][A
 23%|██▎       | 17/73 [00:02<00:08,  6.50it/s][A
 25%|██▍       | 18/73 [00:02<00:08,  6.66it/s][A
 26%|██▌       | 19/73 [00:02<00:08,  6.51it/s][A
 27%|██▋       | 20/73 [00:02<00:07,  6.64it/s][A
 29%|██▉       | 21/73 [00:03<00:07,  6.84it/s][A
 30%|███       | 22/73 [00:03<00:07,  6.88it/s][A
 32%|███▏      | 23/73 [00:03<00:07,  6.50it/s][A
 33%|███▎      | 24/73 [00:03<00:07,  6.66it/s][A
 34%|███▍      | 25/73 [00:03<00:07,  6.57it/s][A
 36%|███▌      | 26/73 [00:03<00:07,  6.70it/s][A
 37%|███▋      | 27/73 [00:03<00:06,  6.81it/s][A
 38%|███▊      | 28/73 [00:04<00:06,  6.76it/s][A
 40%|███▉      | 29/73 [00:04<00:06,  6.33it/s][A
 41%|████      | 30/73 [00:04<00:06,  6.60it/s][A
 42%|████▏     | 31/73 [00:04<00:06,  6.61it/s][A
 44%|████▍     | 32/73 [00:04<00:06,  6.80it/s][A
 45%|████▌     | 33/73 [00:04<00:05,  6.92it/s][A
 47%|████▋     | 34/73 [00:04<00:05,  6.56it/s][A
 48%|████▊     | 35/73 [00:05<00:06,  5.93it/s][A
 49%|████▉     | 36/73 [00:05<00:05,  6.28it/s][A
 51%|█████     | 37/73 [00:05<00:05,  6.55it/s][A
 52%|█████▏    | 38/73 [00:05<00:05,  6.55it/s][A
 53%|█████▎    | 39/73 [00:05<00:05,  6.49it/s][A
 55%|█████▍    | 40/73 [00:05<00:05,  6.42it/s][A
 56%|█████▌    | 41/73 [00:06<00:05,  6.14it/s][A
 58%|█████▊    | 42/73 [00:06<00:04,  6.43it/s][A
 59%|█████▉    | 43/73 [00:06<00:04,  6.59it/s][A
 60%|██████    | 44/73 [00:06<00:04,  6.40it/s][A
 62%|██████▏   | 45/73 [00:06<00:04,  6.38it/s][A
 63%|██████▎   | 46/73 [00:06<00:04,  6.59it/s][A
 64%|██████▍   | 47/73 [00:06<00:03,  6.77it/s][A
 66%|██████▌   | 48/73 [00:07<00:03,  6.89it/s][A
 67%|██████▋   | 49/73 [00:07<00:03,  6.50it/s][A
 68%|██████▊   | 50/73 [00:07<00:03,  6.65it/s][A
 70%|██████▉   | 51/73 [00:07<00:03,  6.72it/s][A
 71%|███████   | 52/73 [00:07<00:03,  6.61it/s][A
 73%|███████▎  | 53/73 [00:07<00:03,  5.79it/s][A
 74%|███████▍  | 54/73 [00:08<00:03,  5.90it/s][A
 75%|███████▌  | 55/73 [00:08<00:03,  5.65it/s][A
 77%|███████▋  | 56/73 [00:08<00:02,  6.05it/s][A
 78%|███████▊  | 57/73 [00:08<00:02,  6.24it/s][A
 79%|███████▉  | 58/73 [00:08<00:02,  6.51it/s][A
 81%|████████  | 59/73 [00:08<00:02,  6.66it/s][A
 82%|████████▏ | 60/73 [00:09<00:02,  6.41it/s][A
 84%|████████▎ | 61/73 [00:09<00:01,  6.59it/s][A
 85%|████████▍ | 62/73 [00:09<00:01,  6.73it/s][A
 86%|████████▋ | 63/73 [00:09<00:01,  6.80it/s][A
 88%|████████▊ | 64/73 [00:09<00:01,  6.78it/s][A
 89%|████████▉ | 65/73 [00:09<00:01,  6.47it/s][A
 90%|█████████ | 66/73 [00:09<00:01,  6.41it/s][A
 92%|█████████▏| 67/73 [00:10<00:00,  6.37it/s][A
 93%|█████████▎| 68/73 [00:10<00:00,  6.61it/s][A
 95%|█████████▍| 69/73 [00:10<00:00,  6.59it/s][A
 96%|█████████▌| 70/73 [00:10<00:00,  6.75it/s][A
 97%|█████████▋| 71/73 [00:10<00:00,  6.45it/s][A
 99%|█████████▊| 72/73 [00:10<00:00,  6.49it/s][A
100%|██████████| 73/73 [00:11<00:00,  6.65it/s][A                                                 
                                               [A{'eval_loss': 0.0004214464861433953, 'eval_runtime': 11.1714, 'eval_samples_per_second': 13.069, 'eval_steps_per_second': 6.535, 'epoch': 3.99}
 80%|████████  | 324/405 [22:07<05:16,  3.91s/it]
100%|██████████| 73/73 [00:11<00:00,  6.65it/s][A
                                               [A 80%|████████  | 325/405 [22:09<09:46,  7.33s/it] 80%|████████  | 326/405 [22:13<08:18,  6.31s/it] 81%|████████  | 327/405 [22:17<07:16,  5.60s/it] 81%|████████  | 328/405 [22:21<06:34,  5.12s/it] 81%|████████  | 329/405 [22:25<06:04,  4.80s/it] 81%|████████▏ | 330/405 [22:29<05:39,  4.52s/it] 82%|████████▏ | 331/405 [22:33<05:23,  4.37s/it] 82%|████████▏ | 332/405 [22:37<05:11,  4.26s/it] 82%|████████▏ | 333/405 [22:41<05:02,  4.20s/it] 82%|████████▏ | 334/405 [22:45<04:54,  4.14s/it] 83%|████████▎ | 335/405 [22:49<04:44,  4.07s/it] 83%|████████▎ | 336/405 [22:53<04:39,  4.05s/it] 83%|████████▎ | 337/405 [22:57<04:33,  4.02s/it] 83%|████████▎ | 338/405 [23:01<04:32,  4.06s/it] 84%|████████▎ | 339/405 [23:05<04:25,  4.03s/it] 84%|████████▍ | 340/405 [23:09<04:18,  3.98s/it] 84%|████████▍ | 341/405 [23:13<04:11,  3.93s/it] 84%|████████▍ | 342/405 [23:17<04:07,  3.93s/it] 85%|████████▍ | 343/405 [23:21<04:05,  3.96s/it] 85%|████████▍ | 344/405 [23:25<04:00,  3.94s/it] 85%|████████▌ | 345/405 [23:29<03:57,  3.96s/it] 85%|████████▌ | 346/405 [23:32<03:51,  3.93s/it] 86%|████████▌ | 347/405 [23:36<03:50,  3.97s/it] 86%|████████▌ | 348/405 [23:41<03:49,  4.03s/it] 86%|████████▌ | 349/405 [23:45<03:48,  4.08s/it] 86%|████████▋ | 350/405 [23:49<03:41,  4.02s/it] 87%|████████▋ | 351/405 [23:53<03:38,  4.04s/it] 87%|████████▋ | 352/405 [23:57<03:30,  3.97s/it] 87%|████████▋ | 353/405 [24:00<03:25,  3.95s/it] 87%|████████▋ | 354/405 [24:04<03:21,  3.95s/it] 88%|████████▊ | 355/405 [24:08<03:15,  3.91s/it] 88%|████████▊ | 356/405 [24:12<03:14,  3.96s/it] 88%|████████▊ | 357/405 [24:16<03:09,  3.96s/it] 88%|████████▊ | 358/405 [24:20<03:05,  3.94s/it] 89%|████████▊ | 359/405 [24:24<03:01,  3.95s/it] 89%|████████▉ | 360/405 [24:28<02:56,  3.93s/it] 89%|████████▉ | 361/405 [24:32<02:55,  4.00s/it] 89%|████████▉ | 362/405 [24:36<02:48,  3.93s/it] 90%|████████▉ | 363/405 [24:40<02:47,  3.99s/it] 90%|████████▉ | 364/405 [24:44<02:42,  3.98s/it] 90%|█████████ | 365/405 [24:48<02:39,  3.99s/it] 90%|█████████ | 366/405 [24:52<02:34,  3.97s/it] 91%|█████████ | 367/405 [24:56<02:31,  3.98s/it] 91%|█████████ | 368/405 [25:00<02:26,  3.96s/it] 91%|█████████ | 369/405 [25:04<02:22,  3.95s/it] 91%|█████████▏| 370/405 [25:08<02:21,  4.04s/it] 92%|█████████▏| 371/405 [25:12<02:15,  3.97s/it] 92%|█████████▏| 372/405 [25:16<02:12,  4.00s/it] 92%|█████████▏| 373/405 [25:20<02:07,  3.98s/it] 92%|█████████▏| 374/405 [25:24<02:02,  3.96s/it] 93%|█████████▎| 375/405 [25:28<01:58,  3.97s/it] 93%|█████████▎| 376/405 [25:32<01:56,  4.00s/it] 93%|█████████▎| 377/405 [25:36<01:51,  3.98s/it] 93%|█████████▎| 378/405 [25:40<01:47,  3.99s/it] 94%|█████████▎| 379/405 [25:44<01:43,  3.98s/it] 94%|█████████▍| 380/405 [25:48<01:39,  3.97s/it] 94%|█████████▍| 381/405 [25:52<01:36,  4.03s/it] 94%|█████████▍| 382/405 [25:56<01:32,  4.02s/it] 95%|█████████▍| 383/405 [26:00<01:28,  4.03s/it] 95%|█████████▍| 384/405 [26:04<01:23,  3.98s/it] 95%|█████████▌| 385/405 [26:08<01:18,  3.94s/it] 95%|█████████▌| 386/405 [26:12<01:15,  3.97s/it] 96%|█████████▌| 387/405 [26:16<01:12,  4.02s/it] 96%|█████████▌| 388/405 [26:20<01:07,  3.95s/it] 96%|█████████▌| 389/405 [26:24<01:03,  3.95s/it] 96%|█████████▋| 390/405 [26:28<00:59,  3.97s/it] 97%|█████████▋| 391/405 [26:31<00:55,  3.95s/it] 97%|█████████▋| 392/405 [26:35<00:51,  3.97s/it] 97%|█████████▋| 393/405 [26:39<00:47,  3.93s/it] 97%|█████████▋| 394/405 [26:43<00:42,  3.91s/it] 98%|█████████▊| 395/405 [26:47<00:38,  3.90s/it] 98%|█████████▊| 396/405 [26:51<00:34,  3.88s/it] 98%|█████████▊| 397/405 [26:55<00:31,  3.89s/it] 98%|█████████▊| 398/405 [26:59<00:27,  3.93s/it] 99%|█████████▊| 399/405 [27:03<00:23,  3.92s/it] 99%|█████████▉| 400/405 [27:06<00:19,  3.88s/it]                                                 {'loss': 0.0002, 'grad_norm': 0.0009569362155161798, 'learning_rate': 4.6548918743033464e-08, 'epoch': 4.92}
 99%|█████████▉| 400/405 [27:07<00:19,  3.88s/it] 99%|█████████▉| 401/405 [27:11<00:15,  3.93s/it] 99%|█████████▉| 402/405 [27:15<00:12,  4.01s/it]100%|█████████▉| 403/405 [27:19<00:08,  4.01s/it]100%|█████████▉| 404/405 [27:23<00:03,  3.97s/it]100%|██████████| 405/405 [27:26<00:00,  3.92s/it][INFO|trainer.py:3801] 2025-01-12 22:44:14,254 >> Saving model checkpoint to saves/qwen-14b-ka-e5/lora/sft/checkpoint-405
[INFO|configuration_utils.py:677] 2025-01-12 22:44:14,274 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 22:44:14,274 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2025-01-12 22:44:14,397 >> tokenizer config file saved in saves/qwen-14b-ka-e5/lora/sft/checkpoint-405/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-12 22:44:14,397 >> Special tokens file saved in saves/qwen-14b-ka-e5/lora/sft/checkpoint-405/special_tokens_map.json
[INFO|trainer.py:4117] 2025-01-12 22:44:14,639 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:44:14,639 >>   Num examples = 146
[INFO|trainer.py:4122] 2025-01-12 22:44:14,639 >>   Batch size = 1

  0%|          | 0/73 [00:00<?, ?it/s][A
  3%|▎         | 2/73 [00:00<00:05, 13.29it/s][A
  5%|▌         | 4/73 [00:00<00:08,  8.08it/s][A
  7%|▋         | 5/73 [00:00<00:08,  7.75it/s][A
  8%|▊         | 6/73 [00:00<00:08,  7.56it/s][A
 10%|▉         | 7/73 [00:00<00:09,  7.13it/s][A
 11%|█         | 8/73 [00:01<00:09,  7.08it/s][A
 12%|█▏        | 9/73 [00:01<00:09,  6.96it/s][A
 14%|█▎        | 10/73 [00:01<00:09,  6.77it/s][A
 15%|█▌        | 11/73 [00:01<00:09,  6.81it/s][A
 16%|█▋        | 12/73 [00:01<00:08,  6.89it/s][A
 18%|█▊        | 13/73 [00:01<00:09,  6.46it/s][A
 19%|█▉        | 14/73 [00:01<00:09,  6.50it/s][A
 21%|██        | 15/73 [00:02<00:08,  6.72it/s][A
 22%|██▏       | 16/73 [00:02<00:08,  6.85it/s][A
 23%|██▎       | 17/73 [00:02<00:08,  6.57it/s][A
 25%|██▍       | 18/73 [00:02<00:08,  6.75it/s][A
 26%|██▌       | 19/73 [00:02<00:08,  6.56it/s][A
 27%|██▋       | 20/73 [00:02<00:07,  6.74it/s][A
 29%|██▉       | 21/73 [00:02<00:07,  6.91it/s][A
 30%|███       | 22/73 [00:03<00:07,  6.99it/s][A
 32%|███▏      | 23/73 [00:03<00:07,  6.61it/s][A
 33%|███▎      | 24/73 [00:03<00:07,  6.71it/s][A
 34%|███▍      | 25/73 [00:03<00:07,  6.56it/s][A
 36%|███▌      | 26/73 [00:03<00:07,  6.67it/s][A
 37%|███▋      | 27/73 [00:03<00:06,  6.79it/s][A
 38%|███▊      | 28/73 [00:04<00:06,  6.75it/s][A
 40%|███▉      | 29/73 [00:04<00:06,  6.32it/s][A
 41%|████      | 30/73 [00:04<00:06,  6.57it/s][A
 42%|████▏     | 31/73 [00:04<00:06,  6.59it/s][A
 44%|████▍     | 32/73 [00:04<00:06,  6.77it/s][A
 45%|████▌     | 33/73 [00:04<00:05,  6.90it/s][A
 47%|████▋     | 34/73 [00:04<00:05,  6.56it/s][A
 48%|████▊     | 35/73 [00:05<00:06,  5.93it/s][A
 49%|████▉     | 36/73 [00:05<00:05,  6.30it/s][A
 51%|█████     | 37/73 [00:05<00:05,  6.56it/s][A
 52%|█████▏    | 38/73 [00:05<00:05,  6.56it/s][A
 53%|█████▎    | 39/73 [00:05<00:05,  6.47it/s][A
 55%|█████▍    | 40/73 [00:05<00:05,  6.40it/s][A
 56%|█████▌    | 41/73 [00:06<00:05,  6.14it/s][A
 58%|█████▊    | 42/73 [00:06<00:04,  6.43it/s][A
 59%|█████▉    | 43/73 [00:06<00:04,  6.59it/s][A
 60%|██████    | 44/73 [00:06<00:04,  6.37it/s][A
 62%|██████▏   | 45/73 [00:06<00:04,  6.34it/s][A
 63%|██████▎   | 46/73 [00:06<00:04,  6.55it/s][A
 64%|██████▍   | 47/73 [00:06<00:03,  6.73it/s][A
 66%|██████▌   | 48/73 [00:07<00:03,  6.85it/s][A
 67%|██████▋   | 49/73 [00:07<00:03,  6.48it/s][A
 68%|██████▊   | 50/73 [00:07<00:03,  6.64it/s][A
 70%|██████▉   | 51/73 [00:07<00:03,  6.71it/s][A
 71%|███████   | 52/73 [00:07<00:03,  6.61it/s][A
 73%|███████▎  | 53/73 [00:07<00:03,  5.78it/s][A
 74%|███████▍  | 54/73 [00:08<00:03,  5.90it/s][A
 75%|███████▌  | 55/73 [00:08<00:03,  5.64it/s][A
 77%|███████▋  | 56/73 [00:08<00:02,  6.04it/s][A
 78%|███████▊  | 57/73 [00:08<00:02,  6.22it/s][A
 79%|███████▉  | 58/73 [00:08<00:02,  6.48it/s][A
 81%|████████  | 59/73 [00:08<00:02,  6.67it/s][A
 82%|████████▏ | 60/73 [00:09<00:02,  6.45it/s][A
 84%|████████▎ | 61/73 [00:09<00:01,  6.66it/s][A
 85%|████████▍ | 62/73 [00:09<00:01,  6.81it/s][A
 86%|████████▋ | 63/73 [00:09<00:01,  6.86it/s][A
 88%|████████▊ | 64/73 [00:09<00:01,  6.82it/s][A
 89%|████████▉ | 65/73 [00:09<00:01,  6.49it/s][A
 90%|█████████ | 66/73 [00:09<00:01,  6.43it/s][A
 92%|█████████▏| 67/73 [00:10<00:00,  6.39it/s][A
 93%|█████████▎| 68/73 [00:10<00:00,  6.56it/s][A
 95%|█████████▍| 69/73 [00:10<00:00,  6.48it/s][A
 96%|█████████▌| 70/73 [00:10<00:00,  6.64it/s][A
 97%|█████████▋| 71/73 [00:10<00:00,  6.39it/s][A
 99%|█████████▊| 72/73 [00:10<00:00,  6.44it/s][A
100%|██████████| 73/73 [00:11<00:00,  6.60it/s][A                                                 
                                               [A{'eval_loss': 0.0004119648365303874, 'eval_runtime': 11.1561, 'eval_samples_per_second': 13.087, 'eval_steps_per_second': 6.544, 'epoch': 4.99}
100%|██████████| 405/405 [27:38<00:00,  3.92s/it]
100%|██████████| 73/73 [00:11<00:00,  6.60it/s][A
                                               [A[INFO|trainer.py:2584] 2025-01-12 22:44:25,795 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 1668.0818, 'train_samples_per_second': 3.912, 'train_steps_per_second': 0.243, 'train_loss': 0.09660711190404578, 'epoch': 4.99}
100%|██████████| 405/405 [27:38<00:00,  3.92s/it]100%|██████████| 405/405 [27:38<00:00,  4.10s/it]
[INFO|trainer.py:3801] 2025-01-12 22:44:25,796 >> Saving model checkpoint to saves/qwen-14b-ka-e5/lora/sft
[INFO|configuration_utils.py:677] 2025-01-12 22:44:25,816 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 22:44:25,816 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2025-01-12 22:44:25,936 >> tokenizer config file saved in saves/qwen-14b-ka-e5/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-12 22:44:25,936 >> Special tokens file saved in saves/qwen-14b-ka-e5/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =      4.9862
  total_flos               = 197182524GF
  train_loss               =      0.0966
  train_runtime            =  0:27:48.08
  train_samples_per_second =       3.912
  train_steps_per_second   =       0.243
Figure saved at: saves/qwen-14b-ka-e5/lora/sft/training_loss.png
Figure saved at: saves/qwen-14b-ka-e5/lora/sft/training_eval_loss.png
[WARNING|2025-01-12 22:44:26] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4117] 2025-01-12 22:44:26,105 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 22:44:26,105 >>   Num examples = 146
[INFO|trainer.py:4122] 2025-01-12 22:44:26,105 >>   Batch size = 1
  0%|          | 0/73 [00:00<?, ?it/s]  3%|▎         | 2/73 [00:00<00:05, 13.21it/s]  5%|▌         | 4/73 [00:00<00:08,  8.12it/s]  7%|▋         | 5/73 [00:00<00:08,  7.79it/s]  8%|▊         | 6/73 [00:00<00:08,  7.63it/s] 10%|▉         | 7/73 [00:00<00:09,  7.24it/s] 11%|█         | 8/73 [00:01<00:09,  7.20it/s] 12%|█▏        | 9/73 [00:01<00:09,  7.04it/s] 14%|█▎        | 10/73 [00:01<00:09,  6.86it/s] 15%|█▌        | 11/73 [00:01<00:09,  6.86it/s] 16%|█▋        | 12/73 [00:01<00:08,  6.91it/s] 18%|█▊        | 13/73 [00:01<00:09,  6.48it/s] 19%|█▉        | 14/73 [00:01<00:09,  6.51it/s] 21%|██        | 15/73 [00:02<00:08,  6.73it/s] 22%|██▏       | 16/73 [00:02<00:08,  6.84it/s] 23%|██▎       | 17/73 [00:02<00:08,  6.53it/s] 25%|██▍       | 18/73 [00:02<00:08,  6.71it/s] 26%|██▌       | 19/73 [00:02<00:08,  6.54it/s] 27%|██▋       | 20/73 [00:02<00:07,  6.74it/s] 29%|██▉       | 21/73 [00:02<00:07,  6.91it/s] 30%|███       | 22/73 [00:03<00:07,  6.99it/s] 32%|███▏      | 23/73 [00:03<00:07,  6.60it/s] 33%|███▎      | 24/73 [00:03<00:07,  6.73it/s] 34%|███▍      | 25/73 [00:03<00:07,  6.62it/s] 36%|███▌      | 26/73 [00:03<00:06,  6.73it/s] 37%|███▋      | 27/73 [00:03<00:06,  6.82it/s] 38%|███▊      | 28/73 [00:04<00:06,  6.77it/s] 40%|███▉      | 29/73 [00:04<00:06,  6.34it/s] 41%|████      | 30/73 [00:04<00:06,  6.61it/s] 42%|████▏     | 31/73 [00:04<00:06,  6.62it/s] 44%|████▍     | 32/73 [00:04<00:06,  6.79it/s] 45%|████▌     | 33/73 [00:04<00:05,  6.88it/s] 47%|████▋     | 34/73 [00:04<00:05,  6.55it/s] 48%|████▊     | 35/73 [00:05<00:06,  5.92it/s] 49%|████▉     | 36/73 [00:05<00:05,  6.29it/s] 51%|█████     | 37/73 [00:05<00:05,  6.53it/s] 52%|█████▏    | 38/73 [00:05<00:05,  6.52it/s] 53%|█████▎    | 39/73 [00:05<00:05,  6.47it/s] 55%|█████▍    | 40/73 [00:05<00:05,  6.40it/s] 56%|█████▌    | 41/73 [00:06<00:05,  6.13it/s] 58%|█████▊    | 42/73 [00:06<00:04,  6.42it/s] 59%|█████▉    | 43/73 [00:06<00:04,  6.56it/s] 60%|██████    | 44/73 [00:06<00:04,  6.37it/s] 62%|██████▏   | 45/73 [00:06<00:04,  6.37it/s] 63%|██████▎   | 46/73 [00:06<00:04,  6.59it/s] 64%|██████▍   | 47/73 [00:06<00:03,  6.77it/s] 66%|██████▌   | 48/73 [00:07<00:03,  6.89it/s] 67%|██████▋   | 49/73 [00:07<00:03,  6.52it/s] 68%|██████▊   | 50/73 [00:07<00:03,  6.67it/s] 70%|██████▉   | 51/73 [00:07<00:03,  6.74it/s] 71%|███████   | 52/73 [00:07<00:03,  6.62it/s] 73%|███████▎  | 53/73 [00:07<00:03,  5.80it/s] 74%|███████▍  | 54/73 [00:08<00:03,  5.91it/s] 75%|███████▌  | 55/73 [00:08<00:03,  5.65it/s] 77%|███████▋  | 56/73 [00:08<00:02,  6.05it/s] 78%|███████▊  | 57/73 [00:08<00:02,  6.23it/s] 79%|███████▉  | 58/73 [00:08<00:02,  6.45it/s] 81%|████████  | 59/73 [00:08<00:02,  6.65it/s] 82%|████████▏ | 60/73 [00:09<00:02,  6.42it/s] 84%|████████▎ | 61/73 [00:09<00:01,  6.60it/s] 85%|████████▍ | 62/73 [00:09<00:01,  6.71it/s] 86%|████████▋ | 63/73 [00:09<00:01,  6.74it/s] 88%|████████▊ | 64/73 [00:09<00:01,  6.74it/s] 89%|████████▉ | 65/73 [00:09<00:01,  6.42it/s] 90%|█████████ | 66/73 [00:09<00:01,  6.38it/s] 92%|█████████▏| 67/73 [00:10<00:00,  6.36it/s] 93%|█████████▎| 68/73 [00:10<00:00,  6.58it/s] 95%|█████████▍| 69/73 [00:10<00:00,  6.53it/s] 96%|█████████▌| 70/73 [00:10<00:00,  6.73it/s] 97%|█████████▋| 71/73 [00:10<00:00,  6.44it/s] 99%|█████████▊| 72/73 [00:10<00:00,  6.49it/s]100%|██████████| 73/73 [00:10<00:00,  6.67it/s]100%|██████████| 73/73 [00:10<00:00,  6.64it/s]
***** eval metrics *****
  epoch                   =     4.9862
  eval_loss               =     0.0004
  eval_runtime            = 0:00:11.13
  eval_samples_per_second =     13.107
  eval_steps_per_second   =      6.553
[INFO|modelcard.py:449] 2025-01-12 22:44:37,245 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[rank0]:[W112 22:44:37.081380948 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

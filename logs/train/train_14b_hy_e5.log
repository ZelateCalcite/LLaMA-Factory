[2025-01-12 21:08:15,049] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|2025-01-12 21:08:16] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:24998
W0112 21:08:16.945000 782145 site-packages/torch/distributed/run.py:793] 
W0112 21:08:16.945000 782145 site-packages/torch/distributed/run.py:793] *****************************************
W0112 21:08:16.945000 782145 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0112 21:08:16.945000 782145 site-packages/torch/distributed/run.py:793] *****************************************
[2025-01-12 21:08:18,427] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-12 21:08:18,427] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[WARNING|2025-01-12 21:08:19] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.
[WARNING|2025-01-12 21:08:19] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|2025-01-12 21:08:19] llamafactory.hparams.parser:359 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:677] 2025-01-12 21:08:19,220 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 21:08:19,220 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,221 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,221 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,221 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,221 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,221 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,221 >> loading file tokenizer_config.json
[INFO|2025-01-12 21:08:19] llamafactory.hparams.parser:359 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2475] 2025-01-12 21:08:19,352 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:677] 2025-01-12 21:08:19,353 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 21:08:19,353 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,353 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,353 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,353 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,353 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,353 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2025-01-12 21:08:19,353 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-12 21:08:19,486 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-01-12 21:08:19] llamafactory.data.template:157 >> Add <|im_end|> to stop words.
[INFO|2025-01-12 21:08:19] llamafactory.data.loader:157 >> Loading dataset entity_trans_hy.json...
[rank1]:[W112 21:08:19.080547081 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1568 examples [00:00, 38557.91 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1568 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1568/1568 [00:00<00:00, 10750.35 examples/s]
[rank0]:[W112 21:08:44.633489507 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1568 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 98/1568 [00:00<00:06, 244.24 examples/s]Running tokenizer on dataset (num_proc=16):  25%|██▌       | 392/1568 [00:00<00:01, 906.29 examples/s]Running tokenizer on dataset (num_proc=16):  44%|████▍     | 686/1568 [00:00<00:00, 1411.58 examples/s]Running tokenizer on dataset (num_proc=16):  62%|██████▎   | 980/1568 [00:00<00:00, 1334.57 examples/s]Running tokenizer on dataset (num_proc=16):  81%|████████▏ | 1274/1568 [00:00<00:00, 1592.11 examples/s]Running tokenizer on dataset (num_proc=16):  94%|█████████▍| 1470/1568 [00:01<00:00, 1670.05 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1568/1568 [00:01<00:00, 1298.47 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2610, 525, 264, 9990, 66742, 12, 22574, 14468, 7341, 11, 4486, 1492, 752, 14683, 1045, 6364, 22870, 1119, 66742, 13, 7036, 429, 279, 4396, 1102, 1265, 387, 264, 17133, 429, 7952, 304, 279, 11652, 624, 40, 686, 2968, 498, 458, 6364, 17133, 323, 264, 66742, 11652, 11, 1380, 279, 6364, 17133, 374, 279, 14468, 1102, 304, 279, 66742, 11652, 13, 358, 1366, 311, 1477, 279, 7112, 66742, 17133, 315, 279, 6364, 14468, 1102, 624, 5501, 1744, 3019, 553, 3019, 13, 151645, 198, 151644, 77091, 198, 39814, 11, 358, 646, 7789, 448, 429, 13, 5209, 3410, 279, 6364, 17133, 323, 279, 66742, 11652, 432, 7952, 304, 773, 358, 646, 10542, 279, 12159, 66742, 17133, 369, 498, 382, 5501, 3561, 279, 1946, 438, 11017, 1447, 22574, 17133, 25, 508, 4208, 6364, 17133, 921, 6953, 5676, 1103, 11652, 25, 508, 4208, 66742, 11652, 2533, 40, 3278, 1221, 3410, 279, 7112, 66742, 17133, 429, 33210, 311, 279, 6364, 14468, 1102, 13, 151645, 198, 151644, 872, 198, 22574, 17133, 25, 9812, 198, 6953, 5676, 1103, 11652, 25, 220, 147992, 146208, 145160, 145330, 147787, 144925, 146088, 144925, 145175, 220, 145729, 145405, 145330, 145755, 146445, 145680, 145330, 145755, 145175, 11, 220, 147992, 146208, 145160, 144925, 145729, 145405, 145330, 145755, 146445, 145680, 145330, 145755, 145175, 320, 147992, 147446, 701, 220, 145405, 145160, 144925, 146208, 145330, 145755, 145175, 146046, 145405, 220, 146046, 144925, 147240, 144925, 146046, 144925, 146088, 144925, 145175, 220, 146515, 144925, 145729, 144925, 145680, 145175, 146046, 149510, 220, 146515, 145405, 145729, 145175, 146208, 144925, 147163, 220, 145729, 145405, 146798, 144925, 147175, 146366, 144925, 145680, 145175, 144925, 146209, 146208, 144925, 147163, 220, 146088, 144925, 147175, 145729, 144925, 146088, 145265, 145160, 147787, 145330, 145755, 146445, 145680, 144925, 145175, 12486, 82, 1963, 73572, 1194, 285, 12992, 220, 146398, 145265, 147965, 145405, 145729, 145405, 220, 146208, 145160, 144925, 11, 220, 145330, 145160, 145405, 220, 145175, 147787, 144925, 146100, 144925, 146088, 145175, 220, 147642, 220, 147992, 146208, 145160, 145330, 147787, 144925, 145680, 145405, 220, 145265, 145160, 146088, 145160, 145175, 145265, 145160, 145405, 220, 145330, 145755, 220, 147965, 145330, 147240, 145330, 146208, 145330, 145755, 145160, 146197, 145175, 145265, 145160, 145405, 220, 145405, 145175, 146100, 145265, 146366, 145160, 145729, 144925, 145175, 220, 145330, 145755, 220, 146088, 144925, 146398, 144925, 146208, 144925, 145160, 145729, 144925, 145175, 220, 146366, 145330, 145160, 147163, 144925, 146398, 145330, 145755, 145680, 146445, 145405, 220, 146088, 144925, 147175, 145729, 144925, 146088, 145265, 145160, 147787, 145330, 145755, 145729, 148012, 150381, 220, 148011, 144925, 147240, 146088, 144925, 146209, 144925, 147163, 220, 147642, 220, 145265, 146208, 145160, 145330, 147787, 144925, 146088, 144925, 145175, 220, 17, 22, 220, 145265, 145160, 146088, 145160, 145175, 145265, 145160, 145405, 146209, 220, 148286, 220, 146515, 145405, 145729, 145175, 146208, 145265, 145923, 220, 147642, 220, 16, 24, 24, 18, 220, 146445, 146208, 144925, 146088, 144925, 145175, 145405, 220, 145175, 145330, 145680, 145265, 145729, 146873, 145265, 145160, 145405, 220, 16, 12, 145405, 145175, 149510, 220, 147992, 146208, 145160, 145330, 147787, 144925, 146088, 144925, 145175, 220, 145729, 145405, 145330, 145755, 146445, 145680, 144925, 145175, 220, 146515, 144925, 145729, 144925, 147985, 144925, 145680, 145175, 144925, 146366, 145160, 145405, 220, 145330, 145755, 147965, 145405, 220, 145729, 145265, 146798, 220, 145729, 146100, 145175, 145265, 145923, 145330, 145755, 220, 147787, 144925, 146515, 145405, 146209, 150381, 151645, 198, 151644, 77091, 198, 785, 2661, 6364, 17133, 9812, 646, 387, 24531, 1119, 66742, 438, 220, 147992, 146208, 145160, 145330, 147787, 144925, 146088, 144925, 145175, 220, 145729, 145405, 145330, 145755, 146445, 145680, 145330, 145755, 145175, 11, 323, 279, 7112, 66742, 17133, 304, 279, 2661, 11652, 1265, 387, 220, 147992, 146208, 145160, 145330, 147787, 144925, 146088, 144925, 145175, 220, 145729, 145405, 145330, 145755, 146445, 145680, 145330, 145755, 145175, 13, 151645, 198, 151644, 872, 198, 3973, 279, 1102, 323, 1281, 2704, 1817, 3409, 315, 279, 1102, 7952, 304, 279, 2661, 66742, 11652, 13, 151645, 198, 151644, 77091, 198, 39814, 0, 358, 614, 10067, 279, 1102, 220, 147992, 146208, 145160, 145330, 147787, 144925, 146088, 144925, 145175, 220, 145729, 145405, 145330, 145755, 146445, 145680, 145330, 145755, 145175, 323, 432, 2167, 7952, 304, 279, 2661, 11652, 13, 151645, 198, 151644, 872, 198, 35127, 752, 279, 1590, 1102, 2041, 1008, 4244, 13, 151645, 198, 151644, 77091, 198, 147992, 146208, 145160, 145330, 147787, 144925, 146088, 144925, 145175, 220, 145729, 145405, 145330, 145755, 146445, 145680, 145330, 145755, 145175, 151645]
inputs:
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
You are a senior Armenian-English translation master, please help me translate some English sentences into Armenian. Note that the correct result should be a phrase that appears in the sentence.
I will give you an English phrase and a Armenian sentence, where the English phrase is the translation result in the Armenian sentence. I want to find the raw Armenian phrase of the English translation result.
Please think step by step.<|im_end|>
<|im_start|>assistant
Sure, I can assist with that. Please provide the English phrase and the Armenian sentence it appears in so I can identify the corresponding Armenian phrase for you.

Please format the input as follows:

English phrase: [insert English phrase]
Armenian sentence: [insert Armenian sentence]

I'll then provide the raw Armenian phrase that corresponds to the English translation result.<|im_end|>
<|im_start|>user
English phrase: EU
Armenian sentence: Եվրոպական միություն, Եվրամիություն (ԵՄ), իրավունքի քաղաքական համայնք՝ հիմնված միջազգայնացված կազմակերպության «sui géneris» ռեժիմի վրա, որի նպատակն է Եվրոպայի երկրների ու ժողովուրդների ինտեգրման ու կառավարման գործառույթի կազմակերպումը։ Բաղկացած է եվրոպական 27 երկրներից և հիմնվել է 1993 թվականի նոյեմբերի 1-ին՝ Եվրոպական միության համաձայնագրի ուժի մեջ մտնելու պահից։<|im_end|>
<|im_start|>assistant
The given English phrase EU can be translated into Armenian as Եվրոպական միություն, and the raw Armenian phrase in the given sentence should be Եվրոպական միություն.<|im_end|>
<|im_start|>user
Check the result and make sure each word of the result appears in the given Armenian sentence.<|im_end|>
<|im_start|>assistant
Sure! I have checked the result Եվրոպական միություն and it really appears in the given sentence.<|im_end|>
<|im_start|>user
Give me the final result without other words.<|im_end|>
<|im_start|>assistant
Եվրոպական միություն<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 39814, 11, 358, 646, 7789, 448, 429, 13, 5209, 3410, 279, 6364, 17133, 323, 279, 66742, 11652, 432, 7952, 304, 773, 358, 646, 10542, 279, 12159, 66742, 17133, 369, 498, 382, 5501, 3561, 279, 1946, 438, 11017, 1447, 22574, 17133, 25, 508, 4208, 6364, 17133, 921, 6953, 5676, 1103, 11652, 25, 508, 4208, 66742, 11652, 2533, 40, 3278, 1221, 3410, 279, 7112, 66742, 17133, 429, 33210, 311, 279, 6364, 14468, 1102, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 2661, 6364, 17133, 9812, 646, 387, 24531, 1119, 66742, 438, 220, 147992, 146208, 145160, 145330, 147787, 144925, 146088, 144925, 145175, 220, 145729, 145405, 145330, 145755, 146445, 145680, 145330, 145755, 145175, 11, 323, 279, 7112, 66742, 17133, 304, 279, 2661, 11652, 1265, 387, 220, 147992, 146208, 145160, 145330, 147787, 144925, 146088, 144925, 145175, 220, 145729, 145405, 145330, 145755, 146445, 145680, 145330, 145755, 145175, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 39814, 0, 358, 614, 10067, 279, 1102, 220, 147992, 146208, 145160, 145330, 147787, 144925, 146088, 144925, 145175, 220, 145729, 145405, 145330, 145755, 146445, 145680, 145330, 145755, 145175, 323, 432, 2167, 7952, 304, 279, 2661, 11652, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 147992, 146208, 145160, 145330, 147787, 144925, 146088, 144925, 145175, 220, 145729, 145405, 145330, 145755, 146445, 145680, 145330, 145755, 145175, 151645]
labels:
Sure, I can assist with that. Please provide the English phrase and the Armenian sentence it appears in so I can identify the corresponding Armenian phrase for you.

Please format the input as follows:

English phrase: [insert English phrase]
Armenian sentence: [insert Armenian sentence]

I'll then provide the raw Armenian phrase that corresponds to the English translation result.<|im_end|>The given English phrase EU can be translated into Armenian as Եվրոպական միություն, and the raw Armenian phrase in the given sentence should be Եվրոպական միություն.<|im_end|>Sure! I have checked the result Եվրոպական միություն and it really appears in the given sentence.<|im_end|>Եվրոպական միություն<|im_end|>
[INFO|configuration_utils.py:677] 2025-01-12 21:09:08,401 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 21:09:08,402 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|2025-01-12 21:09:08] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.
[INFO|modeling_utils.py:3934] 2025-01-12 21:09:08,446 >> loading weights file /mnt/sda/zzh/Qwen2.5-14B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1670] 2025-01-12 21:09:08,447 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1096] 2025-01-12 21:09:08,447 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:04,  1.73it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:03,  1.59it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:01<00:03,  1.64it/s]Loading checkpoint shards:  50%|█████     | 4/8 [00:02<00:02,  1.67it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:02<00:01,  1.70it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:03<00:01,  1.72it/s]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:04<00:00,  1.67it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  2.01it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.79it/s]
[INFO|modeling_utils.py:4800] 2025-01-12 21:09:13,028 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4808] 2025-01-12 21:09:13,028 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /mnt/sda/zzh/Qwen2.5-14B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1049] 2025-01-12 21:09:13,030 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/generation_config.json
[INFO|configuration_utils.py:1096] 2025-01-12 21:09:13,030 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|2025-01-12 21:09:13] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
[INFO|2025-01-12 21:09:13] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.
[INFO|2025-01-12 21:09:13] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.
[INFO|2025-01-12 21:09:13] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA
[INFO|2025-01-12 21:09:13] llamafactory.model.model_utils.misc:157 >> Found linear modules: down_proj,o_proj,v_proj,up_proj,q_proj,gate_proj,k_proj
[INFO|2025-01-12 21:09:13] llamafactory.model.loader:157 >> trainable params: 34,406,400 || all params: 14,804,440,064 || trainable%: 0.2324
[INFO|trainer.py:698] 2025-01-12 21:09:13,353 >> Using auto half precision backend
Loading checkpoint shards:  12%|█▎        | 1/8 [00:04<00:34,  4.97s/it]Loading checkpoint shards:  25%|██▌       | 2/8 [00:10<00:30,  5.05s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:15<00:25,  5.07s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:20<00:20,  5.07s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:25<00:15,  5.07s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:30<00:10,  5.07s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:35<00:05,  5.08s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:37<00:00,  4.16s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:37<00:00,  4.71s/it]
[INFO|trainer.py:2313] 2025-01-12 21:09:49,248 >> ***** Running training *****
[INFO|trainer.py:2314] 2025-01-12 21:09:49,248 >>   Num examples = 1,411
[INFO|trainer.py:2315] 2025-01-12 21:09:49,248 >>   Num Epochs = 5
[INFO|trainer.py:2316] 2025-01-12 21:09:49,248 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2319] 2025-01-12 21:09:49,248 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2320] 2025-01-12 21:09:49,248 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2321] 2025-01-12 21:09:49,248 >>   Total optimization steps = 440
[INFO|trainer.py:2322] 2025-01-12 21:09:49,251 >>   Number of trainable parameters = 34,406,400
  0%|          | 0/440 [00:00<?, ?it/s]  0%|          | 1/440 [00:07<57:04,  7.80s/it]  0%|          | 2/440 [00:13<47:12,  6.47s/it]  1%|          | 3/440 [00:18<42:52,  5.89s/it]  1%|          | 4/440 [00:23<40:27,  5.57s/it]  1%|          | 5/440 [00:28<38:28,  5.31s/it]  1%|▏         | 6/440 [00:33<37:40,  5.21s/it]  2%|▏         | 7/440 [00:39<38:35,  5.35s/it]  2%|▏         | 8/440 [00:44<39:17,  5.46s/it]  2%|▏         | 9/440 [00:50<40:02,  5.58s/it]  2%|▏         | 10/440 [00:56<40:31,  5.66s/it]  2%|▎         | 11/440 [01:02<40:29,  5.66s/it]  3%|▎         | 12/440 [01:08<40:48,  5.72s/it]  3%|▎         | 13/440 [01:13<40:37,  5.71s/it]  3%|▎         | 14/440 [01:19<41:37,  5.86s/it]  3%|▎         | 15/440 [01:26<43:45,  6.18s/it]  4%|▎         | 16/440 [01:32<42:58,  6.08s/it]  4%|▍         | 17/440 [01:38<41:32,  5.89s/it]  4%|▍         | 18/440 [01:44<42:38,  6.06s/it]  4%|▍         | 19/440 [01:50<41:23,  5.90s/it]  5%|▍         | 20/440 [01:55<40:25,  5.78s/it]  5%|▍         | 21/440 [02:03<43:51,  6.28s/it]  5%|▌         | 22/440 [02:09<43:37,  6.26s/it]  5%|▌         | 23/440 [02:15<43:15,  6.22s/it]  5%|▌         | 24/440 [02:21<42:43,  6.16s/it]  6%|▌         | 25/440 [02:27<42:16,  6.11s/it]  6%|▌         | 26/440 [02:34<44:23,  6.43s/it]  6%|▌         | 27/440 [02:39<41:19,  6.00s/it]  6%|▋         | 28/440 [02:45<42:01,  6.12s/it]  7%|▋         | 29/440 [02:53<44:21,  6.48s/it]  7%|▋         | 30/440 [02:59<42:55,  6.28s/it]  7%|▋         | 31/440 [03:04<40:49,  5.99s/it]  7%|▋         | 32/440 [03:10<40:41,  5.98s/it]  8%|▊         | 33/440 [03:16<41:11,  6.07s/it]  8%|▊         | 34/440 [03:22<40:23,  5.97s/it]  8%|▊         | 35/440 [03:27<39:27,  5.85s/it]  8%|▊         | 36/440 [03:34<39:48,  5.91s/it]  8%|▊         | 37/440 [03:39<39:45,  5.92s/it]  9%|▊         | 38/440 [03:45<38:24,  5.73s/it]  9%|▉         | 39/440 [03:50<36:44,  5.50s/it]  9%|▉         | 40/440 [03:56<37:48,  5.67s/it]  9%|▉         | 41/440 [04:01<37:26,  5.63s/it] 10%|▉         | 42/440 [04:08<38:57,  5.87s/it] 10%|▉         | 43/440 [04:13<37:33,  5.68s/it] 10%|█         | 44/440 [04:19<37:28,  5.68s/it] 10%|█         | 45/440 [04:25<38:39,  5.87s/it] 10%|█         | 46/440 [04:31<39:05,  5.95s/it] 11%|█         | 47/440 [04:37<39:45,  6.07s/it] 11%|█         | 48/440 [04:43<38:19,  5.87s/it] 11%|█         | 49/440 [04:49<38:17,  5.88s/it] 11%|█▏        | 50/440 [04:54<36:07,  5.56s/it] 12%|█▏        | 51/440 [04:59<36:26,  5.62s/it] 12%|█▏        | 52/440 [05:04<35:21,  5.47s/it] 12%|█▏        | 53/440 [05:10<35:28,  5.50s/it] 12%|█▏        | 54/440 [05:16<35:21,  5.50s/it] 12%|█▎        | 55/440 [05:21<35:23,  5.52s/it] 13%|█▎        | 56/440 [05:27<36:19,  5.68s/it] 13%|█▎        | 57/440 [05:33<36:57,  5.79s/it] 13%|█▎        | 58/440 [05:39<37:01,  5.82s/it] 13%|█▎        | 59/440 [05:45<36:49,  5.80s/it] 14%|█▎        | 60/440 [05:51<37:01,  5.85s/it] 14%|█▍        | 61/440 [05:57<37:01,  5.86s/it] 14%|█▍        | 62/440 [06:03<37:41,  5.98s/it] 14%|█▍        | 63/440 [06:09<38:03,  6.06s/it] 15%|█▍        | 64/440 [06:15<36:50,  5.88s/it] 15%|█▍        | 65/440 [06:20<36:40,  5.87s/it] 15%|█▌        | 66/440 [06:27<37:10,  5.97s/it] 15%|█▌        | 67/440 [06:32<35:32,  5.72s/it] 15%|█▌        | 68/440 [06:37<34:20,  5.54s/it] 16%|█▌        | 69/440 [06:42<33:39,  5.44s/it] 16%|█▌        | 70/440 [06:48<34:15,  5.56s/it] 16%|█▌        | 71/440 [06:54<34:35,  5.62s/it] 16%|█▋        | 72/440 [06:59<33:24,  5.45s/it] 17%|█▋        | 73/440 [07:04<32:55,  5.38s/it] 17%|█▋        | 74/440 [07:10<33:32,  5.50s/it] 17%|█▋        | 75/440 [07:16<35:22,  5.81s/it] 17%|█▋        | 76/440 [07:23<36:01,  5.94s/it] 18%|█▊        | 77/440 [07:29<37:10,  6.14s/it] 18%|█▊        | 78/440 [07:35<37:03,  6.14s/it] 18%|█▊        | 79/440 [07:41<36:48,  6.12s/it] 18%|█▊        | 80/440 [07:48<36:55,  6.15s/it] 18%|█▊        | 81/440 [07:53<34:52,  5.83s/it] 19%|█▊        | 82/440 [07:59<34:48,  5.83s/it] 19%|█▉        | 83/440 [08:04<33:38,  5.65s/it] 19%|█▉        | 84/440 [08:10<34:30,  5.82s/it] 19%|█▉        | 85/440 [08:15<33:31,  5.67s/it] 20%|█▉        | 86/440 [08:21<34:22,  5.83s/it] 20%|█▉        | 87/440 [08:27<33:08,  5.63s/it] 20%|██        | 88/440 [08:32<33:18,  5.68s/it][INFO|trainer.py:4117] 2025-01-12 21:18:33,132 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 21:18:33,132 >>   Num examples = 157
[INFO|trainer.py:4122] 2025-01-12 21:18:33,132 >>   Batch size = 1

  0%|          | 0/79 [00:00<?, ?it/s][A
  3%|▎         | 2/79 [00:00<00:07, 10.27it/s][A
  5%|▌         | 4/79 [00:00<00:12,  6.22it/s][A
  6%|▋         | 5/79 [00:00<00:12,  5.74it/s][A
  8%|▊         | 6/79 [00:01<00:13,  5.54it/s][A
  9%|▉         | 7/79 [00:01<00:14,  4.87it/s][A
 10%|█         | 8/79 [00:01<00:19,  3.55it/s][A
 11%|█▏        | 9/79 [00:01<00:19,  3.65it/s][A
 13%|█▎        | 10/79 [00:02<00:16,  4.15it/s][A
 14%|█▍        | 11/79 [00:02<00:15,  4.32it/s][A
 15%|█▌        | 12/79 [00:02<00:13,  4.81it/s][A
 16%|█▋        | 13/79 [00:02<00:16,  3.90it/s][A
 18%|█▊        | 14/79 [00:03<00:15,  4.13it/s][A
 19%|█▉        | 15/79 [00:03<00:14,  4.40it/s][A
 20%|██        | 16/79 [00:03<00:15,  4.10it/s][A
 22%|██▏       | 17/79 [00:03<00:14,  4.16it/s][A
 23%|██▎       | 18/79 [00:04<00:17,  3.54it/s][A
 24%|██▍       | 19/79 [00:04<00:15,  3.93it/s][A
 25%|██▌       | 20/79 [00:04<00:13,  4.31it/s][A
 27%|██▋       | 21/79 [00:04<00:12,  4.75it/s][A
 28%|██▊       | 22/79 [00:05<00:18,  3.14it/s][A
 29%|██▉       | 23/79 [00:05<00:16,  3.38it/s][A
 30%|███       | 24/79 [00:05<00:13,  3.96it/s][A
 32%|███▏      | 25/79 [00:06<00:18,  2.96it/s][A
 33%|███▎      | 26/79 [00:06<00:15,  3.44it/s][A
 34%|███▍      | 27/79 [00:06<00:14,  3.70it/s][A
 35%|███▌      | 28/79 [00:06<00:12,  4.04it/s][A
 37%|███▋      | 29/79 [00:06<00:10,  4.57it/s][A
 38%|███▊      | 30/79 [00:07<00:10,  4.48it/s][A
 39%|███▉      | 31/79 [00:07<00:09,  4.89it/s][A
 41%|████      | 32/79 [00:07<00:10,  4.34it/s][A
 42%|████▏     | 33/79 [00:07<00:10,  4.47it/s][A
 43%|████▎     | 34/79 [00:08<00:09,  4.64it/s][A
 44%|████▍     | 35/79 [00:08<00:09,  4.69it/s][A
 46%|████▌     | 36/79 [00:08<00:11,  3.76it/s][A
 47%|████▋     | 37/79 [00:08<00:11,  3.70it/s][A
 48%|████▊     | 38/79 [00:09<00:11,  3.69it/s][A
 49%|████▉     | 39/79 [00:09<00:09,  4.11it/s][A
 51%|█████     | 40/79 [00:09<00:08,  4.43it/s][A
 52%|█████▏    | 41/79 [00:09<00:08,  4.70it/s][A
 53%|█████▎    | 42/79 [00:09<00:07,  4.90it/s][A
 54%|█████▍    | 43/79 [00:10<00:08,  4.47it/s][A
 56%|█████▌    | 44/79 [00:10<00:08,  4.09it/s][A
 57%|█████▋    | 45/79 [00:10<00:07,  4.29it/s][A
 58%|█████▊    | 46/79 [00:10<00:07,  4.53it/s][A
 59%|█████▉    | 47/79 [00:11<00:06,  4.75it/s][A
 61%|██████    | 48/79 [00:11<00:07,  4.02it/s][A
 62%|██████▏   | 49/79 [00:11<00:07,  4.02it/s][A
 63%|██████▎   | 50/79 [00:11<00:06,  4.51it/s][A
 65%|██████▍   | 51/79 [00:12<00:08,  3.36it/s][A
 66%|██████▌   | 52/79 [00:12<00:07,  3.66it/s][A
 67%|██████▋   | 53/79 [00:12<00:07,  3.63it/s][A
 68%|██████▊   | 54/79 [00:12<00:06,  3.99it/s][A
 70%|██████▉   | 55/79 [00:13<00:05,  4.08it/s][A
 71%|███████   | 56/79 [00:13<00:04,  4.61it/s][A
 72%|███████▏  | 57/79 [00:13<00:04,  4.68it/s][A
 73%|███████▎  | 58/79 [00:13<00:04,  4.42it/s][A
 75%|███████▍  | 59/79 [00:14<00:04,  4.52it/s][A
 76%|███████▌  | 60/79 [00:14<00:04,  4.23it/s][A
 77%|███████▋  | 61/79 [00:14<00:04,  3.87it/s][A
 78%|███████▊  | 62/79 [00:14<00:04,  3.57it/s][A
 80%|███████▉  | 63/79 [00:15<00:04,  3.86it/s][A
 81%|████████  | 64/79 [00:15<00:03,  4.23it/s][A
 82%|████████▏ | 65/79 [00:15<00:03,  4.66it/s][A
 84%|████████▎ | 66/79 [00:15<00:02,  4.71it/s][A
 85%|████████▍ | 67/79 [00:15<00:02,  4.71it/s][A
 86%|████████▌ | 68/79 [00:16<00:02,  4.82it/s][A
 87%|████████▋ | 69/79 [00:16<00:02,  4.41it/s][A
 89%|████████▊ | 70/79 [00:16<00:02,  4.42it/s][A
 90%|████████▉ | 71/79 [00:16<00:01,  4.15it/s][A
 91%|█████████ | 72/79 [00:17<00:01,  4.47it/s][A
 92%|█████████▏| 73/79 [00:17<00:01,  4.30it/s][A
 94%|█████████▎| 74/79 [00:17<00:01,  3.98it/s][A
 95%|█████████▍| 75/79 [00:17<00:00,  4.12it/s][A
 96%|█████████▌| 76/79 [00:18<00:00,  4.46it/s][A
 97%|█████████▋| 77/79 [00:18<00:00,  3.87it/s][A
 99%|█████████▊| 78/79 [00:18<00:00,  4.02it/s][A
100%|██████████| 79/79 [00:18<00:00,  4.38it/s][A                                                
                                               [A{'eval_loss': 0.004663583356887102, 'eval_runtime': 18.9719, 'eval_samples_per_second': 8.275, 'eval_steps_per_second': 4.164, 'epoch': 1.0}
 20%|██        | 88/440 [08:53<33:18,  5.68s/it]
100%|██████████| 79/79 [00:18<00:00,  4.38it/s][A
                                               [A 20%|██        | 89/440 [08:57<1:05:39, 11.22s/it] 20%|██        | 90/440 [09:03<56:47,  9.74s/it]   21%|██        | 91/440 [09:09<50:03,  8.61s/it] 21%|██        | 92/440 [09:14<44:20,  7.65s/it] 21%|██        | 93/440 [09:21<41:53,  7.24s/it] 21%|██▏       | 94/440 [09:25<37:13,  6.46s/it] 22%|██▏       | 95/440 [09:31<36:19,  6.32s/it] 22%|██▏       | 96/440 [09:37<36:00,  6.28s/it] 22%|██▏       | 97/440 [09:43<34:48,  6.09s/it] 22%|██▏       | 98/440 [09:49<34:22,  6.03s/it] 22%|██▎       | 99/440 [09:56<36:18,  6.39s/it] 23%|██▎       | 100/440 [10:02<35:37,  6.29s/it] 23%|██▎       | 101/440 [10:08<35:01,  6.20s/it] 23%|██▎       | 102/440 [10:14<34:08,  6.06s/it] 23%|██▎       | 103/440 [10:20<33:23,  5.94s/it] 24%|██▎       | 104/440 [10:26<34:01,  6.08s/it] 24%|██▍       | 105/440 [10:32<33:20,  5.97s/it] 24%|██▍       | 106/440 [10:38<33:02,  5.93s/it] 24%|██▍       | 107/440 [10:43<31:31,  5.68s/it] 25%|██▍       | 108/440 [10:48<31:43,  5.73s/it] 25%|██▍       | 109/440 [10:55<32:22,  5.87s/it] 25%|██▌       | 110/440 [11:01<33:27,  6.08s/it] 25%|██▌       | 111/440 [11:07<32:42,  5.97s/it] 25%|██▌       | 112/440 [11:13<33:19,  6.10s/it] 26%|██▌       | 113/440 [11:18<31:37,  5.80s/it] 26%|██▌       | 114/440 [11:25<32:00,  5.89s/it] 26%|██▌       | 115/440 [11:31<32:34,  6.01s/it] 26%|██▋       | 116/440 [11:37<32:47,  6.07s/it] 27%|██▋       | 117/440 [11:43<31:59,  5.94s/it] 27%|██▋       | 118/440 [11:49<32:38,  6.08s/it] 27%|██▋       | 119/440 [11:54<30:59,  5.79s/it] 27%|██▋       | 120/440 [12:01<32:40,  6.13s/it] 28%|██▊       | 121/440 [12:08<32:59,  6.20s/it] 28%|██▊       | 122/440 [12:14<33:07,  6.25s/it] 28%|██▊       | 123/440 [12:20<32:22,  6.13s/it] 28%|██▊       | 124/440 [12:26<31:50,  6.05s/it] 28%|██▊       | 125/440 [12:31<31:27,  5.99s/it] 29%|██▊       | 126/440 [12:39<33:02,  6.31s/it] 29%|██▉       | 127/440 [12:44<31:47,  6.09s/it] 29%|██▉       | 128/440 [12:50<32:05,  6.17s/it] 29%|██▉       | 129/440 [12:56<31:10,  6.01s/it] 30%|██▉       | 130/440 [13:02<30:16,  5.86s/it] 30%|██▉       | 131/440 [13:07<28:56,  5.62s/it] 30%|███       | 132/440 [13:12<27:53,  5.43s/it] 30%|███       | 133/440 [13:17<27:28,  5.37s/it] 30%|███       | 134/440 [13:22<27:38,  5.42s/it] 31%|███       | 135/440 [13:29<29:48,  5.86s/it] 31%|███       | 136/440 [13:35<29:42,  5.86s/it] 31%|███       | 137/440 [13:41<28:48,  5.70s/it] 31%|███▏      | 138/440 [13:46<28:41,  5.70s/it] 32%|███▏      | 139/440 [13:52<29:17,  5.84s/it] 32%|███▏      | 140/440 [13:58<28:34,  5.71s/it] 32%|███▏      | 141/440 [14:03<27:24,  5.50s/it] 32%|███▏      | 142/440 [14:08<27:03,  5.45s/it] 32%|███▎      | 143/440 [14:15<28:50,  5.83s/it] 33%|███▎      | 144/440 [14:21<28:32,  5.79s/it] 33%|███▎      | 145/440 [14:26<27:23,  5.57s/it] 33%|███▎      | 146/440 [14:31<27:22,  5.59s/it] 33%|███▎      | 147/440 [14:37<27:07,  5.56s/it] 34%|███▎      | 148/440 [14:42<26:37,  5.47s/it] 34%|███▍      | 149/440 [14:49<28:09,  5.81s/it] 34%|███▍      | 150/440 [14:54<27:42,  5.73s/it] 34%|███▍      | 151/440 [15:00<27:35,  5.73s/it] 35%|███▍      | 152/440 [15:06<27:39,  5.76s/it] 35%|███▍      | 153/440 [15:12<28:03,  5.87s/it] 35%|███▌      | 154/440 [15:17<26:49,  5.63s/it] 35%|███▌      | 155/440 [15:24<29:01,  6.11s/it] 35%|███▌      | 156/440 [15:30<28:38,  6.05s/it] 36%|███▌      | 157/440 [15:36<27:50,  5.90s/it] 36%|███▌      | 158/440 [15:41<26:30,  5.64s/it] 36%|███▌      | 159/440 [15:46<26:03,  5.57s/it] 36%|███▋      | 160/440 [15:52<25:59,  5.57s/it] 37%|███▋      | 161/440 [15:59<28:32,  6.14s/it] 37%|███▋      | 162/440 [16:05<27:35,  5.95s/it] 37%|███▋      | 163/440 [16:12<29:03,  6.29s/it] 37%|███▋      | 164/440 [16:17<27:17,  5.93s/it] 38%|███▊      | 165/440 [16:22<26:23,  5.76s/it] 38%|███▊      | 166/440 [16:28<26:13,  5.74s/it] 38%|███▊      | 167/440 [16:34<26:16,  5.78s/it] 38%|███▊      | 168/440 [16:39<25:47,  5.69s/it] 38%|███▊      | 169/440 [16:45<26:16,  5.82s/it] 39%|███▊      | 170/440 [16:51<26:07,  5.81s/it] 39%|███▉      | 171/440 [16:57<26:32,  5.92s/it] 39%|███▉      | 172/440 [17:03<26:40,  5.97s/it] 39%|███▉      | 173/440 [17:09<26:15,  5.90s/it] 40%|███▉      | 174/440 [17:15<26:32,  5.99s/it] 40%|███▉      | 175/440 [17:21<25:44,  5.83s/it] 40%|████      | 176/440 [17:26<25:22,  5.77s/it][INFO|trainer.py:4117] 2025-01-12 21:27:27,770 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 21:27:27,770 >>   Num examples = 157
[INFO|trainer.py:4122] 2025-01-12 21:27:27,770 >>   Batch size = 1

  0%|          | 0/79 [00:00<?, ?it/s][A
  3%|▎         | 2/79 [00:00<00:07, 10.22it/s][A
  5%|▌         | 4/79 [00:00<00:11,  6.29it/s][A
  6%|▋         | 5/79 [00:00<00:12,  5.88it/s][A
  8%|▊         | 6/79 [00:00<00:12,  5.63it/s][A
  9%|▉         | 7/79 [00:01<00:14,  4.92it/s][A
 10%|█         | 8/79 [00:01<00:19,  3.57it/s][A
 11%|█▏        | 9/79 [00:01<00:19,  3.67it/s][A
 13%|█▎        | 10/79 [00:02<00:16,  4.17it/s][A
 14%|█▍        | 11/79 [00:02<00:15,  4.33it/s][A
 15%|█▌        | 12/79 [00:02<00:13,  4.82it/s][A
 16%|█▋        | 13/79 [00:02<00:16,  3.90it/s][A
 18%|█▊        | 14/79 [00:03<00:15,  4.13it/s][A
 19%|█▉        | 15/79 [00:03<00:14,  4.40it/s][A
 20%|██        | 16/79 [00:03<00:15,  4.06it/s][A
 22%|██▏       | 17/79 [00:03<00:15,  4.10it/s][A
 23%|██▎       | 18/79 [00:04<00:17,  3.50it/s][A
 24%|██▍       | 19/79 [00:04<00:15,  3.89it/s][A
 25%|██▌       | 20/79 [00:04<00:13,  4.28it/s][A
 27%|██▋       | 21/79 [00:04<00:12,  4.73it/s][A
 28%|██▊       | 22/79 [00:05<00:18,  3.12it/s][A
 29%|██▉       | 23/79 [00:05<00:16,  3.36it/s][A
 30%|███       | 24/79 [00:05<00:13,  3.94it/s][A
 32%|███▏      | 25/79 [00:06<00:18,  2.96it/s][A
 33%|███▎      | 26/79 [00:06<00:15,  3.43it/s][A
 34%|███▍      | 27/79 [00:06<00:14,  3.69it/s][A
 35%|███▌      | 28/79 [00:06<00:12,  4.03it/s][A
 37%|███▋      | 29/79 [00:06<00:10,  4.56it/s][A
 38%|███▊      | 30/79 [00:07<00:10,  4.46it/s][A
 39%|███▉      | 31/79 [00:07<00:09,  4.88it/s][A
 41%|████      | 32/79 [00:07<00:10,  4.31it/s][A
 42%|████▏     | 33/79 [00:07<00:10,  4.44it/s][A
 43%|████▎     | 34/79 [00:08<00:09,  4.62it/s][A
 44%|████▍     | 35/79 [00:08<00:09,  4.68it/s][A
 46%|████▌     | 36/79 [00:08<00:11,  3.73it/s][A
 47%|████▋     | 37/79 [00:08<00:11,  3.69it/s][A
 48%|████▊     | 38/79 [00:09<00:11,  3.68it/s][A
 49%|████▉     | 39/79 [00:09<00:09,  4.10it/s][A
 51%|█████     | 40/79 [00:09<00:08,  4.43it/s][A
 52%|█████▏    | 41/79 [00:09<00:08,  4.69it/s][A
 53%|█████▎    | 42/79 [00:09<00:07,  4.88it/s][A
 54%|█████▍    | 43/79 [00:10<00:08,  4.46it/s][A
 56%|█████▌    | 44/79 [00:10<00:08,  4.07it/s][A
 57%|█████▋    | 45/79 [00:10<00:07,  4.28it/s][A
 58%|█████▊    | 46/79 [00:10<00:07,  4.50it/s][A
 59%|█████▉    | 47/79 [00:11<00:06,  4.69it/s][A
 61%|██████    | 48/79 [00:11<00:07,  3.99it/s][A
 62%|██████▏   | 49/79 [00:11<00:07,  4.00it/s][A
 63%|██████▎   | 50/79 [00:11<00:06,  4.52it/s][A
 65%|██████▍   | 51/79 [00:12<00:08,  3.39it/s][A
 66%|██████▌   | 52/79 [00:12<00:07,  3.67it/s][A
 67%|██████▋   | 53/79 [00:12<00:07,  3.65it/s][A
 68%|██████▊   | 54/79 [00:13<00:06,  4.00it/s][A
 70%|██████▉   | 55/79 [00:13<00:05,  4.09it/s][A
 71%|███████   | 56/79 [00:13<00:04,  4.62it/s][A
 72%|███████▏  | 57/79 [00:13<00:04,  4.69it/s][A
 73%|███████▎  | 58/79 [00:13<00:04,  4.38it/s][A
 75%|███████▍  | 59/79 [00:14<00:04,  4.49it/s][A
 76%|███████▌  | 60/79 [00:14<00:04,  4.21it/s][A
 77%|███████▋  | 61/79 [00:14<00:04,  3.86it/s][A
 78%|███████▊  | 62/79 [00:14<00:04,  3.56it/s][A
 80%|███████▉  | 63/79 [00:15<00:04,  3.85it/s][A
 81%|████████  | 64/79 [00:15<00:03,  4.23it/s][A
 82%|████████▏ | 65/79 [00:15<00:02,  4.68it/s][A
 84%|████████▎ | 66/79 [00:15<00:02,  4.73it/s][A
 85%|████████▍ | 67/79 [00:15<00:02,  4.72it/s][A
 86%|████████▌ | 68/79 [00:16<00:02,  4.82it/s][A
 87%|████████▋ | 69/79 [00:16<00:02,  4.41it/s][A
 89%|████████▊ | 70/79 [00:16<00:02,  4.42it/s][A
 90%|████████▉ | 71/79 [00:16<00:01,  4.15it/s][A
 91%|█████████ | 72/79 [00:17<00:01,  4.46it/s][A
 92%|█████████▏| 73/79 [00:17<00:01,  4.29it/s][A
 94%|█████████▎| 74/79 [00:17<00:01,  4.00it/s][A
 95%|█████████▍| 75/79 [00:17<00:00,  4.14it/s][A
 96%|█████████▌| 76/79 [00:18<00:00,  4.48it/s][A
 97%|█████████▋| 77/79 [00:18<00:00,  3.88it/s][A
 99%|█████████▊| 78/79 [00:18<00:00,  4.03it/s][A
100%|██████████| 79/79 [00:18<00:00,  4.39it/s][A                                                 
                                               [A{'eval_loss': 0.003982440568506718, 'eval_runtime': 20.1008, 'eval_samples_per_second': 7.811, 'eval_steps_per_second': 3.93, 'epoch': 1.99}
 40%|████      | 176/440 [17:49<25:22,  5.77s/it]
100%|██████████| 79/79 [00:18<00:00,  4.39it/s][A
                                               [A 40%|████      | 177/440 [17:51<50:40, 11.56s/it] 40%|████      | 178/440 [17:58<43:22,  9.93s/it] 41%|████      | 179/440 [18:03<37:48,  8.69s/it] 41%|████      | 180/440 [18:09<33:16,  7.68s/it] 41%|████      | 181/440 [18:16<32:11,  7.46s/it] 41%|████▏     | 182/440 [18:22<31:00,  7.21s/it] 42%|████▏     | 183/440 [18:29<29:42,  6.93s/it] 42%|████▏     | 184/440 [18:34<27:58,  6.56s/it] 42%|████▏     | 185/440 [18:40<27:00,  6.36s/it] 42%|████▏     | 186/440 [18:46<26:41,  6.31s/it] 42%|████▎     | 187/440 [18:52<25:40,  6.09s/it] 43%|████▎     | 188/440 [18:58<25:02,  5.96s/it] 43%|████▎     | 189/440 [19:03<23:45,  5.68s/it] 43%|████▎     | 190/440 [19:08<23:25,  5.62s/it] 43%|████▎     | 191/440 [19:14<23:58,  5.78s/it] 44%|████▎     | 192/440 [19:20<24:13,  5.86s/it] 44%|████▍     | 193/440 [19:26<23:31,  5.71s/it] 44%|████▍     | 194/440 [19:31<23:22,  5.70s/it] 44%|████▍     | 195/440 [19:38<24:03,  5.89s/it] 45%|████▍     | 196/440 [19:43<23:04,  5.67s/it] 45%|████▍     | 197/440 [19:49<23:08,  5.71s/it] 45%|████▌     | 198/440 [19:54<23:01,  5.71s/it] 45%|████▌     | 199/440 [20:01<23:53,  5.95s/it] 45%|████▌     | 200/440 [20:07<24:37,  6.16s/it]                                                 {'loss': 0.1821, 'grad_norm': 0.021168457344174385, 'learning_rate': 6.635339816587109e-05, 'epoch': 2.27}
 45%|████▌     | 200/440 [20:07<24:37,  6.16s/it] 46%|████▌     | 201/440 [20:14<24:29,  6.15s/it] 46%|████▌     | 202/440 [20:19<24:02,  6.06s/it] 46%|████▌     | 203/440 [20:25<23:09,  5.86s/it] 46%|████▋     | 204/440 [20:31<23:18,  5.93s/it] 47%|████▋     | 205/440 [20:36<22:45,  5.81s/it] 47%|████▋     | 206/440 [20:41<21:41,  5.56s/it] 47%|████▋     | 207/440 [20:47<22:12,  5.72s/it] 47%|████▋     | 208/440 [20:53<21:42,  5.61s/it] 48%|████▊     | 209/440 [20:58<20:55,  5.43s/it] 48%|████▊     | 210/440 [21:03<20:20,  5.31s/it] 48%|████▊     | 211/440 [21:08<20:35,  5.40s/it] 48%|████▊     | 212/440 [21:14<20:21,  5.36s/it] 48%|████▊     | 213/440 [21:20<20:49,  5.51s/it] 49%|████▊     | 214/440 [21:25<20:26,  5.43s/it] 49%|████▉     | 215/440 [21:30<20:27,  5.46s/it] 49%|████▉     | 216/440 [21:36<20:39,  5.53s/it] 49%|████▉     | 217/440 [21:41<20:10,  5.43s/it] 50%|████▉     | 218/440 [21:50<23:13,  6.28s/it] 50%|████▉     | 219/440 [21:55<22:13,  6.03s/it] 50%|█████     | 220/440 [22:01<22:30,  6.14s/it] 50%|█████     | 221/440 [22:07<21:58,  6.02s/it] 50%|█████     | 222/440 [22:12<20:52,  5.75s/it] 51%|█████     | 223/440 [22:19<21:52,  6.05s/it] 51%|█████     | 224/440 [22:25<21:49,  6.06s/it] 51%|█████     | 225/440 [22:32<22:12,  6.20s/it] 51%|█████▏    | 226/440 [22:37<21:39,  6.07s/it] 52%|█████▏    | 227/440 [22:43<21:08,  5.96s/it] 52%|█████▏    | 228/440 [22:49<21:03,  5.96s/it] 52%|█████▏    | 229/440 [22:55<21:00,  5.97s/it] 52%|█████▏    | 230/440 [23:01<20:32,  5.87s/it] 52%|█████▎    | 231/440 [23:06<20:14,  5.81s/it] 53%|█████▎    | 232/440 [23:11<19:25,  5.60s/it] 53%|█████▎    | 233/440 [23:17<19:15,  5.58s/it] 53%|█████▎    | 234/440 [23:23<19:11,  5.59s/it] 53%|█████▎    | 235/440 [23:28<18:53,  5.53s/it] 54%|█████▎    | 236/440 [23:33<18:46,  5.52s/it] 54%|█████▍    | 237/440 [23:40<19:16,  5.70s/it] 54%|█████▍    | 238/440 [23:45<18:59,  5.64s/it] 54%|█████▍    | 239/440 [23:51<19:11,  5.73s/it] 55%|█████▍    | 240/440 [23:57<19:19,  5.80s/it] 55%|█████▍    | 241/440 [24:04<20:04,  6.05s/it] 55%|█████▌    | 242/440 [24:10<20:09,  6.11s/it] 55%|█████▌    | 243/440 [24:15<19:11,  5.84s/it] 55%|█████▌    | 244/440 [24:21<19:14,  5.89s/it] 56%|█████▌    | 245/440 [24:27<19:33,  6.02s/it] 56%|█████▌    | 246/440 [24:33<19:05,  5.90s/it] 56%|█████▌    | 247/440 [24:39<19:21,  6.02s/it] 56%|█████▋    | 248/440 [24:45<19:05,  5.97s/it] 57%|█████▋    | 249/440 [24:51<18:51,  5.93s/it] 57%|█████▋    | 250/440 [24:57<18:48,  5.94s/it] 57%|█████▋    | 251/440 [25:03<18:25,  5.85s/it] 57%|█████▋    | 252/440 [25:09<18:44,  5.98s/it] 57%|█████▊    | 253/440 [25:15<18:21,  5.89s/it] 58%|█████▊    | 254/440 [25:21<18:51,  6.08s/it] 58%|█████▊    | 255/440 [25:26<17:58,  5.83s/it] 58%|█████▊    | 256/440 [25:33<18:11,  5.93s/it] 58%|█████▊    | 257/440 [25:39<18:08,  5.95s/it] 59%|█████▊    | 258/440 [25:45<18:30,  6.10s/it] 59%|█████▉    | 259/440 [25:50<17:42,  5.87s/it] 59%|█████▉    | 260/440 [25:56<17:43,  5.91s/it] 59%|█████▉    | 261/440 [26:02<17:27,  5.85s/it] 60%|█████▉    | 262/440 [26:08<17:14,  5.81s/it] 60%|█████▉    | 263/440 [26:15<18:13,  6.18s/it] 60%|██████    | 264/440 [26:20<17:36,  6.00s/it][INFO|trainer.py:4117] 2025-01-12 21:36:23,911 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 21:36:23,911 >>   Num examples = 157
[INFO|trainer.py:4122] 2025-01-12 21:36:23,911 >>   Batch size = 1

  0%|          | 0/79 [00:00<?, ?it/s][A
  3%|▎         | 2/79 [00:00<00:07, 10.29it/s][A
  5%|▌         | 4/79 [00:00<00:12,  6.20it/s][A
  6%|▋         | 5/79 [00:00<00:12,  5.77it/s][A
  8%|▊         | 6/79 [00:01<00:13,  5.56it/s][A
  9%|▉         | 7/79 [00:01<00:14,  4.88it/s][A
 10%|█         | 8/79 [00:01<00:19,  3.56it/s][A
 11%|█▏        | 9/79 [00:01<00:19,  3.66it/s][A
 13%|█▎        | 10/79 [00:02<00:16,  4.16it/s][A
 14%|█▍        | 11/79 [00:02<00:15,  4.31it/s][A
 15%|█▌        | 12/79 [00:02<00:14,  4.75it/s][A
 16%|█▋        | 13/79 [00:02<00:17,  3.83it/s][A
 18%|█▊        | 14/79 [00:03<00:15,  4.08it/s][A
 19%|█▉        | 15/79 [00:03<00:14,  4.36it/s][A
 20%|██        | 16/79 [00:03<00:15,  4.06it/s][A
 22%|██▏       | 17/79 [00:03<00:15,  4.08it/s][A
 23%|██▎       | 18/79 [00:04<00:17,  3.49it/s][A
 24%|██▍       | 19/79 [00:04<00:15,  3.88it/s][A
 25%|██▌       | 20/79 [00:04<00:13,  4.27it/s][A
 27%|██▋       | 21/79 [00:04<00:12,  4.72it/s][A
 28%|██▊       | 22/79 [00:05<00:18,  3.11it/s][A
 29%|██▉       | 23/79 [00:05<00:16,  3.36it/s][A
 30%|███       | 24/79 [00:05<00:14,  3.92it/s][A
 32%|███▏      | 25/79 [00:06<00:18,  2.95it/s][A
 33%|███▎      | 26/79 [00:06<00:15,  3.43it/s][A
 34%|███▍      | 27/79 [00:06<00:14,  3.69it/s][A
 35%|███▌      | 28/79 [00:06<00:12,  3.98it/s][A
 37%|███▋      | 29/79 [00:07<00:11,  4.47it/s][A
 38%|███▊      | 30/79 [00:07<00:11,  4.40it/s][A
 39%|███▉      | 31/79 [00:07<00:09,  4.81it/s][A
 41%|████      | 32/79 [00:07<00:10,  4.29it/s][A
 42%|████▏     | 33/79 [00:07<00:10,  4.43it/s][A
 43%|████▎     | 34/79 [00:08<00:09,  4.61it/s][A
 44%|████▍     | 35/79 [00:08<00:09,  4.67it/s][A
 46%|████▌     | 36/79 [00:08<00:11,  3.72it/s][A
 47%|████▋     | 37/79 [00:08<00:11,  3.68it/s][A
 48%|████▊     | 38/79 [00:09<00:11,  3.67it/s][A
 49%|████▉     | 39/79 [00:09<00:09,  4.08it/s][A
 51%|█████     | 40/79 [00:09<00:08,  4.41it/s][A
 52%|█████▏    | 41/79 [00:09<00:08,  4.67it/s][A
 53%|█████▎    | 42/79 [00:10<00:07,  4.84it/s][A
 54%|█████▍    | 43/79 [00:10<00:08,  4.43it/s][A
 56%|█████▌    | 44/79 [00:10<00:08,  4.03it/s][A
 57%|█████▋    | 45/79 [00:10<00:07,  4.25it/s][A
 58%|█████▊    | 46/79 [00:10<00:07,  4.49it/s][A
 59%|█████▉    | 47/79 [00:11<00:06,  4.72it/s][A
 61%|██████    | 48/79 [00:11<00:07,  4.00it/s][A
 62%|██████▏   | 49/79 [00:11<00:07,  3.99it/s][A
 63%|██████▎   | 50/79 [00:11<00:06,  4.50it/s][A
 65%|██████▍   | 51/79 [00:12<00:08,  3.34it/s][A
 66%|██████▌   | 52/79 [00:12<00:07,  3.61it/s][A
 67%|██████▋   | 53/79 [00:12<00:07,  3.60it/s][A
 68%|██████▊   | 54/79 [00:13<00:06,  3.97it/s][A
 70%|██████▉   | 55/79 [00:13<00:05,  4.08it/s][A
 71%|███████   | 56/79 [00:13<00:04,  4.61it/s][A
 72%|███████▏  | 57/79 [00:13<00:04,  4.68it/s][A
 73%|███████▎  | 58/79 [00:13<00:04,  4.41it/s][A
 75%|███████▍  | 59/79 [00:14<00:04,  4.51it/s][A
 76%|███████▌  | 60/79 [00:14<00:04,  4.22it/s][A
 77%|███████▋  | 61/79 [00:14<00:04,  3.87it/s][A
 78%|███████▊  | 62/79 [00:15<00:04,  3.57it/s][A
 80%|███████▉  | 63/79 [00:15<00:04,  3.85it/s][A
 81%|████████  | 64/79 [00:15<00:03,  4.23it/s][A
 82%|████████▏ | 65/79 [00:15<00:03,  4.65it/s][A
 84%|████████▎ | 66/79 [00:15<00:02,  4.70it/s][A
 85%|████████▍ | 67/79 [00:16<00:02,  4.70it/s][A
 86%|████████▌ | 68/79 [00:16<00:02,  4.81it/s][A
 87%|████████▋ | 69/79 [00:16<00:02,  4.40it/s][A
 89%|████████▊ | 70/79 [00:16<00:02,  4.41it/s][A
 90%|████████▉ | 71/79 [00:17<00:01,  4.14it/s][A
 91%|█████████ | 72/79 [00:17<00:01,  4.45it/s][A
 92%|█████████▏| 73/79 [00:17<00:01,  4.29it/s][A
 94%|█████████▎| 74/79 [00:17<00:01,  3.97it/s][A
 95%|█████████▍| 75/79 [00:17<00:00,  4.11it/s][A
 96%|█████████▌| 76/79 [00:18<00:00,  4.45it/s][A
 97%|█████████▋| 77/79 [00:18<00:00,  3.86it/s][A
 99%|█████████▊| 78/79 [00:18<00:00,  4.02it/s][A
100%|██████████| 79/79 [00:18<00:00,  4.38it/s][A                                                 
                                               [A{'eval_loss': 0.004031541291624308, 'eval_runtime': 19.0808, 'eval_samples_per_second': 8.228, 'eval_steps_per_second': 4.14, 'epoch': 2.99}
 60%|██████    | 264/440 [26:44<17:36,  6.00s/it]
100%|██████████| 79/79 [00:18<00:00,  4.38it/s][A
                                               [A 60%|██████    | 265/440 [26:45<34:12, 11.73s/it] 60%|██████    | 266/440 [26:51<28:29,  9.83s/it] 61%|██████    | 267/440 [26:57<25:03,  8.69s/it] 61%|██████    | 268/440 [27:02<21:44,  7.58s/it] 61%|██████    | 269/440 [27:08<20:34,  7.22s/it] 61%|██████▏   | 270/440 [27:14<19:20,  6.83s/it] 62%|██████▏   | 271/440 [27:20<18:25,  6.54s/it] 62%|██████▏   | 272/440 [27:26<18:04,  6.45s/it] 62%|██████▏   | 273/440 [27:32<17:03,  6.13s/it] 62%|██████▏   | 274/440 [27:38<16:46,  6.06s/it] 62%|██████▎   | 275/440 [27:44<16:57,  6.17s/it] 63%|██████▎   | 276/440 [27:50<16:46,  6.14s/it] 63%|██████▎   | 277/440 [27:56<16:20,  6.01s/it] 63%|██████▎   | 278/440 [28:02<16:33,  6.13s/it] 63%|██████▎   | 279/440 [28:09<17:19,  6.46s/it] 64%|██████▎   | 280/440 [28:15<16:48,  6.30s/it] 64%|██████▍   | 281/440 [28:22<17:10,  6.48s/it] 64%|██████▍   | 282/440 [28:27<16:02,  6.09s/it] 64%|██████▍   | 283/440 [28:33<15:12,  5.81s/it] 65%|██████▍   | 284/440 [28:39<15:24,  5.93s/it] 65%|██████▍   | 285/440 [28:45<15:52,  6.14s/it] 65%|██████▌   | 286/440 [28:51<15:18,  5.96s/it] 65%|██████▌   | 287/440 [28:56<14:22,  5.64s/it] 65%|██████▌   | 288/440 [29:01<13:54,  5.49s/it] 66%|██████▌   | 289/440 [29:07<14:10,  5.63s/it] 66%|██████▌   | 290/440 [29:13<14:45,  5.90s/it] 66%|██████▌   | 291/440 [29:19<14:39,  5.90s/it] 66%|██████▋   | 292/440 [29:25<14:05,  5.71s/it] 67%|██████▋   | 293/440 [29:31<14:30,  5.92s/it] 67%|██████▋   | 294/440 [29:37<14:31,  5.97s/it] 67%|██████▋   | 295/440 [29:43<14:12,  5.88s/it] 67%|██████▋   | 296/440 [29:48<13:57,  5.82s/it] 68%|██████▊   | 297/440 [29:54<13:45,  5.77s/it] 68%|██████▊   | 298/440 [30:00<13:36,  5.75s/it] 68%|██████▊   | 299/440 [30:06<13:34,  5.78s/it] 68%|██████▊   | 300/440 [30:12<13:46,  5.90s/it] 68%|██████▊   | 301/440 [30:17<13:07,  5.66s/it] 69%|██████▊   | 302/440 [30:23<12:58,  5.64s/it] 69%|██████▉   | 303/440 [30:29<13:16,  5.82s/it] 69%|██████▉   | 304/440 [30:34<13:00,  5.74s/it] 69%|██████▉   | 305/440 [30:39<12:24,  5.51s/it] 70%|██████▉   | 306/440 [30:45<12:21,  5.53s/it] 70%|██████▉   | 307/440 [30:50<12:15,  5.53s/it] 70%|███████   | 308/440 [30:56<12:20,  5.61s/it] 70%|███████   | 309/440 [31:02<12:18,  5.64s/it] 70%|███████   | 310/440 [31:09<12:55,  5.97s/it] 71%|███████   | 311/440 [31:15<12:50,  5.97s/it] 71%|███████   | 312/440 [31:21<12:40,  5.94s/it] 71%|███████   | 313/440 [31:26<12:21,  5.84s/it] 71%|███████▏  | 314/440 [31:32<12:30,  5.96s/it] 72%|███████▏  | 315/440 [31:37<11:39,  5.60s/it] 72%|███████▏  | 316/440 [31:43<11:53,  5.75s/it] 72%|███████▏  | 317/440 [31:49<12:00,  5.86s/it] 72%|███████▏  | 318/440 [31:56<12:21,  6.08s/it] 72%|███████▎  | 319/440 [32:02<12:01,  5.97s/it] 73%|███████▎  | 320/440 [32:08<12:07,  6.06s/it] 73%|███████▎  | 321/440 [32:14<11:47,  5.95s/it] 73%|███████▎  | 322/440 [32:19<11:37,  5.91s/it] 73%|███████▎  | 323/440 [32:25<11:18,  5.80s/it] 74%|███████▎  | 324/440 [32:31<11:28,  5.93s/it] 74%|███████▍  | 325/440 [32:37<11:23,  5.95s/it] 74%|███████▍  | 326/440 [32:43<11:17,  5.95s/it] 74%|███████▍  | 327/440 [32:48<10:49,  5.75s/it] 75%|███████▍  | 328/440 [32:54<10:37,  5.69s/it] 75%|███████▍  | 329/440 [33:00<10:26,  5.64s/it] 75%|███████▌  | 330/440 [33:05<10:26,  5.70s/it] 75%|███████▌  | 331/440 [33:11<10:27,  5.76s/it] 75%|███████▌  | 332/440 [33:18<10:53,  6.05s/it] 76%|███████▌  | 333/440 [33:24<10:32,  5.92s/it] 76%|███████▌  | 334/440 [33:30<10:38,  6.02s/it] 76%|███████▌  | 335/440 [33:36<10:29,  5.99s/it] 76%|███████▋  | 336/440 [33:42<10:35,  6.11s/it] 77%|███████▋  | 337/440 [33:48<10:30,  6.12s/it] 77%|███████▋  | 338/440 [33:54<10:08,  5.96s/it] 77%|███████▋  | 339/440 [34:00<10:08,  6.02s/it] 77%|███████▋  | 340/440 [34:06<09:49,  5.89s/it] 78%|███████▊  | 341/440 [34:12<09:47,  5.94s/it] 78%|███████▊  | 342/440 [34:17<09:32,  5.84s/it] 78%|███████▊  | 343/440 [34:23<09:18,  5.76s/it] 78%|███████▊  | 344/440 [34:29<09:17,  5.81s/it] 78%|███████▊  | 345/440 [34:36<09:53,  6.25s/it] 79%|███████▊  | 346/440 [34:42<09:33,  6.10s/it] 79%|███████▉  | 347/440 [34:48<09:32,  6.16s/it] 79%|███████▉  | 348/440 [34:53<08:59,  5.87s/it] 79%|███████▉  | 349/440 [35:00<09:05,  6.00s/it] 80%|███████▉  | 350/440 [35:06<09:06,  6.07s/it] 80%|███████▉  | 351/440 [35:12<09:06,  6.14s/it] 80%|████████  | 352/440 [35:17<08:29,  5.79s/it] 80%|████████  | 353/440 [35:23<08:38,  5.95s/it][INFO|trainer.py:4117] 2025-01-12 21:45:22,797 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 21:45:22,797 >>   Num examples = 157
[INFO|trainer.py:4122] 2025-01-12 21:45:22,797 >>   Batch size = 1

  0%|          | 0/79 [00:00<?, ?it/s][A
  3%|▎         | 2/79 [00:00<00:07, 10.27it/s][A
  5%|▌         | 4/79 [00:00<00:11,  6.27it/s][A
  6%|▋         | 5/79 [00:00<00:12,  5.84it/s][A
  8%|▊         | 6/79 [00:00<00:13,  5.60it/s][A
  9%|▉         | 7/79 [00:01<00:14,  4.90it/s][A
 10%|█         | 8/79 [00:01<00:19,  3.56it/s][A
 11%|█▏        | 9/79 [00:01<00:19,  3.66it/s][A
 13%|█▎        | 10/79 [00:02<00:16,  4.17it/s][A
 14%|█▍        | 11/79 [00:02<00:15,  4.32it/s][A
 15%|█▌        | 12/79 [00:02<00:13,  4.80it/s][A
 16%|█▋        | 13/79 [00:02<00:17,  3.85it/s][A
 18%|█▊        | 14/79 [00:03<00:15,  4.09it/s][A
 19%|█▉        | 15/79 [00:03<00:14,  4.38it/s][A
 20%|██        | 16/79 [00:03<00:15,  4.10it/s][A
 22%|██▏       | 17/79 [00:03<00:14,  4.14it/s][A
 23%|██▎       | 18/79 [00:04<00:17,  3.51it/s][A
 24%|██▍       | 19/79 [00:04<00:15,  3.90it/s][A
 25%|██▌       | 20/79 [00:04<00:13,  4.29it/s][A
 27%|██▋       | 21/79 [00:04<00:12,  4.73it/s][A
 28%|██▊       | 22/79 [00:05<00:18,  3.14it/s][A
 29%|██▉       | 23/79 [00:05<00:16,  3.38it/s][A
 30%|███       | 24/79 [00:05<00:13,  3.96it/s][A
 32%|███▏      | 25/79 [00:06<00:18,  2.96it/s][A
 33%|███▎      | 26/79 [00:06<00:15,  3.44it/s][A
 34%|███▍      | 27/79 [00:06<00:14,  3.69it/s][A
 35%|███▌      | 28/79 [00:06<00:12,  4.02it/s][A
 37%|███▋      | 29/79 [00:06<00:11,  4.52it/s][A
 38%|███▊      | 30/79 [00:07<00:11,  4.42it/s][A
 39%|███▉      | 31/79 [00:07<00:09,  4.84it/s][A
 41%|████      | 32/79 [00:07<00:10,  4.30it/s][A
 42%|████▏     | 33/79 [00:07<00:10,  4.43it/s][A
 43%|████▎     | 34/79 [00:08<00:09,  4.62it/s][A
 44%|████▍     | 35/79 [00:08<00:09,  4.67it/s][A
 46%|████▌     | 36/79 [00:08<00:11,  3.71it/s][A
 47%|████▋     | 37/79 [00:08<00:11,  3.67it/s][A
 48%|████▊     | 38/79 [00:09<00:11,  3.66it/s][A
 49%|████▉     | 39/79 [00:09<00:09,  4.07it/s][A
 51%|█████     | 40/79 [00:09<00:08,  4.40it/s][A
 52%|█████▏    | 41/79 [00:09<00:08,  4.66it/s][A
 53%|█████▎    | 42/79 [00:09<00:07,  4.85it/s][A
 54%|█████▍    | 43/79 [00:10<00:08,  4.43it/s][A
 56%|█████▌    | 44/79 [00:10<00:08,  4.04it/s][A
 57%|█████▋    | 45/79 [00:10<00:07,  4.25it/s][A
 58%|█████▊    | 46/79 [00:10<00:07,  4.45it/s][A
 59%|█████▉    | 47/79 [00:11<00:06,  4.65it/s][A
 61%|██████    | 48/79 [00:11<00:07,  3.96it/s][A
 62%|██████▏   | 49/79 [00:11<00:07,  3.97it/s][A
 63%|██████▎   | 50/79 [00:11<00:06,  4.45it/s][A
 65%|██████▍   | 51/79 [00:12<00:08,  3.33it/s][A
 66%|██████▌   | 52/79 [00:12<00:07,  3.61it/s][A
 67%|██████▋   | 53/79 [00:12<00:07,  3.60it/s][A
 68%|██████▊   | 54/79 [00:13<00:06,  3.97it/s][A
 70%|██████▉   | 55/79 [00:13<00:05,  4.04it/s][A
 71%|███████   | 56/79 [00:13<00:05,  4.58it/s][A
 72%|███████▏  | 57/79 [00:13<00:04,  4.66it/s][A
 73%|███████▎  | 58/79 [00:13<00:04,  4.34it/s][A
 75%|███████▍  | 59/79 [00:14<00:04,  4.46it/s][A
 76%|███████▌  | 60/79 [00:14<00:04,  4.19it/s][A
 77%|███████▋  | 61/79 [00:14<00:04,  3.85it/s][A
 78%|███████▊  | 62/79 [00:15<00:04,  3.56it/s][A
 80%|███████▉  | 63/79 [00:15<00:04,  3.85it/s][A
 81%|████████  | 64/79 [00:15<00:03,  4.23it/s][A
 82%|████████▏ | 65/79 [00:15<00:02,  4.67it/s][A
 84%|████████▎ | 66/79 [00:15<00:02,  4.72it/s][A
 85%|████████▍ | 67/79 [00:16<00:02,  4.71it/s][A
 86%|████████▌ | 68/79 [00:16<00:02,  4.81it/s][A
 87%|████████▋ | 69/79 [00:16<00:02,  4.40it/s][A
 89%|████████▊ | 70/79 [00:16<00:02,  4.42it/s][A
 90%|████████▉ | 71/79 [00:16<00:01,  4.13it/s][A
 91%|█████████ | 72/79 [00:17<00:01,  4.44it/s][A
 92%|█████████▏| 73/79 [00:17<00:01,  4.28it/s][A
 94%|█████████▎| 74/79 [00:17<00:01,  3.95it/s][A
 95%|█████████▍| 75/79 [00:17<00:00,  4.10it/s][A
 96%|█████████▌| 76/79 [00:18<00:00,  4.44it/s][A
 97%|█████████▋| 77/79 [00:18<00:00,  3.86it/s][A
 99%|█████████▊| 78/79 [00:18<00:00,  4.02it/s][A
100%|██████████| 79/79 [00:18<00:00,  4.37it/s][A                                                 
                                               [A{'eval_loss': 0.0038458644412457943, 'eval_runtime': 19.0622, 'eval_samples_per_second': 8.236, 'eval_steps_per_second': 4.144, 'epoch': 4.0}
 80%|████████  | 353/440 [35:43<08:38,  5.95s/it]
100%|██████████| 79/79 [00:18<00:00,  4.37it/s][A
                                               [A 80%|████████  | 354/440 [35:49<16:44, 11.68s/it] 81%|████████  | 355/440 [35:54<14:00,  9.89s/it] 81%|████████  | 356/440 [36:00<12:14,  8.74s/it] 81%|████████  | 357/440 [36:07<11:09,  8.06s/it] 81%|████████▏ | 358/440 [36:14<10:50,  7.94s/it] 82%|████████▏ | 359/440 [36:21<10:04,  7.47s/it] 82%|████████▏ | 360/440 [36:26<09:08,  6.86s/it] 82%|████████▏ | 361/440 [36:32<08:31,  6.47s/it] 82%|████████▏ | 362/440 [36:38<08:18,  6.39s/it] 82%|████████▎ | 363/440 [36:45<08:17,  6.46s/it] 83%|████████▎ | 364/440 [36:50<07:38,  6.03s/it] 83%|████████▎ | 365/440 [36:55<07:20,  5.88s/it] 83%|████████▎ | 366/440 [37:02<07:25,  6.02s/it] 83%|████████▎ | 367/440 [37:09<07:50,  6.44s/it] 84%|████████▎ | 368/440 [37:15<07:29,  6.24s/it] 84%|████████▍ | 369/440 [37:21<07:16,  6.15s/it] 84%|████████▍ | 370/440 [37:26<06:57,  5.96s/it] 84%|████████▍ | 371/440 [37:32<06:41,  5.82s/it] 85%|████████▍ | 372/440 [37:38<06:45,  5.97s/it] 85%|████████▍ | 373/440 [37:43<06:26,  5.77s/it] 85%|████████▌ | 374/440 [37:48<06:06,  5.55s/it] 85%|████████▌ | 375/440 [37:54<05:57,  5.50s/it] 85%|████████▌ | 376/440 [38:00<06:03,  5.68s/it] 86%|████████▌ | 377/440 [38:05<05:45,  5.48s/it] 86%|████████▌ | 378/440 [38:10<05:37,  5.45s/it] 86%|████████▌ | 379/440 [38:16<05:41,  5.60s/it] 86%|████████▋ | 380/440 [38:23<05:53,  5.90s/it] 87%|████████▋ | 381/440 [38:28<05:40,  5.78s/it] 87%|████████▋ | 382/440 [38:34<05:41,  5.88s/it] 87%|████████▋ | 383/440 [38:41<05:42,  6.01s/it] 87%|████████▋ | 384/440 [38:47<05:46,  6.19s/it] 88%|████████▊ | 385/440 [38:53<05:35,  6.10s/it] 88%|████████▊ | 386/440 [39:00<05:34,  6.20s/it] 88%|████████▊ | 387/440 [39:05<05:18,  6.01s/it] 88%|████████▊ | 388/440 [39:11<05:09,  5.95s/it] 88%|████████▊ | 389/440 [39:17<05:00,  5.89s/it] 89%|████████▊ | 390/440 [39:22<04:49,  5.78s/it] 89%|████████▉ | 391/440 [39:28<04:50,  5.92s/it] 89%|████████▉ | 392/440 [39:34<04:41,  5.87s/it] 89%|████████▉ | 393/440 [39:40<04:33,  5.82s/it] 90%|████████▉ | 394/440 [39:45<04:21,  5.68s/it] 90%|████████▉ | 395/440 [39:52<04:25,  5.90s/it] 90%|█████████ | 396/440 [39:57<04:11,  5.71s/it] 90%|█████████ | 397/440 [40:03<04:11,  5.85s/it] 90%|█████████ | 398/440 [40:09<04:03,  5.80s/it] 91%|█████████ | 399/440 [40:15<03:58,  5.82s/it] 91%|█████████ | 400/440 [40:20<03:52,  5.81s/it]                                                 {'loss': 0.0005, 'grad_norm': 0.008855363354086876, 'learning_rate': 2.496444112952734e-06, 'epoch': 4.53}
 91%|█████████ | 400/440 [40:20<03:52,  5.81s/it] 91%|█████████ | 401/440 [40:26<03:44,  5.75s/it] 91%|█████████▏| 402/440 [40:31<03:33,  5.62s/it] 92%|█████████▏| 403/440 [40:37<03:23,  5.51s/it] 92%|█████████▏| 404/440 [40:43<03:23,  5.65s/it] 92%|█████████▏| 405/440 [40:49<03:22,  5.78s/it] 92%|█████████▏| 406/440 [40:54<03:13,  5.68s/it] 92%|█████████▎| 407/440 [41:01<03:14,  5.89s/it] 93%|█████████▎| 408/440 [41:06<03:07,  5.85s/it] 93%|█████████▎| 409/440 [41:12<03:03,  5.91s/it] 93%|█████████▎| 410/440 [41:18<02:55,  5.86s/it] 93%|█████████▎| 411/440 [41:24<02:46,  5.73s/it] 94%|█████████▎| 412/440 [41:30<02:49,  6.06s/it] 94%|█████████▍| 413/440 [41:36<02:42,  6.03s/it] 94%|█████████▍| 414/440 [41:43<02:45,  6.35s/it] 94%|█████████▍| 415/440 [41:50<02:39,  6.37s/it] 95%|█████████▍| 416/440 [41:55<02:24,  6.02s/it] 95%|█████████▍| 417/440 [42:00<02:14,  5.85s/it] 95%|█████████▌| 418/440 [42:06<02:03,  5.62s/it] 95%|█████████▌| 419/440 [42:11<01:55,  5.49s/it] 95%|█████████▌| 420/440 [42:16<01:50,  5.53s/it] 96%|█████████▌| 421/440 [42:22<01:44,  5.49s/it] 96%|█████████▌| 422/440 [42:27<01:39,  5.50s/it] 96%|█████████▌| 423/440 [42:33<01:36,  5.68s/it] 96%|█████████▋| 424/440 [42:39<01:30,  5.69s/it] 97%|█████████▋| 425/440 [42:45<01:25,  5.73s/it] 97%|█████████▋| 426/440 [42:51<01:21,  5.86s/it] 97%|█████████▋| 427/440 [42:58<01:19,  6.10s/it] 97%|█████████▋| 428/440 [43:04<01:14,  6.19s/it] 98%|█████████▊| 429/440 [43:10<01:06,  6.03s/it] 98%|█████████▊| 430/440 [43:16<01:01,  6.14s/it] 98%|█████████▊| 431/440 [43:22<00:53,  5.99s/it] 98%|█████████▊| 432/440 [43:29<00:50,  6.35s/it] 98%|█████████▊| 433/440 [43:35<00:43,  6.18s/it] 99%|█████████▊| 434/440 [43:41<00:37,  6.20s/it] 99%|█████████▉| 435/440 [43:47<00:30,  6.15s/it] 99%|█████████▉| 436/440 [43:53<00:23,  5.97s/it] 99%|█████████▉| 437/440 [43:58<00:17,  5.87s/it]100%|█████████▉| 438/440 [44:05<00:12,  6.10s/it]100%|█████████▉| 439/440 [44:10<00:05,  5.90s/it]100%|██████████| 440/440 [44:16<00:00,  5.70s/it][INFO|trainer.py:3801] 2025-01-12 21:54:14,929 >> Saving model checkpoint to saves/qwen-14b-hy-e5/lora/sft/checkpoint-440
[INFO|configuration_utils.py:677] 2025-01-12 21:54:14,949 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 21:54:14,949 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2025-01-12 21:54:15,074 >> tokenizer config file saved in saves/qwen-14b-hy-e5/lora/sft/checkpoint-440/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-12 21:54:15,074 >> Special tokens file saved in saves/qwen-14b-hy-e5/lora/sft/checkpoint-440/special_tokens_map.json
[INFO|trainer.py:4117] 2025-01-12 21:54:15,312 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 21:54:15,312 >>   Num examples = 157
[INFO|trainer.py:4122] 2025-01-12 21:54:15,312 >>   Batch size = 1

  0%|          | 0/79 [00:00<?, ?it/s][A
  3%|▎         | 2/79 [00:00<00:07, 10.27it/s][A
  5%|▌         | 4/79 [00:00<00:11,  6.27it/s][A
  6%|▋         | 5/79 [00:00<00:12,  5.87it/s][A
  8%|▊         | 6/79 [00:00<00:12,  5.62it/s][A
  9%|▉         | 7/79 [00:01<00:14,  4.92it/s][A
 10%|█         | 8/79 [00:01<00:19,  3.57it/s][A
 11%|█▏        | 9/79 [00:01<00:19,  3.67it/s][A
 13%|█▎        | 10/79 [00:02<00:16,  4.17it/s][A
 14%|█▍        | 11/79 [00:02<00:15,  4.33it/s][A
 15%|█▌        | 12/79 [00:02<00:13,  4.82it/s][A
 16%|█▋        | 13/79 [00:02<00:16,  3.89it/s][A
 18%|█▊        | 14/79 [00:03<00:15,  4.12it/s][A
 19%|█▉        | 15/79 [00:03<00:14,  4.40it/s][A
 20%|██        | 16/79 [00:03<00:15,  4.09it/s][A
 22%|██▏       | 17/79 [00:03<00:14,  4.14it/s][A
 23%|██▎       | 18/79 [00:04<00:17,  3.52it/s][A
 24%|██▍       | 19/79 [00:04<00:15,  3.91it/s][A
 25%|██▌       | 20/79 [00:04<00:13,  4.30it/s][A
 27%|██▋       | 21/79 [00:04<00:12,  4.74it/s][A
 28%|██▊       | 22/79 [00:05<00:18,  3.15it/s][A
 29%|██▉       | 23/79 [00:05<00:16,  3.39it/s][A
 30%|███       | 24/79 [00:05<00:13,  3.96it/s][A
 32%|███▏      | 25/79 [00:06<00:18,  2.96it/s][A
 33%|███▎      | 26/79 [00:06<00:15,  3.44it/s][A
 34%|███▍      | 27/79 [00:06<00:14,  3.70it/s][A
 35%|███▌      | 28/79 [00:06<00:12,  4.04it/s][A
 37%|███▋      | 29/79 [00:06<00:10,  4.57it/s][A
 38%|███▊      | 30/79 [00:07<00:10,  4.50it/s][A
 39%|███▉      | 31/79 [00:07<00:09,  4.90it/s][A
 41%|████      | 32/79 [00:07<00:10,  4.32it/s][A
 42%|████▏     | 33/79 [00:07<00:10,  4.45it/s][A
 43%|████▎     | 34/79 [00:08<00:09,  4.63it/s][A
 44%|████▍     | 35/79 [00:08<00:09,  4.68it/s][A
 46%|████▌     | 36/79 [00:08<00:11,  3.72it/s][A
 47%|████▋     | 37/79 [00:08<00:11,  3.68it/s][A
 48%|████▊     | 38/79 [00:09<00:11,  3.67it/s][A
 49%|████▉     | 39/79 [00:09<00:09,  4.08it/s][A
 51%|█████     | 40/79 [00:09<00:08,  4.41it/s][A
 52%|█████▏    | 41/79 [00:09<00:08,  4.67it/s][A
 53%|█████▎    | 42/79 [00:09<00:07,  4.85it/s][A
 54%|█████▍    | 43/79 [00:10<00:08,  4.44it/s][A
 56%|█████▌    | 44/79 [00:10<00:08,  4.07it/s][A
 57%|█████▋    | 45/79 [00:10<00:07,  4.28it/s][A
 58%|█████▊    | 46/79 [00:10<00:07,  4.50it/s][A
 59%|█████▉    | 47/79 [00:11<00:06,  4.71it/s][A
 61%|██████    | 48/79 [00:11<00:07,  4.00it/s][A
 62%|██████▏   | 49/79 [00:11<00:07,  3.99it/s][A
 63%|██████▎   | 50/79 [00:11<00:06,  4.49it/s][A
 65%|██████▍   | 51/79 [00:12<00:08,  3.36it/s][A
 66%|██████▌   | 52/79 [00:12<00:07,  3.61it/s][A
 67%|██████▋   | 53/79 [00:12<00:07,  3.60it/s][A
 68%|██████▊   | 54/79 [00:13<00:06,  3.97it/s][A
 70%|██████▉   | 55/79 [00:13<00:05,  4.04it/s][A
 71%|███████   | 56/79 [00:13<00:05,  4.58it/s][A
 72%|███████▏  | 57/79 [00:13<00:04,  4.66it/s][A
 73%|███████▎  | 58/79 [00:13<00:04,  4.39it/s][A
 75%|███████▍  | 59/79 [00:14<00:04,  4.50it/s][A
 76%|███████▌  | 60/79 [00:14<00:04,  4.22it/s][A
 77%|███████▋  | 61/79 [00:14<00:04,  3.86it/s][A
 78%|███████▊  | 62/79 [00:14<00:04,  3.56it/s][A
 80%|███████▉  | 63/79 [00:15<00:04,  3.85it/s][A
 81%|████████  | 64/79 [00:15<00:03,  4.23it/s][A
 82%|████████▏ | 65/79 [00:15<00:03,  4.63it/s][A
 84%|████████▎ | 66/79 [00:15<00:02,  4.68it/s][A
 85%|████████▍ | 67/79 [00:15<00:02,  4.68it/s][A
 86%|████████▌ | 68/79 [00:16<00:02,  4.80it/s][A
 87%|████████▋ | 69/79 [00:16<00:02,  4.39it/s][A
 89%|████████▊ | 70/79 [00:16<00:02,  4.41it/s][A
 90%|████████▉ | 71/79 [00:16<00:01,  4.14it/s][A
 91%|█████████ | 72/79 [00:17<00:01,  4.44it/s][A
 92%|█████████▏| 73/79 [00:17<00:01,  4.28it/s][A
 94%|█████████▎| 74/79 [00:17<00:01,  3.99it/s][A
 95%|█████████▍| 75/79 [00:17<00:00,  4.13it/s][A
 96%|█████████▌| 76/79 [00:18<00:00,  4.47it/s][A
 97%|█████████▋| 77/79 [00:18<00:00,  3.87it/s][A
 99%|█████████▊| 78/79 [00:18<00:00,  4.02it/s][A
100%|██████████| 79/79 [00:18<00:00,  4.38it/s][A                                                 
                                               [A{'eval_loss': 0.0038796509616076946, 'eval_runtime': 19.0041, 'eval_samples_per_second': 8.261, 'eval_steps_per_second': 4.157, 'epoch': 4.99}
100%|██████████| 440/440 [44:35<00:00,  5.70s/it]
100%|██████████| 79/79 [00:18<00:00,  4.38it/s][A
                                               [A[INFO|trainer.py:2584] 2025-01-12 21:54:34,316 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 2685.0647, 'train_samples_per_second': 2.627, 'train_steps_per_second': 0.164, 'train_loss': 0.08299096459099516, 'epoch': 4.99}
100%|██████████| 440/440 [44:35<00:00,  5.70s/it]100%|██████████| 440/440 [44:35<00:00,  6.08s/it]
[INFO|trainer.py:3801] 2025-01-12 21:54:34,318 >> Saving model checkpoint to saves/qwen-14b-hy-e5/lora/sft
[INFO|configuration_utils.py:677] 2025-01-12 21:54:34,338 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-12 21:54:34,338 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2025-01-12 21:54:34,444 >> tokenizer config file saved in saves/qwen-14b-hy-e5/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-12 21:54:34,445 >> Special tokens file saved in saves/qwen-14b-hy-e5/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =      4.9858
  total_flos               = 360377598GF
  train_loss               =       0.083
  train_runtime            =  0:44:45.06
  train_samples_per_second =       2.627
  train_steps_per_second   =       0.164
Figure saved at: saves/qwen-14b-hy-e5/lora/sft/training_loss.png
Figure saved at: saves/qwen-14b-hy-e5/lora/sft/training_eval_loss.png
[WARNING|2025-01-12 21:54:34] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4117] 2025-01-12 21:54:34,594 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-12 21:54:34,594 >>   Num examples = 157
[INFO|trainer.py:4122] 2025-01-12 21:54:34,594 >>   Batch size = 1
  0%|          | 0/79 [00:00<?, ?it/s]  3%|▎         | 2/79 [00:00<00:07, 10.26it/s]  5%|▌         | 4/79 [00:00<00:11,  6.28it/s]  6%|▋         | 5/79 [00:00<00:12,  5.80it/s]  8%|▊         | 6/79 [00:00<00:13,  5.58it/s]  9%|▉         | 7/79 [00:01<00:14,  4.89it/s] 10%|█         | 8/79 [00:01<00:19,  3.56it/s] 11%|█▏        | 9/79 [00:01<00:19,  3.66it/s] 13%|█▎        | 10/79 [00:02<00:16,  4.16it/s] 14%|█▍        | 11/79 [00:02<00:15,  4.32it/s] 15%|█▌        | 12/79 [00:02<00:13,  4.81it/s] 16%|█▋        | 13/79 [00:02<00:16,  3.89it/s] 18%|█▊        | 14/79 [00:03<00:15,  4.12it/s] 19%|█▉        | 15/79 [00:03<00:14,  4.40it/s] 20%|██        | 16/79 [00:03<00:15,  4.04it/s] 22%|██▏       | 17/79 [00:03<00:15,  4.10it/s] 23%|██▎       | 18/79 [00:04<00:17,  3.49it/s] 24%|██▍       | 19/79 [00:04<00:15,  3.88it/s] 25%|██▌       | 20/79 [00:04<00:13,  4.28it/s] 27%|██▋       | 21/79 [00:04<00:12,  4.72it/s] 28%|██▊       | 22/79 [00:05<00:18,  3.12it/s] 29%|██▉       | 23/79 [00:05<00:16,  3.37it/s] 30%|███       | 24/79 [00:05<00:13,  3.94it/s] 32%|███▏      | 25/79 [00:06<00:18,  2.96it/s] 33%|███▎      | 26/79 [00:06<00:15,  3.43it/s] 34%|███▍      | 27/79 [00:06<00:14,  3.69it/s] 35%|███▌      | 28/79 [00:06<00:12,  4.03it/s] 37%|███▋      | 29/79 [00:06<00:10,  4.56it/s] 38%|███▊      | 30/79 [00:07<00:10,  4.47it/s] 39%|███▉      | 31/79 [00:07<00:09,  4.89it/s] 41%|████      | 32/79 [00:07<00:10,  4.33it/s] 42%|████▏     | 33/79 [00:07<00:10,  4.46it/s] 43%|████▎     | 34/79 [00:08<00:09,  4.64it/s] 44%|████▍     | 35/79 [00:08<00:09,  4.69it/s] 46%|████▌     | 36/79 [00:08<00:11,  3.72it/s] 47%|████▋     | 37/79 [00:08<00:11,  3.67it/s] 48%|████▊     | 38/79 [00:09<00:11,  3.66it/s] 49%|████▉     | 39/79 [00:09<00:09,  4.09it/s] 51%|█████     | 40/79 [00:09<00:08,  4.41it/s] 52%|█████▏    | 41/79 [00:09<00:08,  4.67it/s] 53%|█████▎    | 42/79 [00:09<00:07,  4.86it/s] 54%|█████▍    | 43/79 [00:10<00:08,  4.44it/s] 56%|█████▌    | 44/79 [00:10<00:08,  4.07it/s] 57%|█████▋    | 45/79 [00:10<00:07,  4.28it/s] 58%|█████▊    | 46/79 [00:10<00:07,  4.51it/s] 59%|█████▉    | 47/79 [00:11<00:06,  4.72it/s] 61%|██████    | 48/79 [00:11<00:07,  4.01it/s] 62%|██████▏   | 49/79 [00:11<00:07,  4.01it/s] 63%|██████▎   | 50/79 [00:11<00:06,  4.52it/s] 65%|██████▍   | 51/79 [00:12<00:08,  3.39it/s] 66%|██████▌   | 52/79 [00:12<00:07,  3.66it/s] 67%|██████▋   | 53/79 [00:12<00:07,  3.64it/s] 68%|██████▊   | 54/79 [00:13<00:06,  4.00it/s] 70%|██████▉   | 55/79 [00:13<00:05,  4.09it/s] 71%|███████   | 56/79 [00:13<00:04,  4.62it/s] 72%|███████▏  | 57/79 [00:13<00:04,  4.69it/s] 73%|███████▎  | 58/79 [00:13<00:04,  4.38it/s] 75%|███████▍  | 59/79 [00:14<00:04,  4.49it/s] 76%|███████▌  | 60/79 [00:14<00:04,  4.21it/s] 77%|███████▋  | 61/79 [00:14<00:04,  3.85it/s] 78%|███████▊  | 62/79 [00:14<00:04,  3.56it/s] 80%|███████▉  | 63/79 [00:15<00:04,  3.85it/s] 81%|████████  | 64/79 [00:15<00:03,  4.23it/s] 82%|████████▏ | 65/79 [00:15<00:02,  4.68it/s] 84%|████████▎ | 66/79 [00:15<00:02,  4.72it/s] 85%|████████▍ | 67/79 [00:15<00:02,  4.71it/s] 86%|████████▌ | 68/79 [00:16<00:02,  4.81it/s] 87%|████████▋ | 69/79 [00:16<00:02,  4.41it/s] 89%|████████▊ | 70/79 [00:16<00:02,  4.42it/s] 90%|████████▉ | 71/79 [00:16<00:01,  4.13it/s] 91%|█████████ | 72/79 [00:17<00:01,  4.45it/s] 92%|█████████▏| 73/79 [00:17<00:01,  4.28it/s] 94%|█████████▎| 74/79 [00:17<00:01,  3.96it/s] 95%|█████████▍| 75/79 [00:17<00:00,  4.11it/s] 96%|█████████▌| 76/79 [00:18<00:00,  4.45it/s] 97%|█████████▋| 77/79 [00:18<00:00,  3.86it/s] 99%|█████████▊| 78/79 [00:18<00:00,  4.02it/s]100%|██████████| 79/79 [00:18<00:00,  4.37it/s]100%|██████████| 79/79 [00:18<00:00,  4.20it/s]
***** eval metrics *****
  epoch                   =     4.9858
  eval_loss               =     0.0039
  eval_runtime            = 0:00:19.00
  eval_samples_per_second =       8.26
  eval_steps_per_second   =      4.156
[INFO|modelcard.py:449] 2025-01-12 21:54:53,603 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[rank0]:[W112 21:54:53.433945125 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

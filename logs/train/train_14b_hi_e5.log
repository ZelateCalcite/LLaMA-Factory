[2025-01-06 20:10:20,860] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|2025-01-06 20:10:22] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:21690
W0106 20:10:22.742000 1884685 site-packages/torch/distributed/run.py:793] 
W0106 20:10:22.742000 1884685 site-packages/torch/distributed/run.py:793] *****************************************
W0106 20:10:22.742000 1884685 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0106 20:10:22.742000 1884685 site-packages/torch/distributed/run.py:793] *****************************************
[2025-01-06 20:10:24,267] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 20:10:24,298] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[WARNING|2025-01-06 20:10:25] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.
[WARNING|2025-01-06 20:10:25] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|2025-01-06 20:10:25] llamafactory.hparams.parser:359 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:677] 2025-01-06 20:10:25,111 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-06 20:10:25,111 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file tokenizer_config.json
[INFO|2025-01-06 20:10:25] llamafactory.hparams.parser:359 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2475] 2025-01-06 20:10:25,258 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:677] 2025-01-06 20:10:25,258 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-06 20:10:25,258 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-06 20:10:25,395 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-01-06 20:10:25] llamafactory.data.template:157 >> Add <|im_end|> to stop words.
[INFO|2025-01-06 20:10:25] llamafactory.data.loader:157 >> Loading dataset entity_trans_hi.json...
[rank1]:[W106 20:10:25.590066881 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1195 examples [00:00, 36948.91 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1195 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|██████████| 1195/1195 [00:00<00:00, 9993.83 examples/s]
[rank0]:[W106 20:10:49.932559052 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1195 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|▋         | 75/1195 [00:00<00:05, 213.35 examples/s]Running tokenizer on dataset (num_proc=16):  19%|█▉        | 225/1195 [00:00<00:01, 564.76 examples/s]Running tokenizer on dataset (num_proc=16):  31%|███▏      | 375/1195 [00:00<00:00, 823.29 examples/s]Running tokenizer on dataset (num_proc=16):  50%|█████     | 600/1195 [00:00<00:00, 999.02 examples/s]Running tokenizer on dataset (num_proc=16):  69%|██████▉   | 825/1195 [00:00<00:00, 1245.00 examples/s]Running tokenizer on dataset (num_proc=16):  88%|████████▊ | 1047/1195 [00:01<00:00, 1328.24 examples/s]Running tokenizer on dataset (num_proc=16): 100%|██████████| 1195/1195 [00:01<00:00, 1005.35 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2610, 525, 264, 9990, 43980, 12, 22574, 14468, 7341, 11, 4486, 1492, 752, 14683, 1045, 6364, 22870, 1119, 43980, 13, 7036, 429, 279, 4396, 1102, 1265, 387, 264, 17133, 429, 7952, 304, 279, 11652, 624, 40, 686, 2968, 498, 458, 6364, 17133, 323, 264, 43980, 11652, 11, 1380, 279, 6364, 17133, 374, 279, 14468, 1102, 304, 279, 43980, 11652, 13, 358, 1366, 311, 1477, 279, 7112, 43980, 17133, 315, 279, 6364, 14468, 1102, 624, 5501, 1744, 3019, 553, 3019, 13, 151645, 198, 151644, 77091, 198, 39814, 11, 358, 646, 7789, 448, 429, 13, 5209, 3410, 279, 6364, 17133, 323, 279, 43980, 11652, 432, 7952, 304, 773, 358, 646, 10542, 279, 12159, 43980, 17133, 369, 498, 382, 5501, 3561, 279, 1946, 438, 11017, 1447, 22574, 17133, 25, 508, 4208, 6364, 17133, 921, 39, 28685, 11652, 25, 508, 4208, 43980, 11652, 2533, 40, 3278, 1221, 3410, 279, 7112, 43980, 17133, 429, 33210, 311, 279, 6364, 14468, 1102, 13, 151645, 198, 151644, 872, 198, 22574, 17133, 25, 9812, 198, 39, 28685, 11652, 25, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 320, 146378, 72314, 145959, 85033, 54784, 250, 5502, 120, 43647, 25, 7513, 9145, 2376, 38807, 8, 91217, 72653, 146800, 30484, 107, 79238, 25, 14925, 107, 12619, 224, 44179, 54575, 86162, 91217, 54784, 224, 68158, 30484, 98, 42311, 97, 220, 17, 22, 14925, 99, 54784, 114, 54575, 72314, 47809, 23868, 14925, 237, 64704, 14925, 108, 31411, 250, 60096, 12619, 230, 79238, 42311, 243, 14925, 237, 145535, 72314, 14925, 228, 44179, 30484, 98, 42311, 243, 91217, 72314, 146113, 84310, 12619, 230, 14925, 250, 42311, 101, 87244, 54784, 224, 14925, 228, 86162, 78368, 91217, 54784, 224, 83636, 85033, 145966, 31411, 116, 64704, 43647, 145420, 68158, 31411, 251, 54784, 99, 31411, 108, 43647, 84310, 54575, 79238, 43647, 84310, 12619, 230, 14925, 250, 54575, 68158, 72314, 147676, 47809, 34370, 47809, 147131, 14925, 107, 23868, 68158, 146101, 43647, 14925, 108, 31411, 115, 30484, 253, 85033, 54575, 83636, 44179, 14925, 110, 31411, 245, 12619, 224, 84310, 54575, 79238, 43647, 84310, 12619, 230, 146031, 14925, 229, 78368, 64704, 23868, 14925, 227, 146101, 30484, 107, 72653, 145256, 145420, 220, 16, 14925, 250, 60096, 145535, 44179, 43647, 220, 16, 24, 24, 18, 91217, 54784, 224, 14925, 108, 54575, 87244, 47809, 43647, 68158, 72314, 146821, 38851, 14925, 99, 30484, 113, 31411, 108, 23868, 14925, 107, 12619, 224, 44179, 54575, 86162, 42311, 107, 14925, 228, 44179, 30484, 98, 42311, 243, 83636, 44179, 42311, 115, 145256, 47809, 34370, 91217, 31411, 100, 30484, 107, 87244, 68158, 34370, 14925, 249, 93948, 14925, 107, 12619, 224, 44179, 54575, 86162, 42311, 107, 14925, 99, 54784, 114, 54575, 72314, 47809, 43647, 14925, 228, 44179, 30484, 98, 42311, 243, 14925, 255, 31411, 245, 43647, 145256, 31411, 108, 43647, 68158, 34370, 84310, 72653, 146399, 14925, 98, 23868, 146031, 14925, 97, 145799, 68158, 34370, 14925, 229, 78368, 87244, 54784, 224, 68158, 145256, 78368, 30484, 107, 14925, 99, 54784, 114, 54575, 72314, 47809, 43647, 68158, 72314, 146800, 30484, 107, 23868, 91217, 54784, 224, 14925, 110, 145959, 31411, 97, 31411, 108, 14925, 105, 149269, 54575, 79238, 30484, 97, 44179, 43647, 84310, 54575, 79238, 43647, 14925, 108, 93948, 43647, 14925, 242, 44179, 14925, 229, 78368, 64704, 43647, 14925, 101, 43647, 79238, 42311, 107, 54575, 72314, 91217, 54784, 224, 14925, 105, 93948, 72653, 79238, 68158, 34370, 83636, 44179, 42311, 113, 44179, 30484, 97, 60096, 14925, 255, 43647, 14925, 114, 31411, 106, 42311, 110, 47809, 42311, 107, 34370, 14925, 245, 145420, 34370, 146031, 220, 16, 24, 24, 17, 91217, 54784, 224, 91217, 31411, 116, 30484, 97, 85033, 42311, 244, 68158, 72314, 146821, 38851, 14925, 99, 30484, 113, 31411, 108, 23868, 14925, 229, 78368, 64704, 34370, 14925, 228, 146821, 72653, 60096, 42311, 243, 14925, 113, 12619, 230, 146821, 31411, 101, 42311, 243, 68158, 30484, 113, 44179, 12619, 224, 86162, 47809, 43647, 14925, 101, 43647, 72314, 145535, 14925, 108, 146800, 43647, 14925, 245, 145420, 43647, 146031, 14925, 99, 42311, 116, 87244, 30484, 105, 44179, 220, 17, 15, 15, 24, 91217, 54784, 224, 14925, 110, 42311, 116, 30484, 105, 60096, 68158, 87244, 148971, 12619, 234, 79238, 23868, 14925, 250, 42311, 116, 64704, 34370, 14925, 99, 30484, 113, 31411, 108, 23868, 14925, 229, 78368, 87244, 54784, 224, 14925, 242, 44179, 14925, 113, 30484, 107, 31411, 103, 64704, 68158, 72653, 146821, 31411, 108, 54575, 72314, 47809, 43647, 83636, 85033, 64704, 85033, 42311, 107, 23868, 220, 16, 14925, 250, 60096, 145535, 44179, 43647, 220, 17, 15, 15, 23, 68158, 34370, 14925, 114, 72653, 44179, 72653, 47809, 43647, 14925, 245, 145420, 43647, 84310, 12619, 230, 146031, 220, 151645, 198, 151644, 77091, 198, 785, 2661, 6364, 17133, 9812, 646, 387, 24531, 1119, 43980, 438, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 11, 323, 279, 7112, 43980, 17133, 304, 279, 2661, 11652, 1265, 387, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 13, 151645, 198, 151644, 872, 198, 3973, 279, 1102, 323, 1281, 2704, 1817, 3409, 315, 279, 1102, 7952, 304, 279, 2661, 43980, 11652, 13, 151645, 198, 151644, 77091, 198, 39814, 0, 358, 614, 10067, 279, 1102, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 323, 432, 2167, 7952, 304, 279, 2661, 11652, 13, 151645, 198, 151644, 872, 198, 35127, 752, 279, 1590, 1102, 2041, 1008, 4244, 13, 151645, 198, 151644, 77091, 198, 145420, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 151645]
inputs:
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
You are a senior Hindi-English translation master, please help me translate some English sentences into Hindi. Note that the correct result should be a phrase that appears in the sentence.
I will give you an English phrase and a Hindi sentence, where the English phrase is the translation result in the Hindi sentence. I want to find the raw Hindi phrase of the English translation result.
Please think step by step.<|im_end|>
<|im_start|>assistant
Sure, I can assist with that. Please provide the English phrase and the Hindi sentence it appears in so I can identify the corresponding Hindi phrase for you.

Please format the input as follows:

English phrase: [insert English phrase]
Hindi sentence: [insert Hindi sentence]

I'll then provide the raw Hindi phrase that corresponds to the English translation result.<|im_end|>
<|im_start|>user
English phrase: EU
Hindi sentence: यूरोपीय संघ (अंग्रेज़ी: European Union)(EU) मुख्यत: यूरोप में स्थित 27 देशों का एक राजनैतिक एवं आर्थिक मंच है जिनमें आपस में प्रशासकीय साझेदारी होती है जो संघ के कई या सभी राष्ट्रो पर लागू होती है। इसका अभ्युदय 1 जनवरी 1993 में रोम की संधि द्वारा यूरोपिय आर्थिक परिषद के माध्यम से छह यूरोपिय देशों की आर्थिक भागीदारी से हुआ था। तब से इसमें सदस्य देशों की संख्या में लगातार बढोत्तरी होती रही और इसकी नीतियों में बहुत से परिवर्तन भी शामिल किये गये। 1992 में मास्त्रिख संधि द्वारा इसके आधुनिक वैधानिक स्वरूप की नींव रखी गयी। दिसम्बर 2009 में लिस्बन समझौता जिसके द्वारा इसमें और व्यापक सुधारों की प्रक्रिया 1 जनवरी 2008 से शुरु की गयी है। <|im_end|>
<|im_start|>assistant
The given English phrase EU can be translated into Hindi as यूरोपीय संघ, and the raw Hindi phrase in the given sentence should be यूरोपीय संघ.<|im_end|>
<|im_start|>user
Check the result and make sure each word of the result appears in the given Hindi sentence.<|im_end|>
<|im_start|>assistant
Sure! I have checked the result यूरोपीय संघ and it really appears in the given sentence.<|im_end|>
<|im_start|>user
Give me the final result without other words.<|im_end|>
<|im_start|>assistant
यूरोपीय संघ<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 39814, 11, 358, 646, 7789, 448, 429, 13, 5209, 3410, 279, 6364, 17133, 323, 279, 43980, 11652, 432, 7952, 304, 773, 358, 646, 10542, 279, 12159, 43980, 17133, 369, 498, 382, 5501, 3561, 279, 1946, 438, 11017, 1447, 22574, 17133, 25, 508, 4208, 6364, 17133, 921, 39, 28685, 11652, 25, 508, 4208, 43980, 11652, 2533, 40, 3278, 1221, 3410, 279, 7112, 43980, 17133, 429, 33210, 311, 279, 6364, 14468, 1102, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 2661, 6364, 17133, 9812, 646, 387, 24531, 1119, 43980, 438, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 11, 323, 279, 7112, 43980, 17133, 304, 279, 2661, 11652, 1265, 387, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 39814, 0, 358, 614, 10067, 279, 1102, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 323, 432, 2167, 7952, 304, 279, 2661, 11652, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 145420, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 151645]
labels:
Sure, I can assist with that. Please provide the English phrase and the Hindi sentence it appears in so I can identify the corresponding Hindi phrase for you.

Please format the input as follows:

English phrase: [insert English phrase]
Hindi sentence: [insert Hindi sentence]

I'll then provide the raw Hindi phrase that corresponds to the English translation result.<|im_end|>The given English phrase EU can be translated into Hindi as यूरोपीय संघ, and the raw Hindi phrase in the given sentence should be यूरोपीय संघ.<|im_end|>Sure! I have checked the result यूरोपीय संघ and it really appears in the given sentence.<|im_end|>यूरोपीय संघ<|im_end|>
[INFO|configuration_utils.py:677] 2025-01-06 20:11:13,961 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-06 20:11:13,961 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|2025-01-06 20:11:13] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.
[INFO|modeling_utils.py:3934] 2025-01-06 20:11:14,007 >> loading weights file /mnt/sda/zzh/Qwen2.5-14B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1670] 2025-01-06 20:11:14,007 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1096] 2025-01-06 20:11:14,008 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:00<00:04,  1.62it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:01<00:03,  1.62it/s]Loading checkpoint shards:  12%|█▎        | 1/8 [00:01<00:10,  1.43s/it]Loading checkpoint shards:  38%|███▊      | 3/8 [00:02<00:04,  1.13it/s]Loading checkpoint shards:  25%|██▌       | 2/8 [00:02<00:08,  1.44s/it]Loading checkpoint shards:  50%|█████     | 4/8 [00:03<00:03,  1.28it/s]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:03<00:02,  1.39it/s]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:04<00:01,  1.45it/s]Loading checkpoint shards:  38%|███▊      | 3/8 [00:04<00:07,  1.45s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:04<00:00,  1.50it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.81it/s]Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.53it/s]
[INFO|modeling_utils.py:4800] 2025-01-06 20:11:19,352 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4808] 2025-01-06 20:11:19,352 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /mnt/sda/zzh/Qwen2.5-14B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1049] 2025-01-06 20:11:19,353 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/generation_config.json
[INFO|configuration_utils.py:1096] 2025-01-06 20:11:19,354 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|2025-01-06 20:11:19] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
[INFO|2025-01-06 20:11:19] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.
[INFO|2025-01-06 20:11:19] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.
[INFO|2025-01-06 20:11:19] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA
[INFO|2025-01-06 20:11:19] llamafactory.model.model_utils.misc:157 >> Found linear modules: gate_proj,k_proj,q_proj,down_proj,v_proj,o_proj,up_proj
[INFO|2025-01-06 20:11:19] llamafactory.model.loader:157 >> trainable params: 34,406,400 || all params: 14,804,440,064 || trainable%: 0.2324
[INFO|trainer.py:698] 2025-01-06 20:11:19,683 >> Using auto half precision backend
Loading checkpoint shards:  50%|█████     | 4/8 [00:05<00:05,  1.45s/it]Loading checkpoint shards:  62%|██████▎   | 5/8 [00:07<00:04,  1.44s/it]Loading checkpoint shards:  75%|███████▌  | 6/8 [00:08<00:02,  1.44s/it]Loading checkpoint shards:  88%|████████▊ | 7/8 [00:10<00:01,  1.43s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:10<00:00,  1.18s/it]Loading checkpoint shards: 100%|██████████| 8/8 [00:10<00:00,  1.34s/it]
[INFO|trainer.py:2313] 2025-01-06 20:11:26,023 >> ***** Running training *****
[INFO|trainer.py:2314] 2025-01-06 20:11:26,023 >>   Num examples = 1,075
[INFO|trainer.py:2315] 2025-01-06 20:11:26,023 >>   Num Epochs = 5
[INFO|trainer.py:2316] 2025-01-06 20:11:26,023 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2319] 2025-01-06 20:11:26,023 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2320] 2025-01-06 20:11:26,023 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2321] 2025-01-06 20:11:26,023 >>   Total optimization steps = 335
[INFO|trainer.py:2322] 2025-01-06 20:11:26,025 >>   Number of trainable parameters = 34,406,400
  0%|          | 0/335 [00:00<?, ?it/s]  0%|          | 1/335 [00:06<34:38,  6.22s/it]  1%|          | 2/335 [00:11<30:08,  5.43s/it]  1%|          | 3/335 [00:17<32:03,  5.79s/it]  1%|          | 4/335 [00:23<32:16,  5.85s/it]  1%|▏         | 5/335 [00:28<31:46,  5.78s/it]  2%|▏         | 6/335 [00:34<31:37,  5.77s/it]  2%|▏         | 7/335 [00:40<31:28,  5.76s/it]  2%|▏         | 8/335 [00:46<31:45,  5.83s/it]  3%|▎         | 9/335 [00:52<32:34,  5.99s/it]  3%|▎         | 10/335 [00:58<32:12,  5.95s/it]  3%|▎         | 11/335 [01:04<31:25,  5.82s/it]  4%|▎         | 12/335 [01:10<32:35,  6.06s/it]  4%|▍         | 13/335 [01:16<32:03,  5.97s/it]  4%|▍         | 14/335 [01:22<31:32,  5.90s/it]  4%|▍         | 15/335 [01:27<30:49,  5.78s/it]  5%|▍         | 16/335 [01:33<30:38,  5.76s/it]  5%|▌         | 17/335 [01:39<30:36,  5.78s/it]  5%|▌         | 18/335 [01:45<31:30,  5.96s/it]  6%|▌         | 19/335 [01:51<31:27,  5.97s/it]  6%|▌         | 20/335 [01:57<31:07,  5.93s/it]  6%|▋         | 21/335 [02:04<32:21,  6.18s/it]  7%|▋         | 22/335 [02:09<31:08,  5.97s/it]  7%|▋         | 23/335 [02:15<31:06,  5.98s/it]  7%|▋         | 24/335 [02:22<32:02,  6.18s/it]  7%|▋         | 25/335 [02:27<30:16,  5.86s/it]  8%|▊         | 26/335 [02:33<30:27,  5.91s/it]  8%|▊         | 27/335 [02:38<29:09,  5.68s/it]  8%|▊         | 28/335 [02:44<30:01,  5.87s/it]  9%|▊         | 29/335 [02:50<30:03,  5.89s/it]  9%|▉         | 30/335 [02:57<31:30,  6.20s/it]  9%|▉         | 31/335 [03:04<31:55,  6.30s/it] 10%|▉         | 32/335 [03:10<30:53,  6.12s/it] 10%|▉         | 33/335 [03:16<31:09,  6.19s/it] 10%|█         | 34/335 [03:21<29:29,  5.88s/it] 10%|█         | 35/335 [03:28<30:14,  6.05s/it] 11%|█         | 36/335 [03:34<30:42,  6.16s/it] 11%|█         | 37/335 [03:40<30:05,  6.06s/it] 11%|█▏        | 38/335 [03:46<30:32,  6.17s/it] 12%|█▏        | 39/335 [03:52<29:37,  6.01s/it] 12%|█▏        | 40/335 [03:58<29:26,  5.99s/it] 12%|█▏        | 41/335 [04:03<28:43,  5.86s/it] 13%|█▎        | 42/335 [04:09<28:03,  5.75s/it] 13%|█▎        | 43/335 [04:14<27:46,  5.71s/it] 13%|█▎        | 44/335 [04:20<27:42,  5.71s/it] 13%|█▎        | 45/335 [04:26<28:25,  5.88s/it] 14%|█▎        | 46/335 [04:32<28:16,  5.87s/it] 14%|█▍        | 47/335 [04:37<27:10,  5.66s/it] 14%|█▍        | 48/335 [04:43<27:04,  5.66s/it] 15%|█▍        | 49/335 [04:48<26:04,  5.47s/it] 15%|█▍        | 50/335 [04:54<26:07,  5.50s/it] 15%|█▌        | 51/335 [04:59<25:50,  5.46s/it] 16%|█▌        | 52/335 [05:05<26:36,  5.64s/it] 16%|█▌        | 53/335 [05:11<27:24,  5.83s/it] 16%|█▌        | 54/335 [05:18<28:21,  6.05s/it] 16%|█▋        | 55/335 [05:24<27:58,  6.00s/it] 17%|█▋        | 56/335 [05:30<27:36,  5.94s/it] 17%|█▋        | 57/335 [05:36<27:30,  5.94s/it] 17%|█▋        | 58/335 [05:41<27:23,  5.93s/it] 18%|█▊        | 59/335 [05:48<27:48,  6.04s/it] 18%|█▊        | 60/335 [05:54<27:27,  5.99s/it] 18%|█▊        | 61/335 [06:00<27:30,  6.02s/it] 19%|█▊        | 62/335 [06:05<26:10,  5.75s/it] 19%|█▉        | 63/335 [06:11<26:15,  5.79s/it] 19%|█▉        | 64/335 [06:18<27:27,  6.08s/it] 19%|█▉        | 65/335 [06:23<26:48,  5.96s/it] 20%|█▉        | 66/335 [06:29<26:59,  6.02s/it] 20%|██        | 67/335 [06:36<27:14,  6.10s/it][INFO|trainer.py:4117] 2025-01-06 20:18:05,627 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:18:05,627 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:18:05,627 >>   Batch size = 1

  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|▎         | 2/60 [00:00<00:05,  9.87it/s][A
  5%|▌         | 3/60 [00:00<00:10,  5.67it/s][A
  7%|▋         | 4/60 [00:00<00:11,  4.94it/s][A
  8%|▊         | 5/60 [00:00<00:11,  4.59it/s][A
 10%|█         | 6/60 [00:01<00:12,  4.16it/s][A
 12%|█▏        | 7/60 [00:01<00:12,  4.40it/s][A
 13%|█▎        | 8/60 [00:01<00:12,  4.11it/s][A
 15%|█▌        | 9/60 [00:02<00:13,  3.70it/s][A
 17%|█▋        | 10/60 [00:02<00:12,  4.08it/s][A
 18%|█▊        | 11/60 [00:02<00:10,  4.47it/s][A
 20%|██        | 12/60 [00:02<00:11,  4.29it/s][A
 22%|██▏       | 13/60 [00:02<00:10,  4.70it/s][A
 23%|██▎       | 14/60 [00:03<00:11,  4.09it/s][A
 25%|██▌       | 15/60 [00:03<00:11,  3.83it/s][A
 27%|██▋       | 16/60 [00:03<00:10,  4.36it/s][A
 28%|██▊       | 17/60 [00:03<00:09,  4.43it/s][A
 30%|███       | 18/60 [00:04<00:09,  4.42it/s][A
 32%|███▏      | 19/60 [00:04<00:09,  4.40it/s][A
 33%|███▎      | 20/60 [00:04<00:08,  4.88it/s][A
 35%|███▌      | 21/60 [00:04<00:07,  4.93it/s][A
 37%|███▋      | 22/60 [00:04<00:08,  4.56it/s][A
 38%|███▊      | 23/60 [00:05<00:08,  4.31it/s][A
 40%|████      | 24/60 [00:05<00:07,  4.56it/s][A
 42%|████▏     | 25/60 [00:05<00:08,  4.22it/s][A
 43%|████▎     | 26/60 [00:05<00:08,  4.10it/s][A
 45%|████▌     | 27/60 [00:06<00:08,  4.05it/s][A
 47%|████▋     | 28/60 [00:06<00:08,  3.68it/s][A
 48%|████▊     | 29/60 [00:06<00:07,  3.88it/s][A
 50%|█████     | 30/60 [00:06<00:06,  4.38it/s][A
 52%|█████▏    | 31/60 [00:07<00:07,  3.74it/s][A
 53%|█████▎    | 32/60 [00:07<00:07,  3.86it/s][A
 55%|█████▌    | 33/60 [00:07<00:06,  4.06it/s][A
 57%|█████▋    | 34/60 [00:07<00:06,  4.18it/s][A
 58%|█████▊    | 35/60 [00:08<00:06,  4.13it/s][A
 60%|██████    | 36/60 [00:08<00:06,  3.67it/s][A
 62%|██████▏   | 37/60 [00:08<00:05,  3.87it/s][A
 63%|██████▎   | 38/60 [00:08<00:05,  3.91it/s][A
 65%|██████▌   | 39/60 [00:09<00:05,  3.95it/s][A
 67%|██████▋   | 40/60 [00:09<00:04,  4.25it/s][A
 68%|██████▊   | 41/60 [00:09<00:04,  4.22it/s][A
 70%|███████   | 42/60 [00:09<00:04,  4.16it/s][A
 72%|███████▏  | 43/60 [00:10<00:04,  4.19it/s][A
 73%|███████▎  | 44/60 [00:10<00:03,  4.34it/s][A
 75%|███████▌  | 45/60 [00:10<00:03,  4.31it/s][A
 77%|███████▋  | 46/60 [00:10<00:03,  4.19it/s][A
 78%|███████▊  | 47/60 [00:11<00:03,  3.90it/s][A
 80%|████████  | 48/60 [00:11<00:02,  4.04it/s][A
 82%|████████▏ | 49/60 [00:11<00:02,  4.24it/s][A
 83%|████████▎ | 50/60 [00:11<00:02,  4.54it/s][A
 85%|████████▌ | 51/60 [00:12<00:02,  4.22it/s][A
 87%|████████▋ | 52/60 [00:12<00:02,  3.84it/s][A
 88%|████████▊ | 53/60 [00:12<00:01,  3.73it/s][A
 90%|█████████ | 54/60 [00:12<00:01,  3.84it/s][A
 92%|█████████▏| 55/60 [00:13<00:01,  3.90it/s][A
 93%|█████████▎| 56/60 [00:13<00:01,  3.49it/s][A
 95%|█████████▌| 57/60 [00:13<00:00,  3.89it/s][A
 97%|█████████▋| 58/60 [00:13<00:00,  4.18it/s][A
 98%|█████████▊| 59/60 [00:14<00:00,  4.41it/s][A
100%|██████████| 60/60 [00:14<00:00,  4.42it/s][A                                                
                                               [A{'eval_loss': 0.0028568485286086798, 'eval_runtime': 14.9863, 'eval_samples_per_second': 8.007, 'eval_steps_per_second': 4.004, 'epoch': 1.0}
 20%|██        | 67/335 [06:52<27:14,  6.10s/it]
100%|██████████| 60/60 [00:14<00:00,  4.42it/s][A
                                               [A 20%|██        | 68/335 [06:56<46:17, 10.40s/it] 21%|██        | 69/335 [07:02<40:30,  9.14s/it] 21%|██        | 70/335 [07:08<35:51,  8.12s/it] 21%|██        | 71/335 [07:14<33:02,  7.51s/it] 21%|██▏       | 72/335 [07:22<32:57,  7.52s/it] 22%|██▏       | 73/335 [07:27<30:30,  6.99s/it] 22%|██▏       | 74/335 [07:33<29:11,  6.71s/it] 22%|██▏       | 75/335 [07:39<28:10,  6.50s/it] 23%|██▎       | 76/335 [07:45<26:41,  6.19s/it] 23%|██▎       | 77/335 [07:52<27:07,  6.31s/it] 23%|██▎       | 78/335 [07:58<26:57,  6.29s/it] 24%|██▎       | 79/335 [08:03<24:55,  5.84s/it] 24%|██▍       | 80/335 [08:07<23:41,  5.57s/it] 24%|██▍       | 81/335 [08:13<24:03,  5.68s/it] 24%|██▍       | 82/335 [08:19<23:48,  5.65s/it] 25%|██▍       | 83/335 [08:26<25:01,  5.96s/it] 25%|██▌       | 84/335 [08:31<24:02,  5.75s/it] 25%|██▌       | 85/335 [08:37<23:45,  5.70s/it] 26%|██▌       | 86/335 [08:43<24:42,  5.95s/it] 26%|██▌       | 87/335 [08:48<23:42,  5.73s/it] 26%|██▋       | 88/335 [08:55<24:46,  6.02s/it] 27%|██▋       | 89/335 [09:00<23:46,  5.80s/it] 27%|██▋       | 90/335 [09:06<23:26,  5.74s/it] 27%|██▋       | 91/335 [09:12<23:16,  5.72s/it] 27%|██▋       | 92/335 [09:18<24:16,  5.99s/it] 28%|██▊       | 93/335 [09:24<23:23,  5.80s/it] 28%|██▊       | 94/335 [09:29<23:01,  5.73s/it] 28%|██▊       | 95/335 [09:35<23:24,  5.85s/it] 29%|██▊       | 96/335 [09:41<23:29,  5.90s/it] 29%|██▉       | 97/335 [09:47<23:24,  5.90s/it] 29%|██▉       | 98/335 [09:52<22:07,  5.60s/it] 30%|██▉       | 99/335 [09:58<22:52,  5.81s/it] 30%|██▉       | 100/335 [10:04<22:18,  5.69s/it] 30%|███       | 101/335 [10:10<22:30,  5.77s/it] 30%|███       | 102/335 [10:17<24:12,  6.23s/it] 31%|███       | 103/335 [10:24<24:30,  6.34s/it] 31%|███       | 104/335 [10:30<24:20,  6.32s/it] 31%|███▏      | 105/335 [10:36<23:35,  6.15s/it] 32%|███▏      | 106/335 [10:42<23:57,  6.28s/it] 32%|███▏      | 107/335 [10:49<24:25,  6.43s/it] 32%|███▏      | 108/335 [10:55<24:07,  6.38s/it] 33%|███▎      | 109/335 [11:01<23:47,  6.32s/it] 33%|███▎      | 110/335 [11:07<23:09,  6.18s/it] 33%|███▎      | 111/335 [11:13<22:41,  6.08s/it] 33%|███▎      | 112/335 [11:19<21:53,  5.89s/it] 34%|███▎      | 113/335 [11:25<21:52,  5.91s/it] 34%|███▍      | 114/335 [11:30<21:34,  5.86s/it] 34%|███▍      | 115/335 [11:36<21:10,  5.77s/it] 35%|███▍      | 116/335 [11:41<20:42,  5.67s/it] 35%|███▍      | 117/335 [11:47<20:45,  5.71s/it] 35%|███▌      | 118/335 [11:54<21:44,  6.01s/it] 36%|███▌      | 119/335 [11:59<21:05,  5.86s/it] 36%|███▌      | 120/335 [12:04<20:12,  5.64s/it] 36%|███▌      | 121/335 [12:10<20:10,  5.66s/it] 36%|███▋      | 122/335 [12:16<20:31,  5.78s/it] 37%|███▋      | 123/335 [12:22<20:08,  5.70s/it] 37%|███▋      | 124/335 [12:27<19:33,  5.56s/it] 37%|███▋      | 125/335 [12:33<19:41,  5.62s/it] 38%|███▊      | 126/335 [12:39<19:51,  5.70s/it] 38%|███▊      | 127/335 [12:44<19:08,  5.52s/it] 38%|███▊      | 128/335 [12:50<20:11,  5.85s/it] 39%|███▊      | 129/335 [12:56<20:12,  5.89s/it] 39%|███▉      | 130/335 [13:02<19:47,  5.79s/it] 39%|███▉      | 131/335 [13:08<19:35,  5.76s/it] 39%|███▉      | 132/335 [13:14<20:00,  5.91s/it] 40%|███▉      | 133/335 [13:20<19:41,  5.85s/it] 40%|████      | 134/335 [13:26<19:55,  5.95s/it][INFO|trainer.py:4117] 2025-01-06 20:24:57,076 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:24:57,076 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:24:57,076 >>   Batch size = 1

  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|▎         | 2/60 [00:00<00:05,  9.87it/s][A
  5%|▌         | 3/60 [00:00<00:10,  5.67it/s][A
  7%|▋         | 4/60 [00:00<00:11,  4.94it/s][A
  8%|▊         | 5/60 [00:00<00:11,  4.59it/s][A
 10%|█         | 6/60 [00:01<00:12,  4.17it/s][A
 12%|█▏        | 7/60 [00:01<00:12,  4.40it/s][A
 13%|█▎        | 8/60 [00:01<00:12,  4.11it/s][A
 15%|█▌        | 9/60 [00:02<00:13,  3.70it/s][A
 17%|█▋        | 10/60 [00:02<00:12,  4.08it/s][A
 18%|█▊        | 11/60 [00:02<00:10,  4.47it/s][A
 20%|██        | 12/60 [00:02<00:11,  4.29it/s][A
 22%|██▏       | 13/60 [00:02<00:10,  4.70it/s][A
 23%|██▎       | 14/60 [00:03<00:11,  4.09it/s][A
 25%|██▌       | 15/60 [00:03<00:11,  3.83it/s][A
 27%|██▋       | 16/60 [00:03<00:10,  4.36it/s][A
 28%|██▊       | 17/60 [00:03<00:09,  4.43it/s][A
 30%|███       | 18/60 [00:04<00:09,  4.42it/s][A
 32%|███▏      | 19/60 [00:04<00:09,  4.41it/s][A
 33%|███▎      | 20/60 [00:04<00:08,  4.88it/s][A
 35%|███▌      | 21/60 [00:04<00:07,  4.93it/s][A
 37%|███▋      | 22/60 [00:04<00:08,  4.56it/s][A
 38%|███▊      | 23/60 [00:05<00:08,  4.31it/s][A
 40%|████      | 24/60 [00:05<00:07,  4.56it/s][A
 42%|████▏     | 25/60 [00:05<00:08,  4.22it/s][A
 43%|████▎     | 26/60 [00:05<00:08,  4.10it/s][A
 45%|████▌     | 27/60 [00:06<00:08,  4.05it/s][A
 47%|████▋     | 28/60 [00:06<00:08,  3.68it/s][A
 48%|████▊     | 29/60 [00:06<00:07,  3.88it/s][A
 50%|█████     | 30/60 [00:06<00:06,  4.38it/s][A
 52%|█████▏    | 31/60 [00:07<00:07,  3.74it/s][A
 53%|█████▎    | 32/60 [00:07<00:07,  3.87it/s][A
 55%|█████▌    | 33/60 [00:07<00:06,  4.06it/s][A
 57%|█████▋    | 34/60 [00:07<00:06,  4.18it/s][A
 58%|█████▊    | 35/60 [00:08<00:06,  4.13it/s][A
 60%|██████    | 36/60 [00:08<00:06,  3.67it/s][A
 62%|██████▏   | 37/60 [00:08<00:05,  3.87it/s][A
 63%|██████▎   | 38/60 [00:08<00:05,  3.91it/s][A
 65%|██████▌   | 39/60 [00:09<00:05,  3.95it/s][A
 67%|██████▋   | 40/60 [00:09<00:04,  4.25it/s][A
 68%|██████▊   | 41/60 [00:09<00:04,  4.22it/s][A
 70%|███████   | 42/60 [00:09<00:04,  4.16it/s][A
 72%|███████▏  | 43/60 [00:10<00:04,  4.19it/s][A
 73%|███████▎  | 44/60 [00:10<00:03,  4.34it/s][A
 75%|███████▌  | 45/60 [00:10<00:03,  4.32it/s][A
 77%|███████▋  | 46/60 [00:10<00:03,  4.19it/s][A
 78%|███████▊  | 47/60 [00:11<00:03,  3.90it/s][A
 80%|████████  | 48/60 [00:11<00:02,  4.05it/s][A
 82%|████████▏ | 49/60 [00:11<00:02,  4.24it/s][A
 83%|████████▎ | 50/60 [00:11<00:02,  4.54it/s][A
 85%|████████▌ | 51/60 [00:12<00:02,  4.21it/s][A
 87%|████████▋ | 52/60 [00:12<00:02,  3.84it/s][A
 88%|████████▊ | 53/60 [00:12<00:01,  3.73it/s][A
 90%|█████████ | 54/60 [00:12<00:01,  3.84it/s][A
 92%|█████████▏| 55/60 [00:13<00:01,  3.90it/s][A
 93%|█████████▎| 56/60 [00:13<00:01,  3.49it/s][A
 95%|█████████▌| 57/60 [00:13<00:00,  3.89it/s][A
 97%|█████████▋| 58/60 [00:13<00:00,  4.18it/s][A
 98%|█████████▊| 59/60 [00:14<00:00,  4.41it/s][A
100%|██████████| 60/60 [00:14<00:00,  4.43it/s][A                                                 
                                               [A{'eval_loss': 0.0022934062872081995, 'eval_runtime': 15.143, 'eval_samples_per_second': 7.924, 'eval_steps_per_second': 3.962, 'epoch': 1.99}
 40%|████      | 134/335 [13:43<19:55,  5.95s/it]
100%|██████████| 60/60 [00:14<00:00,  4.43it/s][A
                                               [A 40%|████      | 135/335 [13:46<34:40, 10.40s/it] 41%|████      | 136/335 [13:52<29:30,  8.90s/it] 41%|████      | 137/335 [13:58<26:15,  7.96s/it] 41%|████      | 138/335 [14:04<24:23,  7.43s/it] 41%|████▏     | 139/335 [14:10<22:50,  6.99s/it] 42%|████▏     | 140/335 [14:15<21:25,  6.59s/it] 42%|████▏     | 141/335 [14:21<19:55,  6.16s/it] 42%|████▏     | 142/335 [14:27<19:32,  6.07s/it] 43%|████▎     | 143/335 [14:33<19:52,  6.21s/it] 43%|████▎     | 144/335 [14:39<19:15,  6.05s/it] 43%|████▎     | 145/335 [14:45<19:24,  6.13s/it] 44%|████▎     | 146/335 [14:50<18:29,  5.87s/it] 44%|████▍     | 147/335 [14:57<18:52,  6.02s/it] 44%|████▍     | 148/335 [15:02<18:05,  5.80s/it] 44%|████▍     | 149/335 [15:08<17:56,  5.79s/it] 45%|████▍     | 150/335 [15:14<17:56,  5.82s/it] 45%|████▌     | 151/335 [15:20<18:15,  5.95s/it] 45%|████▌     | 152/335 [15:26<17:54,  5.87s/it] 46%|████▌     | 153/335 [15:31<17:50,  5.88s/it] 46%|████▌     | 154/335 [15:37<17:31,  5.81s/it] 46%|████▋     | 155/335 [15:43<17:21,  5.79s/it] 47%|████▋     | 156/335 [15:48<16:48,  5.63s/it] 47%|████▋     | 157/335 [15:54<17:02,  5.75s/it] 47%|████▋     | 158/335 [16:01<17:39,  5.98s/it] 47%|████▋     | 159/335 [16:06<17:24,  5.93s/it] 48%|████▊     | 160/335 [16:12<17:01,  5.84s/it] 48%|████▊     | 161/335 [16:18<16:48,  5.80s/it] 48%|████▊     | 162/335 [16:24<17:03,  5.92s/it] 49%|████▊     | 163/335 [16:30<16:53,  5.89s/it] 49%|████▉     | 164/335 [16:36<17:23,  6.10s/it] 49%|████▉     | 165/335 [16:42<17:12,  6.07s/it] 50%|████▉     | 166/335 [16:48<17:06,  6.08s/it] 50%|████▉     | 167/335 [16:54<16:53,  6.03s/it] 50%|█████     | 168/335 [17:00<16:48,  6.04s/it] 50%|█████     | 169/335 [17:06<16:24,  5.93s/it] 51%|█████     | 170/335 [17:13<16:53,  6.14s/it] 51%|█████     | 171/335 [17:19<16:39,  6.09s/it] 51%|█████▏    | 172/335 [17:24<16:08,  5.94s/it] 52%|█████▏    | 173/335 [17:30<15:31,  5.75s/it] 52%|█████▏    | 174/335 [17:36<15:40,  5.84s/it] 52%|█████▏    | 175/335 [17:41<15:26,  5.79s/it] 53%|█████▎    | 176/335 [17:46<14:21,  5.42s/it] 53%|█████▎    | 177/335 [17:51<13:45,  5.22s/it] 53%|█████▎    | 178/335 [17:56<13:44,  5.25s/it] 53%|█████▎    | 179/335 [18:03<15:07,  5.82s/it] 54%|█████▎    | 180/335 [18:10<15:34,  6.03s/it] 54%|█████▍    | 181/335 [18:15<15:14,  5.94s/it] 54%|█████▍    | 182/335 [18:21<15:13,  5.97s/it] 55%|█████▍    | 183/335 [18:27<14:50,  5.86s/it] 55%|█████▍    | 184/335 [18:34<15:22,  6.11s/it] 55%|█████▌    | 185/335 [18:39<14:53,  5.96s/it] 56%|█████▌    | 186/335 [18:45<14:31,  5.85s/it] 56%|█████▌    | 187/335 [18:51<14:32,  5.89s/it] 56%|█████▌    | 188/335 [18:56<14:07,  5.76s/it] 56%|█████▋    | 189/335 [19:02<14:10,  5.82s/it] 57%|█████▋    | 190/335 [19:08<13:57,  5.77s/it] 57%|█████▋    | 191/335 [19:14<14:06,  5.88s/it] 57%|█████▋    | 192/335 [19:20<14:17,  6.00s/it] 58%|█████▊    | 193/335 [19:27<14:45,  6.24s/it] 58%|█████▊    | 194/335 [19:33<14:33,  6.19s/it] 58%|█████▊    | 195/335 [19:40<14:49,  6.35s/it] 59%|█████▊    | 196/335 [19:46<14:14,  6.15s/it] 59%|█████▉    | 197/335 [19:51<13:31,  5.88s/it] 59%|█████▉    | 198/335 [19:56<13:02,  5.71s/it] 59%|█████▉    | 199/335 [20:02<13:03,  5.76s/it] 60%|█████▉    | 200/335 [20:09<13:30,  6.00s/it]                                                 {'loss': 0.178, 'grad_norm': 0.05876597389578819, 'learning_rate': 4.194641844029227e-05, 'epoch': 2.97}
 60%|█████▉    | 200/335 [20:09<13:30,  6.00s/it] 60%|██████    | 201/335 [20:14<13:14,  5.93s/it][INFO|trainer.py:4117] 2025-01-06 20:31:47,894 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:31:47,894 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:31:47,894 >>   Batch size = 1

  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|▎         | 2/60 [00:00<00:05,  9.88it/s][A
  5%|▌         | 3/60 [00:00<00:10,  5.68it/s][A
  7%|▋         | 4/60 [00:00<00:11,  4.95it/s][A
  8%|▊         | 5/60 [00:00<00:11,  4.59it/s][A
 10%|█         | 6/60 [00:01<00:12,  4.17it/s][A
 12%|█▏        | 7/60 [00:01<00:12,  4.40it/s][A
 13%|█▎        | 8/60 [00:01<00:12,  4.12it/s][A
 15%|█▌        | 9/60 [00:02<00:13,  3.70it/s][A
 17%|█▋        | 10/60 [00:02<00:12,  4.09it/s][A
 18%|█▊        | 11/60 [00:02<00:10,  4.47it/s][A
 20%|██        | 12/60 [00:02<00:11,  4.29it/s][A
 22%|██▏       | 13/60 [00:02<00:09,  4.70it/s][A
 23%|██▎       | 14/60 [00:03<00:11,  4.09it/s][A
 25%|██▌       | 15/60 [00:03<00:11,  3.83it/s][A
 27%|██▋       | 16/60 [00:03<00:10,  4.37it/s][A
 28%|██▊       | 17/60 [00:03<00:09,  4.44it/s][A
 30%|███       | 18/60 [00:04<00:09,  4.42it/s][A
 32%|███▏      | 19/60 [00:04<00:09,  4.41it/s][A
 33%|███▎      | 20/60 [00:04<00:08,  4.89it/s][A
 35%|███▌      | 21/60 [00:04<00:07,  4.94it/s][A
 37%|███▋      | 22/60 [00:04<00:08,  4.56it/s][A
 38%|███▊      | 23/60 [00:05<00:08,  4.31it/s][A
 40%|████      | 24/60 [00:05<00:07,  4.55it/s][A
 42%|████▏     | 25/60 [00:05<00:08,  4.22it/s][A
 43%|████▎     | 26/60 [00:05<00:08,  4.10it/s][A
 45%|████▌     | 27/60 [00:06<00:08,  4.05it/s][A
 47%|████▋     | 28/60 [00:06<00:08,  3.68it/s][A
 48%|████▊     | 29/60 [00:06<00:08,  3.87it/s][A
 50%|█████     | 30/60 [00:06<00:06,  4.38it/s][A
 52%|█████▏    | 31/60 [00:07<00:07,  3.74it/s][A
 53%|█████▎    | 32/60 [00:07<00:07,  3.86it/s][A
 55%|█████▌    | 33/60 [00:07<00:06,  4.06it/s][A
 57%|█████▋    | 34/60 [00:07<00:06,  4.18it/s][A
 58%|█████▊    | 35/60 [00:08<00:06,  4.13it/s][A
 60%|██████    | 36/60 [00:08<00:06,  3.67it/s][A
 62%|██████▏   | 37/60 [00:08<00:05,  3.87it/s][A
 63%|██████▎   | 38/60 [00:08<00:05,  3.92it/s][A
 65%|██████▌   | 39/60 [00:09<00:05,  3.95it/s][A
 67%|██████▋   | 40/60 [00:09<00:04,  4.25it/s][A
 68%|██████▊   | 41/60 [00:09<00:04,  4.22it/s][A
 70%|███████   | 42/60 [00:09<00:04,  4.16it/s][A
 72%|███████▏  | 43/60 [00:10<00:04,  4.19it/s][A
 73%|███████▎  | 44/60 [00:10<00:03,  4.34it/s][A
 75%|███████▌  | 45/60 [00:10<00:03,  4.31it/s][A
 77%|███████▋  | 46/60 [00:10<00:03,  4.19it/s][A
 78%|███████▊  | 47/60 [00:11<00:03,  3.90it/s][A
 80%|████████  | 48/60 [00:11<00:02,  4.04it/s][A
 82%|████████▏ | 49/60 [00:11<00:02,  4.24it/s][A
 83%|████████▎ | 50/60 [00:11<00:02,  4.54it/s][A
 85%|████████▌ | 51/60 [00:12<00:02,  4.21it/s][A
 87%|████████▋ | 52/60 [00:12<00:02,  3.84it/s][A
 88%|████████▊ | 53/60 [00:12<00:01,  3.73it/s][A
 90%|█████████ | 54/60 [00:12<00:01,  3.84it/s][A
 92%|█████████▏| 55/60 [00:13<00:01,  3.90it/s][A
 93%|█████████▎| 56/60 [00:13<00:01,  3.49it/s][A
 95%|█████████▌| 57/60 [00:13<00:00,  3.89it/s][A
 97%|█████████▋| 58/60 [00:13<00:00,  4.18it/s][A
 98%|█████████▊| 59/60 [00:14<00:00,  4.42it/s][A
100%|██████████| 60/60 [00:14<00:00,  4.43it/s][A                                                 
                                               [A{'eval_loss': 0.002161339158192277, 'eval_runtime': 15.0163, 'eval_samples_per_second': 7.991, 'eval_steps_per_second': 3.996, 'epoch': 2.99}
 60%|██████    | 201/335 [20:34<13:14,  5.93s/it]
100%|██████████| 60/60 [00:14<00:00,  4.43it/s][A
                                               [A 60%|██████    | 202/335 [20:36<23:19, 10.52s/it] 61%|██████    | 203/335 [20:43<20:57,  9.53s/it] 61%|██████    | 204/335 [20:48<18:08,  8.31s/it] 61%|██████    | 205/335 [20:55<16:57,  7.83s/it] 61%|██████▏   | 206/335 [21:01<15:42,  7.31s/it] 62%|██████▏   | 207/335 [21:07<14:23,  6.75s/it] 62%|██████▏   | 208/335 [21:13<14:02,  6.63s/it] 62%|██████▏   | 209/335 [21:18<13:08,  6.26s/it] 63%|██████▎   | 210/335 [21:24<12:43,  6.11s/it] 63%|██████▎   | 211/335 [21:30<12:30,  6.05s/it] 63%|██████▎   | 212/335 [21:36<12:18,  6.01s/it] 64%|██████▎   | 213/335 [21:41<11:54,  5.86s/it] 64%|██████▍   | 214/335 [21:47<11:41,  5.80s/it] 64%|██████▍   | 215/335 [21:54<11:58,  5.99s/it] 64%|██████▍   | 216/335 [22:00<11:52,  5.98s/it] 65%|██████▍   | 217/335 [22:05<11:38,  5.92s/it] 65%|██████▌   | 218/335 [22:11<11:37,  5.96s/it] 65%|██████▌   | 219/335 [22:17<11:35,  5.99s/it] 66%|██████▌   | 220/335 [22:23<11:04,  5.78s/it] 66%|██████▌   | 221/335 [22:28<10:36,  5.58s/it] 66%|██████▋   | 222/335 [22:34<10:59,  5.84s/it] 67%|██████▋   | 223/335 [22:40<10:59,  5.89s/it] 67%|██████▋   | 224/335 [22:46<10:37,  5.74s/it] 67%|██████▋   | 225/335 [22:52<10:39,  5.81s/it] 67%|██████▋   | 226/335 [22:59<11:08,  6.14s/it] 68%|██████▊   | 227/335 [23:04<10:46,  5.99s/it] 68%|██████▊   | 228/335 [23:11<10:57,  6.14s/it] 68%|██████▊   | 229/335 [23:17<10:42,  6.06s/it] 69%|██████▊   | 230/335 [23:22<10:02,  5.74s/it] 69%|██████▉   | 231/335 [23:27<09:55,  5.73s/it] 69%|██████▉   | 232/335 [23:33<09:46,  5.69s/it] 70%|██████▉   | 233/335 [23:39<09:40,  5.69s/it] 70%|██████▉   | 234/335 [23:44<09:31,  5.66s/it] 70%|███████   | 235/335 [23:50<09:36,  5.77s/it] 70%|███████   | 236/335 [23:56<09:44,  5.91s/it] 71%|███████   | 237/335 [24:02<09:25,  5.77s/it] 71%|███████   | 238/335 [24:08<09:21,  5.79s/it] 71%|███████▏  | 239/335 [24:14<09:28,  5.93s/it] 72%|███████▏  | 240/335 [24:20<09:38,  6.09s/it] 72%|███████▏  | 241/335 [24:26<09:14,  5.90s/it] 72%|███████▏  | 242/335 [24:31<08:47,  5.67s/it] 73%|███████▎  | 243/335 [24:37<08:40,  5.66s/it] 73%|███████▎  | 244/335 [24:42<08:17,  5.47s/it] 73%|███████▎  | 245/335 [24:48<08:29,  5.66s/it] 73%|███████▎  | 246/335 [24:53<08:22,  5.64s/it] 74%|███████▎  | 247/335 [25:00<08:31,  5.81s/it] 74%|███████▍  | 248/335 [25:06<08:42,  6.01s/it] 74%|███████▍  | 249/335 [25:11<08:20,  5.82s/it] 75%|███████▍  | 250/335 [25:17<08:13,  5.81s/it] 75%|███████▍  | 251/335 [25:24<08:26,  6.02s/it] 75%|███████▌  | 252/335 [25:30<08:39,  6.25s/it] 76%|███████▌  | 253/335 [25:36<08:20,  6.10s/it] 76%|███████▌  | 254/335 [25:42<08:16,  6.13s/it] 76%|███████▌  | 255/335 [25:49<08:25,  6.32s/it] 76%|███████▋  | 256/335 [25:55<08:09,  6.20s/it] 77%|███████▋  | 257/335 [26:01<08:00,  6.15s/it] 77%|███████▋  | 258/335 [26:07<07:41,  6.00s/it] 77%|███████▋  | 259/335 [26:13<07:42,  6.09s/it] 78%|███████▊  | 260/335 [26:19<07:25,  5.94s/it] 78%|███████▊  | 261/335 [26:24<07:09,  5.80s/it] 78%|███████▊  | 262/335 [26:30<07:01,  5.77s/it] 79%|███████▊  | 263/335 [26:36<06:53,  5.75s/it] 79%|███████▉  | 264/335 [26:41<06:46,  5.72s/it] 79%|███████▉  | 265/335 [26:47<06:51,  5.88s/it] 79%|███████▉  | 266/335 [26:53<06:44,  5.87s/it] 80%|███████▉  | 267/335 [27:00<06:49,  6.02s/it] 80%|████████  | 268/335 [27:05<06:27,  5.78s/it] 80%|████████  | 269/335 [27:11<06:34,  5.98s/it][INFO|trainer.py:4117] 2025-01-06 20:38:40,238 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:38:40,238 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:38:40,238 >>   Batch size = 1

  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|▎         | 2/60 [00:00<00:05,  9.83it/s][A
  5%|▌         | 3/60 [00:00<00:10,  5.66it/s][A
  7%|▋         | 4/60 [00:00<00:11,  4.94it/s][A
  8%|▊         | 5/60 [00:00<00:11,  4.59it/s][A
 10%|█         | 6/60 [00:01<00:12,  4.17it/s][A
 12%|█▏        | 7/60 [00:01<00:12,  4.40it/s][A
 13%|█▎        | 8/60 [00:01<00:12,  4.11it/s][A
 15%|█▌        | 9/60 [00:02<00:13,  3.70it/s][A
 17%|█▋        | 10/60 [00:02<00:12,  4.09it/s][A
 18%|█▊        | 11/60 [00:02<00:10,  4.48it/s][A
 20%|██        | 12/60 [00:02<00:11,  4.29it/s][A
 22%|██▏       | 13/60 [00:02<00:09,  4.70it/s][A
 23%|██▎       | 14/60 [00:03<00:11,  4.09it/s][A
 25%|██▌       | 15/60 [00:03<00:11,  3.83it/s][A
 27%|██▋       | 16/60 [00:03<00:10,  4.37it/s][A
 28%|██▊       | 17/60 [00:03<00:09,  4.43it/s][A
 30%|███       | 18/60 [00:04<00:09,  4.42it/s][A
 32%|███▏      | 19/60 [00:04<00:09,  4.41it/s][A
 33%|███▎      | 20/60 [00:04<00:08,  4.89it/s][A
 35%|███▌      | 21/60 [00:04<00:07,  4.94it/s][A
 37%|███▋      | 22/60 [00:04<00:08,  4.56it/s][A
 38%|███▊      | 23/60 [00:05<00:08,  4.31it/s][A
 40%|████      | 24/60 [00:05<00:07,  4.56it/s][A
 42%|████▏     | 25/60 [00:05<00:08,  4.22it/s][A
 43%|████▎     | 26/60 [00:05<00:08,  4.10it/s][A
 45%|████▌     | 27/60 [00:06<00:08,  4.05it/s][A
 47%|████▋     | 28/60 [00:06<00:08,  3.68it/s][A
 48%|████▊     | 29/60 [00:06<00:07,  3.88it/s][A
 50%|█████     | 30/60 [00:06<00:06,  4.38it/s][A
 52%|█████▏    | 31/60 [00:07<00:07,  3.74it/s][A
 53%|█████▎    | 32/60 [00:07<00:07,  3.86it/s][A
 55%|█████▌    | 33/60 [00:07<00:06,  4.06it/s][A
 57%|█████▋    | 34/60 [00:07<00:06,  4.18it/s][A
 58%|█████▊    | 35/60 [00:08<00:06,  4.13it/s][A
 60%|██████    | 36/60 [00:08<00:06,  3.67it/s][A
 62%|██████▏   | 37/60 [00:08<00:05,  3.87it/s][A
 63%|██████▎   | 38/60 [00:08<00:05,  3.92it/s][A
 65%|██████▌   | 39/60 [00:09<00:05,  3.95it/s][A
 67%|██████▋   | 40/60 [00:09<00:04,  4.25it/s][A
 68%|██████▊   | 41/60 [00:09<00:04,  4.22it/s][A
 70%|███████   | 42/60 [00:09<00:04,  4.16it/s][A
 72%|███████▏  | 43/60 [00:10<00:04,  4.19it/s][A
 73%|███████▎  | 44/60 [00:10<00:03,  4.34it/s][A
 75%|███████▌  | 45/60 [00:10<00:03,  4.32it/s][A
 77%|███████▋  | 46/60 [00:10<00:03,  4.19it/s][A
 78%|███████▊  | 47/60 [00:11<00:03,  3.90it/s][A
 80%|████████  | 48/60 [00:11<00:02,  4.04it/s][A
 82%|████████▏ | 49/60 [00:11<00:02,  4.24it/s][A
 83%|████████▎ | 50/60 [00:11<00:02,  4.54it/s][A
 85%|████████▌ | 51/60 [00:12<00:02,  4.21it/s][A
 87%|████████▋ | 52/60 [00:12<00:02,  3.84it/s][A
 88%|████████▊ | 53/60 [00:12<00:01,  3.72it/s][A
 90%|█████████ | 54/60 [00:12<00:01,  3.84it/s][A
 92%|█████████▏| 55/60 [00:13<00:01,  3.90it/s][A
 93%|█████████▎| 56/60 [00:13<00:01,  3.49it/s][A
 95%|█████████▌| 57/60 [00:13<00:00,  3.89it/s][A
 97%|█████████▋| 58/60 [00:13<00:00,  4.18it/s][A
 98%|█████████▊| 59/60 [00:14<00:00,  4.41it/s][A
100%|██████████| 60/60 [00:14<00:00,  4.43it/s][A                                                 
                                               [A{'eval_loss': 0.002379727317020297, 'eval_runtime': 14.5383, 'eval_samples_per_second': 8.254, 'eval_steps_per_second': 4.127, 'epoch': 4.0}
 80%|████████  | 269/335 [27:26<06:34,  5.98s/it]
100%|██████████| 60/60 [00:14<00:00,  4.43it/s][A
                                               [A 81%|████████  | 270/335 [27:32<11:18, 10.43s/it] 81%|████████  | 271/335 [27:38<09:47,  9.18s/it] 81%|████████  | 272/335 [27:45<08:44,  8.32s/it] 81%|████████▏ | 273/335 [27:51<07:50,  7.59s/it] 82%|████████▏ | 274/335 [27:57<07:26,  7.32s/it] 82%|████████▏ | 275/335 [28:03<06:47,  6.78s/it] 82%|████████▏ | 276/335 [28:08<06:18,  6.41s/it] 83%|████████▎ | 277/335 [28:14<05:53,  6.10s/it] 83%|████████▎ | 278/335 [28:20<05:42,  6.01s/it] 83%|████████▎ | 279/335 [28:26<05:42,  6.11s/it] 84%|████████▎ | 280/335 [28:32<05:27,  5.96s/it] 84%|████████▍ | 281/335 [28:38<05:24,  6.00s/it] 84%|████████▍ | 282/335 [28:43<05:10,  5.85s/it] 84%|████████▍ | 283/335 [28:49<05:03,  5.83s/it] 85%|████████▍ | 284/335 [28:56<05:11,  6.10s/it] 85%|████████▌ | 285/335 [29:02<05:02,  6.05s/it] 85%|████████▌ | 286/335 [29:08<04:56,  6.04s/it] 86%|████████▌ | 287/335 [29:14<04:54,  6.13s/it] 86%|████████▌ | 288/335 [29:20<04:51,  6.21s/it] 86%|████████▋ | 289/335 [29:26<04:39,  6.08s/it] 87%|████████▋ | 290/335 [29:32<04:24,  5.88s/it] 87%|████████▋ | 291/335 [29:38<04:30,  6.15s/it] 87%|████████▋ | 292/335 [29:44<04:18,  6.02s/it] 87%|████████▋ | 293/335 [29:50<04:07,  5.90s/it] 88%|████████▊ | 294/335 [29:55<03:54,  5.71s/it] 88%|████████▊ | 295/335 [30:00<03:37,  5.43s/it] 88%|████████▊ | 296/335 [30:06<03:39,  5.64s/it] 89%|████████▊ | 297/335 [30:11<03:27,  5.47s/it] 89%|████████▉ | 298/335 [30:16<03:19,  5.39s/it] 89%|████████▉ | 299/335 [30:21<03:13,  5.36s/it] 90%|████████▉ | 300/335 [30:27<03:12,  5.49s/it] 90%|████████▉ | 301/335 [30:34<03:16,  5.77s/it] 90%|█████████ | 302/335 [30:39<03:08,  5.72s/it] 90%|█████████ | 303/335 [30:45<03:01,  5.66s/it] 91%|█████████ | 304/335 [30:50<02:56,  5.69s/it] 91%|█████████ | 305/335 [30:56<02:46,  5.56s/it] 91%|█████████▏| 306/335 [31:03<02:52,  5.96s/it] 92%|█████████▏| 307/335 [31:09<02:50,  6.11s/it] 92%|█████████▏| 308/335 [31:15<02:41,  5.99s/it] 92%|█████████▏| 309/335 [31:21<02:35,  5.98s/it] 93%|█████████▎| 310/335 [31:26<02:20,  5.64s/it] 93%|█████████▎| 311/335 [31:31<02:11,  5.49s/it] 93%|█████████▎| 312/335 [31:37<02:09,  5.64s/it] 93%|█████████▎| 313/335 [31:42<02:04,  5.64s/it] 94%|█████████▎| 314/335 [31:49<02:02,  5.84s/it] 94%|█████████▍| 315/335 [31:55<01:57,  5.88s/it] 94%|█████████▍| 316/335 [32:00<01:48,  5.72s/it] 95%|█████████▍| 317/335 [32:06<01:44,  5.83s/it] 95%|█████████▍| 318/335 [32:11<01:35,  5.61s/it] 95%|█████████▌| 319/335 [32:17<01:32,  5.77s/it] 96%|█████████▌| 320/335 [32:23<01:26,  5.79s/it] 96%|█████████▌| 321/335 [32:29<01:22,  5.86s/it] 96%|█████████▌| 322/335 [32:34<01:13,  5.65s/it] 96%|█████████▋| 323/335 [32:40<01:08,  5.68s/it] 97%|█████████▋| 324/335 [32:45<01:01,  5.58s/it] 97%|█████████▋| 325/335 [32:51<00:55,  5.57s/it] 97%|█████████▋| 326/335 [32:57<00:50,  5.66s/it] 98%|█████████▊| 327/335 [33:03<00:45,  5.69s/it] 98%|█████████▊| 328/335 [33:09<00:41,  5.87s/it] 98%|█████████▊| 329/335 [33:15<00:35,  5.95s/it] 99%|█████████▊| 330/335 [33:22<00:30,  6.10s/it] 99%|█████████▉| 331/335 [33:27<00:24,  6.06s/it] 99%|█████████▉| 332/335 [33:34<00:18,  6.25s/it] 99%|█████████▉| 333/335 [33:40<00:12,  6.27s/it]100%|█████████▉| 334/335 [33:46<00:06,  6.09s/it]100%|██████████| 335/335 [33:52<00:00,  6.04s/it][INFO|trainer.py:3801] 2025-01-06 20:45:20,961 >> Saving model checkpoint to saves/qwen-14b-hi-e5/lora/sft/checkpoint-335
[INFO|configuration_utils.py:677] 2025-01-06 20:45:20,981 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-06 20:45:20,982 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2025-01-06 20:45:21,106 >> tokenizer config file saved in saves/qwen-14b-hi-e5/lora/sft/checkpoint-335/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-06 20:45:21,106 >> Special tokens file saved in saves/qwen-14b-hi-e5/lora/sft/checkpoint-335/special_tokens_map.json
[INFO|trainer.py:4117] 2025-01-06 20:45:21,346 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:45:21,346 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:45:21,346 >>   Batch size = 1

  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|▎         | 2/60 [00:00<00:05,  9.86it/s][A
  5%|▌         | 3/60 [00:00<00:10,  5.67it/s][A
  7%|▋         | 4/60 [00:00<00:11,  4.94it/s][A
  8%|▊         | 5/60 [00:00<00:11,  4.59it/s][A
 10%|█         | 6/60 [00:01<00:12,  4.17it/s][A
 12%|█▏        | 7/60 [00:01<00:12,  4.40it/s][A
 13%|█▎        | 8/60 [00:01<00:12,  4.11it/s][A
 15%|█▌        | 9/60 [00:02<00:13,  3.70it/s][A
 17%|█▋        | 10/60 [00:02<00:12,  4.09it/s][A
 18%|█▊        | 11/60 [00:02<00:10,  4.48it/s][A
 20%|██        | 12/60 [00:02<00:11,  4.29it/s][A
 22%|██▏       | 13/60 [00:02<00:09,  4.71it/s][A
 23%|██▎       | 14/60 [00:03<00:11,  4.09it/s][A
 25%|██▌       | 15/60 [00:03<00:11,  3.83it/s][A
 27%|██▋       | 16/60 [00:03<00:10,  4.37it/s][A
 28%|██▊       | 17/60 [00:03<00:09,  4.43it/s][A
 30%|███       | 18/60 [00:04<00:09,  4.42it/s][A
 32%|███▏      | 19/60 [00:04<00:09,  4.41it/s][A
 33%|███▎      | 20/60 [00:04<00:08,  4.88it/s][A
 35%|███▌      | 21/60 [00:04<00:07,  4.93it/s][A
 37%|███▋      | 22/60 [00:04<00:08,  4.56it/s][A
 38%|███▊      | 23/60 [00:05<00:08,  4.31it/s][A
 40%|████      | 24/60 [00:05<00:07,  4.55it/s][A
 42%|████▏     | 25/60 [00:05<00:08,  4.22it/s][A
 43%|████▎     | 26/60 [00:05<00:08,  4.10it/s][A
 45%|████▌     | 27/60 [00:06<00:08,  4.05it/s][A
 47%|████▋     | 28/60 [00:06<00:08,  3.68it/s][A
 48%|████▊     | 29/60 [00:06<00:07,  3.88it/s][A
 50%|█████     | 30/60 [00:06<00:06,  4.38it/s][A
 52%|█████▏    | 31/60 [00:07<00:07,  3.74it/s][A
 53%|█████▎    | 32/60 [00:07<00:07,  3.86it/s][A
 55%|█████▌    | 33/60 [00:07<00:06,  4.06it/s][A
 57%|█████▋    | 34/60 [00:07<00:06,  4.18it/s][A
 58%|█████▊    | 35/60 [00:08<00:06,  4.13it/s][A
 60%|██████    | 36/60 [00:08<00:06,  3.67it/s][A
 62%|██████▏   | 37/60 [00:08<00:05,  3.87it/s][A
 63%|██████▎   | 38/60 [00:08<00:05,  3.91it/s][A
 65%|██████▌   | 39/60 [00:09<00:05,  3.95it/s][A
 67%|██████▋   | 40/60 [00:09<00:04,  4.25it/s][A
 68%|██████▊   | 41/60 [00:09<00:04,  4.22it/s][A
 70%|███████   | 42/60 [00:09<00:04,  4.16it/s][A
 72%|███████▏  | 43/60 [00:10<00:04,  4.19it/s][A
 73%|███████▎  | 44/60 [00:10<00:03,  4.34it/s][A
 75%|███████▌  | 45/60 [00:10<00:03,  4.32it/s][A
 77%|███████▋  | 46/60 [00:10<00:03,  4.19it/s][A
 78%|███████▊  | 47/60 [00:11<00:03,  3.90it/s][A
 80%|████████  | 48/60 [00:11<00:02,  4.04it/s][A
 82%|████████▏ | 49/60 [00:11<00:02,  4.24it/s][A
 83%|████████▎ | 50/60 [00:11<00:02,  4.53it/s][A
 85%|████████▌ | 51/60 [00:12<00:02,  4.21it/s][A
 87%|████████▋ | 52/60 [00:12<00:02,  3.84it/s][A
 88%|████████▊ | 53/60 [00:12<00:01,  3.73it/s][A
 90%|█████████ | 54/60 [00:12<00:01,  3.84it/s][A
 92%|█████████▏| 55/60 [00:13<00:01,  3.90it/s][A
 93%|█████████▎| 56/60 [00:13<00:01,  3.49it/s][A
 95%|█████████▌| 57/60 [00:13<00:00,  3.89it/s][A
 97%|█████████▋| 58/60 [00:13<00:00,  4.18it/s][A
 98%|█████████▊| 59/60 [00:14<00:00,  4.42it/s][A
100%|██████████| 60/60 [00:14<00:00,  4.43it/s][A                                                 
                                               [A{'eval_loss': 0.002399302087724209, 'eval_runtime': 14.5403, 'eval_samples_per_second': 8.253, 'eval_steps_per_second': 4.126, 'epoch': 4.98}
100%|██████████| 335/335 [34:07<00:00,  6.04s/it]
100%|██████████| 60/60 [00:14<00:00,  4.43it/s][A
                                               [A[INFO|trainer.py:2584] 2025-01-06 20:45:35,886 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 2049.8609, 'train_samples_per_second': 2.622, 'train_steps_per_second': 0.163, 'train_loss': 0.10667623343752392, 'epoch': 4.98}
100%|██████████| 335/335 [34:07<00:00,  6.04s/it]100%|██████████| 335/335 [34:07<00:00,  6.11s/it]
[INFO|trainer.py:3801] 2025-01-06 20:45:35,887 >> Saving model checkpoint to saves/qwen-14b-hi-e5/lora/sft
[INFO|configuration_utils.py:677] 2025-01-06 20:45:35,906 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-06 20:45:35,907 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2025-01-06 20:45:36,029 >> tokenizer config file saved in saves/qwen-14b-hi-e5/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-06 20:45:36,030 >> Special tokens file saved in saves/qwen-14b-hi-e5/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =      4.9814
  total_flos               = 291033312GF
  train_loss               =      0.1067
  train_runtime            =  0:34:09.86
  train_samples_per_second =       2.622
  train_steps_per_second   =       0.163
Figure saved at: saves/qwen-14b-hi-e5/lora/sft/training_loss.png
Figure saved at: saves/qwen-14b-hi-e5/lora/sft/training_eval_loss.png
[WARNING|2025-01-06 20:45:36] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4117] 2025-01-06 20:45:36,177 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:45:36,177 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:45:36,178 >>   Batch size = 1
  0%|          | 0/60 [00:00<?, ?it/s]  3%|▎         | 2/60 [00:00<00:05,  9.85it/s]  5%|▌         | 3/60 [00:00<00:10,  5.67it/s]  7%|▋         | 4/60 [00:00<00:11,  4.94it/s]  8%|▊         | 5/60 [00:00<00:11,  4.59it/s] 10%|█         | 6/60 [00:01<00:12,  4.17it/s] 12%|█▏        | 7/60 [00:01<00:12,  4.40it/s] 13%|█▎        | 8/60 [00:01<00:12,  4.12it/s] 15%|█▌        | 9/60 [00:02<00:13,  3.70it/s] 17%|█▋        | 10/60 [00:02<00:12,  4.09it/s] 18%|█▊        | 11/60 [00:02<00:10,  4.48it/s] 20%|██        | 12/60 [00:02<00:11,  4.29it/s] 22%|██▏       | 13/60 [00:02<00:09,  4.71it/s] 23%|██▎       | 14/60 [00:03<00:11,  4.09it/s] 25%|██▌       | 15/60 [00:03<00:11,  3.83it/s] 27%|██▋       | 16/60 [00:03<00:10,  4.37it/s] 28%|██▊       | 17/60 [00:03<00:09,  4.43it/s] 30%|███       | 18/60 [00:04<00:09,  4.42it/s] 32%|███▏      | 19/60 [00:04<00:09,  4.41it/s] 33%|███▎      | 20/60 [00:04<00:08,  4.88it/s] 35%|███▌      | 21/60 [00:04<00:07,  4.93it/s] 37%|███▋      | 22/60 [00:04<00:08,  4.56it/s] 38%|███▊      | 23/60 [00:05<00:08,  4.31it/s] 40%|████      | 24/60 [00:05<00:07,  4.55it/s] 42%|████▏     | 25/60 [00:05<00:08,  4.22it/s] 43%|████▎     | 26/60 [00:05<00:08,  4.10it/s] 45%|████▌     | 27/60 [00:06<00:08,  4.05it/s] 47%|████▋     | 28/60 [00:06<00:08,  3.68it/s] 48%|████▊     | 29/60 [00:06<00:07,  3.88it/s] 50%|█████     | 30/60 [00:06<00:06,  4.38it/s] 52%|█████▏    | 31/60 [00:07<00:07,  3.74it/s] 53%|█████▎    | 32/60 [00:07<00:07,  3.87it/s] 55%|█████▌    | 33/60 [00:07<00:06,  4.06it/s] 57%|█████▋    | 34/60 [00:07<00:06,  4.18it/s] 58%|█████▊    | 35/60 [00:08<00:06,  4.13it/s] 60%|██████    | 36/60 [00:08<00:06,  3.67it/s] 62%|██████▏   | 37/60 [00:08<00:05,  3.87it/s] 63%|██████▎   | 38/60 [00:08<00:05,  3.92it/s] 65%|██████▌   | 39/60 [00:09<00:05,  3.96it/s] 67%|██████▋   | 40/60 [00:09<00:04,  4.25it/s] 68%|██████▊   | 41/60 [00:09<00:04,  4.22it/s] 70%|███████   | 42/60 [00:09<00:04,  4.16it/s] 72%|███████▏  | 43/60 [00:10<00:04,  4.19it/s] 73%|███████▎  | 44/60 [00:10<00:03,  4.34it/s] 75%|███████▌  | 45/60 [00:10<00:03,  4.32it/s] 77%|███████▋  | 46/60 [00:10<00:03,  4.19it/s] 78%|███████▊  | 47/60 [00:11<00:03,  3.90it/s] 80%|████████  | 48/60 [00:11<00:02,  4.05it/s] 82%|████████▏ | 49/60 [00:11<00:02,  4.24it/s] 83%|████████▎ | 50/60 [00:11<00:02,  4.54it/s] 85%|████████▌ | 51/60 [00:12<00:02,  4.21it/s] 87%|████████▋ | 52/60 [00:12<00:02,  3.84it/s] 88%|████████▊ | 53/60 [00:12<00:01,  3.73it/s] 90%|█████████ | 54/60 [00:12<00:01,  3.85it/s] 92%|█████████▏| 55/60 [00:13<00:01,  3.90it/s] 93%|█████████▎| 56/60 [00:13<00:01,  3.49it/s] 95%|█████████▌| 57/60 [00:13<00:00,  3.89it/s] 97%|█████████▋| 58/60 [00:13<00:00,  4.18it/s] 98%|█████████▊| 59/60 [00:14<00:00,  4.42it/s]100%|██████████| 60/60 [00:14<00:00,  4.43it/s]100%|██████████| 60/60 [00:14<00:00,  4.20it/s]
***** eval metrics *****
  epoch                   =     4.9814
  eval_loss               =     0.0024
  eval_runtime            = 0:00:14.53
  eval_samples_per_second =      8.256
  eval_steps_per_second   =      4.128
[INFO|modelcard.py:449] 2025-01-06 20:45:50,713 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[rank0]:[W106 20:45:51.263996954 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())

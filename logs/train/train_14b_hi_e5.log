[2025-01-06 20:10:20,860] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[INFO|2025-01-06 20:10:22] llamafactory.cli:157 >> Initializing distributed tasks at: 127.0.0.1:21690
W0106 20:10:22.742000 1884685 site-packages/torch/distributed/run.py:793] 
W0106 20:10:22.742000 1884685 site-packages/torch/distributed/run.py:793] *****************************************
W0106 20:10:22.742000 1884685 site-packages/torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0106 20:10:22.742000 1884685 site-packages/torch/distributed/run.py:793] *****************************************
[2025-01-06 20:10:24,267] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-01-06 20:10:24,298] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[WARNING|2025-01-06 20:10:25] llamafactory.hparams.parser:162 >> We recommend enable `upcast_layernorm` in quantized training.
[WARNING|2025-01-06 20:10:25] llamafactory.hparams.parser:162 >> `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
[INFO|2025-01-06 20:10:25] llamafactory.hparams.parser:359 >> Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|configuration_utils.py:677] 2025-01-06 20:10:25,111 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-06 20:10:25,111 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,112 >> loading file tokenizer_config.json
[INFO|2025-01-06 20:10:25] llamafactory.hparams.parser:359 >> Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2475] 2025-01-06 20:10:25,258 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:677] 2025-01-06 20:10:25,258 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-06 20:10:25,258 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file vocab.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file merges.txt
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file tokenizer.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2209] 2025-01-06 20:10:25,259 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2475] 2025-01-06 20:10:25,395 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|2025-01-06 20:10:25] llamafactory.data.template:157 >> Add <|im_end|> to stop words.
[INFO|2025-01-06 20:10:25] llamafactory.data.loader:157 >> Loading dataset entity_trans_hi.json...
[rank1]:[W106 20:10:25.590066881 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.
Generating train split: 0 examples [00:00, ? examples/s]Generating train split: 1195 examples [00:00, 36948.91 examples/s]
Converting format of dataset (num_proc=16):   0%|          | 0/1195 [00:00<?, ? examples/s]Converting format of dataset (num_proc=16): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1195/1195 [00:00<00:00, 9993.83 examples/s]
[rank0]:[W106 20:10:49.932559052 ProcessGroupNCCL.cpp:4115] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device,or call init_process_group() with a device_id.
Running tokenizer on dataset (num_proc=16):   0%|          | 0/1195 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=16):   6%|‚ñã         | 75/1195 [00:00<00:05, 213.35 examples/s]Running tokenizer on dataset (num_proc=16):  19%|‚ñà‚ñâ        | 225/1195 [00:00<00:01, 564.76 examples/s]Running tokenizer on dataset (num_proc=16):  31%|‚ñà‚ñà‚ñà‚ñè      | 375/1195 [00:00<00:00, 823.29 examples/s]Running tokenizer on dataset (num_proc=16):  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 600/1195 [00:00<00:00, 999.02 examples/s]Running tokenizer on dataset (num_proc=16):  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 825/1195 [00:00<00:00, 1245.00 examples/s]Running tokenizer on dataset (num_proc=16):  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 1047/1195 [00:01<00:00, 1328.24 examples/s]Running tokenizer on dataset (num_proc=16): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1195/1195 [00:01<00:00, 1005.35 examples/s]
training example:
input_ids:
[151644, 8948, 198, 2610, 525, 1207, 16948, 11, 3465, 553, 54364, 14817, 13, 1446, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 2610, 525, 264, 9990, 43980, 12, 22574, 14468, 7341, 11, 4486, 1492, 752, 14683, 1045, 6364, 22870, 1119, 43980, 13, 7036, 429, 279, 4396, 1102, 1265, 387, 264, 17133, 429, 7952, 304, 279, 11652, 624, 40, 686, 2968, 498, 458, 6364, 17133, 323, 264, 43980, 11652, 11, 1380, 279, 6364, 17133, 374, 279, 14468, 1102, 304, 279, 43980, 11652, 13, 358, 1366, 311, 1477, 279, 7112, 43980, 17133, 315, 279, 6364, 14468, 1102, 624, 5501, 1744, 3019, 553, 3019, 13, 151645, 198, 151644, 77091, 198, 39814, 11, 358, 646, 7789, 448, 429, 13, 5209, 3410, 279, 6364, 17133, 323, 279, 43980, 11652, 432, 7952, 304, 773, 358, 646, 10542, 279, 12159, 43980, 17133, 369, 498, 382, 5501, 3561, 279, 1946, 438, 11017, 1447, 22574, 17133, 25, 508, 4208, 6364, 17133, 921, 39, 28685, 11652, 25, 508, 4208, 43980, 11652, 2533, 40, 3278, 1221, 3410, 279, 7112, 43980, 17133, 429, 33210, 311, 279, 6364, 14468, 1102, 13, 151645, 198, 151644, 872, 198, 22574, 17133, 25, 9812, 198, 39, 28685, 11652, 25, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 320, 146378, 72314, 145959, 85033, 54784, 250, 5502, 120, 43647, 25, 7513, 9145, 2376, 38807, 8, 91217, 72653, 146800, 30484, 107, 79238, 25, 14925, 107, 12619, 224, 44179, 54575, 86162, 91217, 54784, 224, 68158, 30484, 98, 42311, 97, 220, 17, 22, 14925, 99, 54784, 114, 54575, 72314, 47809, 23868, 14925, 237, 64704, 14925, 108, 31411, 250, 60096, 12619, 230, 79238, 42311, 243, 14925, 237, 145535, 72314, 14925, 228, 44179, 30484, 98, 42311, 243, 91217, 72314, 146113, 84310, 12619, 230, 14925, 250, 42311, 101, 87244, 54784, 224, 14925, 228, 86162, 78368, 91217, 54784, 224, 83636, 85033, 145966, 31411, 116, 64704, 43647, 145420, 68158, 31411, 251, 54784, 99, 31411, 108, 43647, 84310, 54575, 79238, 43647, 84310, 12619, 230, 14925, 250, 54575, 68158, 72314, 147676, 47809, 34370, 47809, 147131, 14925, 107, 23868, 68158, 146101, 43647, 14925, 108, 31411, 115, 30484, 253, 85033, 54575, 83636, 44179, 14925, 110, 31411, 245, 12619, 224, 84310, 54575, 79238, 43647, 84310, 12619, 230, 146031, 14925, 229, 78368, 64704, 23868, 14925, 227, 146101, 30484, 107, 72653, 145256, 145420, 220, 16, 14925, 250, 60096, 145535, 44179, 43647, 220, 16, 24, 24, 18, 91217, 54784, 224, 14925, 108, 54575, 87244, 47809, 43647, 68158, 72314, 146821, 38851, 14925, 99, 30484, 113, 31411, 108, 23868, 14925, 107, 12619, 224, 44179, 54575, 86162, 42311, 107, 14925, 228, 44179, 30484, 98, 42311, 243, 83636, 44179, 42311, 115, 145256, 47809, 34370, 91217, 31411, 100, 30484, 107, 87244, 68158, 34370, 14925, 249, 93948, 14925, 107, 12619, 224, 44179, 54575, 86162, 42311, 107, 14925, 99, 54784, 114, 54575, 72314, 47809, 43647, 14925, 228, 44179, 30484, 98, 42311, 243, 14925, 255, 31411, 245, 43647, 145256, 31411, 108, 43647, 68158, 34370, 84310, 72653, 146399, 14925, 98, 23868, 146031, 14925, 97, 145799, 68158, 34370, 14925, 229, 78368, 87244, 54784, 224, 68158, 145256, 78368, 30484, 107, 14925, 99, 54784, 114, 54575, 72314, 47809, 43647, 68158, 72314, 146800, 30484, 107, 23868, 91217, 54784, 224, 14925, 110, 145959, 31411, 97, 31411, 108, 14925, 105, 149269, 54575, 79238, 30484, 97, 44179, 43647, 84310, 54575, 79238, 43647, 14925, 108, 93948, 43647, 14925, 242, 44179, 14925, 229, 78368, 64704, 43647, 14925, 101, 43647, 79238, 42311, 107, 54575, 72314, 91217, 54784, 224, 14925, 105, 93948, 72653, 79238, 68158, 34370, 83636, 44179, 42311, 113, 44179, 30484, 97, 60096, 14925, 255, 43647, 14925, 114, 31411, 106, 42311, 110, 47809, 42311, 107, 34370, 14925, 245, 145420, 34370, 146031, 220, 16, 24, 24, 17, 91217, 54784, 224, 91217, 31411, 116, 30484, 97, 85033, 42311, 244, 68158, 72314, 146821, 38851, 14925, 99, 30484, 113, 31411, 108, 23868, 14925, 229, 78368, 64704, 34370, 14925, 228, 146821, 72653, 60096, 42311, 243, 14925, 113, 12619, 230, 146821, 31411, 101, 42311, 243, 68158, 30484, 113, 44179, 12619, 224, 86162, 47809, 43647, 14925, 101, 43647, 72314, 145535, 14925, 108, 146800, 43647, 14925, 245, 145420, 43647, 146031, 14925, 99, 42311, 116, 87244, 30484, 105, 44179, 220, 17, 15, 15, 24, 91217, 54784, 224, 14925, 110, 42311, 116, 30484, 105, 60096, 68158, 87244, 148971, 12619, 234, 79238, 23868, 14925, 250, 42311, 116, 64704, 34370, 14925, 99, 30484, 113, 31411, 108, 23868, 14925, 229, 78368, 87244, 54784, 224, 14925, 242, 44179, 14925, 113, 30484, 107, 31411, 103, 64704, 68158, 72653, 146821, 31411, 108, 54575, 72314, 47809, 43647, 83636, 85033, 64704, 85033, 42311, 107, 23868, 220, 16, 14925, 250, 60096, 145535, 44179, 43647, 220, 17, 15, 15, 23, 68158, 34370, 14925, 114, 72653, 44179, 72653, 47809, 43647, 14925, 245, 145420, 43647, 84310, 12619, 230, 146031, 220, 151645, 198, 151644, 77091, 198, 785, 2661, 6364, 17133, 9812, 646, 387, 24531, 1119, 43980, 438, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 11, 323, 279, 7112, 43980, 17133, 304, 279, 2661, 11652, 1265, 387, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 13, 151645, 198, 151644, 872, 198, 3973, 279, 1102, 323, 1281, 2704, 1817, 3409, 315, 279, 1102, 7952, 304, 279, 2661, 43980, 11652, 13, 151645, 198, 151644, 77091, 198, 39814, 0, 358, 614, 10067, 279, 1102, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 323, 432, 2167, 7952, 304, 279, 2661, 11652, 13, 151645, 198, 151644, 872, 198, 35127, 752, 279, 1590, 1102, 2041, 1008, 4244, 13, 151645, 198, 151644, 77091, 198, 145420, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 151645]
inputs:
<|im_start|>system
You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>
<|im_start|>user
You are a senior Hindi-English translation master, please help me translate some English sentences into Hindi. Note that the correct result should be a phrase that appears in the sentence.
I will give you an English phrase and a Hindi sentence, where the English phrase is the translation result in the Hindi sentence. I want to find the raw Hindi phrase of the English translation result.
Please think step by step.<|im_end|>
<|im_start|>assistant
Sure, I can assist with that. Please provide the English phrase and the Hindi sentence it appears in so I can identify the corresponding Hindi phrase for you.

Please format the input as follows:

English phrase: [insert English phrase]
Hindi sentence: [insert Hindi sentence]

I'll then provide the raw Hindi phrase that corresponds to the English translation result.<|im_end|>
<|im_start|>user
English phrase: EU
Hindi sentence: ‡§Ø‡•Ç‡§∞‡•ã‡§™‡•Ä‡§Ø ‡§∏‡§Ç‡§ò (‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡§º‡•Ä: European Union)(EU) ‡§Æ‡•Å‡§ñ‡•ç‡§Ø‡§§: ‡§Ø‡•Ç‡§∞‡•ã‡§™ ‡§Æ‡•á‡§Ç ‡§∏‡•ç‡§•‡§ø‡§§ 27 ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§ï‡§æ ‡§è‡§ï ‡§∞‡§æ‡§ú‡§®‡•à‡§§‡§ø‡§ï ‡§è‡§µ‡§Ç ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§Æ‡§Ç‡§ö ‡§π‡•à ‡§ú‡§ø‡§®‡§Æ‡•á‡§Ç ‡§Ü‡§™‡§∏ ‡§Æ‡•á‡§Ç ‡§™‡•ç‡§∞‡§∂‡§æ‡§∏‡§ï‡•Ä‡§Ø ‡§∏‡§æ‡§ù‡•á‡§¶‡§æ‡§∞‡•Ä ‡§π‡•ã‡§§‡•Ä ‡§π‡•à ‡§ú‡•ã ‡§∏‡§Ç‡§ò ‡§ï‡•á ‡§ï‡§à ‡§Ø‡§æ ‡§∏‡§≠‡•Ä ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡•ã ‡§™‡§∞ ‡§≤‡§æ‡§ó‡•Ç ‡§π‡•ã‡§§‡•Ä ‡§π‡•à‡•§ ‡§á‡§∏‡§ï‡§æ ‡§Ö‡§≠‡•ç‡§Ø‡•Å‡§¶‡§Ø 1 ‡§ú‡§®‡§µ‡§∞‡•Ä 1993 ‡§Æ‡•á‡§Ç ‡§∞‡•ã‡§Æ ‡§ï‡•Ä ‡§∏‡§Ç‡§ß‡§ø ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§Ø‡•Ç‡§∞‡•ã‡§™‡§ø‡§Ø ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§™‡§∞‡§ø‡§∑‡§¶ ‡§ï‡•á ‡§Æ‡§æ‡§ß‡•ç‡§Ø‡§Æ ‡§∏‡•á ‡§õ‡§π ‡§Ø‡•Ç‡§∞‡•ã‡§™‡§ø‡§Ø ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§ï‡•Ä ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§≠‡§æ‡§ó‡•Ä‡§¶‡§æ‡§∞‡•Ä ‡§∏‡•á ‡§π‡•Å‡§Ü ‡§•‡§æ‡•§ ‡§§‡§¨ ‡§∏‡•á ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§∏‡§¶‡§∏‡•ç‡§Ø ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§ï‡•Ä ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ ‡§Æ‡•á‡§Ç ‡§≤‡§ó‡§æ‡§§‡§æ‡§∞ ‡§¨‡§¢‡•ã‡§§‡•ç‡§§‡§∞‡•Ä ‡§π‡•ã‡§§‡•Ä ‡§∞‡§π‡•Ä ‡§î‡§∞ ‡§á‡§∏‡§ï‡•Ä ‡§®‡•Ä‡§§‡§ø‡§Ø‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¨‡§π‡•Å‡§§ ‡§∏‡•á ‡§™‡§∞‡§ø‡§µ‡§∞‡•ç‡§§‡§® ‡§≠‡•Ä ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§ï‡§ø‡§Ø‡•á ‡§ó‡§Ø‡•á‡•§ 1992 ‡§Æ‡•á‡§Ç ‡§Æ‡§æ‡§∏‡•ç‡§§‡•ç‡§∞‡§ø‡§ñ ‡§∏‡§Ç‡§ß‡§ø ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§á‡§∏‡§ï‡•á ‡§Ü‡§ß‡•Å‡§®‡§ø‡§ï ‡§µ‡•à‡§ß‡§æ‡§®‡§ø‡§ï ‡§∏‡•ç‡§µ‡§∞‡•Ç‡§™ ‡§ï‡•Ä ‡§®‡•Ä‡§Ç‡§µ ‡§∞‡§ñ‡•Ä ‡§ó‡§Ø‡•Ä‡•§ ‡§¶‡§ø‡§∏‡§Æ‡•ç‡§¨‡§∞ 2009 ‡§Æ‡•á‡§Ç ‡§≤‡§ø‡§∏‡•ç‡§¨‡§® ‡§∏‡§Æ‡§ù‡•å‡§§‡§æ ‡§ú‡§ø‡§∏‡§ï‡•á ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§á‡§∏‡§Æ‡•á‡§Ç ‡§î‡§∞ ‡§µ‡•ç‡§Ø‡§æ‡§™‡§ï ‡§∏‡•Å‡§ß‡§æ‡§∞‡•ã‡§Ç ‡§ï‡•Ä ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ 1 ‡§ú‡§®‡§µ‡§∞‡•Ä 2008 ‡§∏‡•á ‡§∂‡•Å‡§∞‡•Å ‡§ï‡•Ä ‡§ó‡§Ø‡•Ä ‡§π‡•à‡•§ <|im_end|>
<|im_start|>assistant
The given English phrase EU can be translated into Hindi as ‡§Ø‡•Ç‡§∞‡•ã‡§™‡•Ä‡§Ø ‡§∏‡§Ç‡§ò, and the raw Hindi phrase in the given sentence should be ‡§Ø‡•Ç‡§∞‡•ã‡§™‡•Ä‡§Ø ‡§∏‡§Ç‡§ò.<|im_end|>
<|im_start|>user
Check the result and make sure each word of the result appears in the given Hindi sentence.<|im_end|>
<|im_start|>assistant
Sure! I have checked the result ‡§Ø‡•Ç‡§∞‡•ã‡§™‡•Ä‡§Ø ‡§∏‡§Ç‡§ò and it really appears in the given sentence.<|im_end|>
<|im_start|>user
Give me the final result without other words.<|im_end|>
<|im_start|>assistant
‡§Ø‡•Ç‡§∞‡•ã‡§™‡•Ä‡§Ø ‡§∏‡§Ç‡§ò<|im_end|>
label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 39814, 11, 358, 646, 7789, 448, 429, 13, 5209, 3410, 279, 6364, 17133, 323, 279, 43980, 11652, 432, 7952, 304, 773, 358, 646, 10542, 279, 12159, 43980, 17133, 369, 498, 382, 5501, 3561, 279, 1946, 438, 11017, 1447, 22574, 17133, 25, 508, 4208, 6364, 17133, 921, 39, 28685, 11652, 25, 508, 4208, 43980, 11652, 2533, 40, 3278, 1221, 3410, 279, 7112, 43980, 17133, 429, 33210, 311, 279, 6364, 14468, 1102, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 785, 2661, 6364, 17133, 9812, 646, 387, 24531, 1119, 43980, 438, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 11, 323, 279, 7112, 43980, 17133, 304, 279, 2661, 11652, 1265, 387, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 39814, 0, 358, 614, 10067, 279, 1102, 14925, 107, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 323, 432, 2167, 7952, 304, 279, 2661, 11652, 13, 151645, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 145420, 12619, 224, 44179, 54575, 86162, 43647, 145420, 68158, 72314, 147676, 151645]
labels:
Sure, I can assist with that. Please provide the English phrase and the Hindi sentence it appears in so I can identify the corresponding Hindi phrase for you.

Please format the input as follows:

English phrase: [insert English phrase]
Hindi sentence: [insert Hindi sentence]

I'll then provide the raw Hindi phrase that corresponds to the English translation result.<|im_end|>The given English phrase EU can be translated into Hindi as ‡§Ø‡•Ç‡§∞‡•ã‡§™‡•Ä‡§Ø ‡§∏‡§Ç‡§ò, and the raw Hindi phrase in the given sentence should be ‡§Ø‡•Ç‡§∞‡•ã‡§™‡•Ä‡§Ø ‡§∏‡§Ç‡§ò.<|im_end|>Sure! I have checked the result ‡§Ø‡•Ç‡§∞‡•ã‡§™‡•Ä‡§Ø ‡§∏‡§Ç‡§ò and it really appears in the given sentence.<|im_end|>‡§Ø‡•Ç‡§∞‡•ã‡§™‡•Ä‡§Ø ‡§∏‡§Ç‡§ò<|im_end|>
[INFO|configuration_utils.py:677] 2025-01-06 20:11:13,961 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-06 20:11:13,961 >> Model config Qwen2Config {
  "_name_or_path": "/mnt/sda/zzh/Qwen2.5-14B-Instruct",
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|2025-01-06 20:11:13] llamafactory.model.model_utils.quantization:157 >> Quantizing model to 4 bit with bitsandbytes.
[INFO|modeling_utils.py:3934] 2025-01-06 20:11:14,007 >> loading weights file /mnt/sda/zzh/Qwen2.5-14B-Instruct/model.safetensors.index.json
[INFO|modeling_utils.py:1670] 2025-01-06 20:11:14,007 >> Instantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:1096] 2025-01-06 20:11:14,008 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:  12%|‚ñà‚ñé        | 1/8 [00:00<00:04,  1.62it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:01<00:03,  1.62it/s]Loading checkpoint shards:  12%|‚ñà‚ñé        | 1/8 [00:01<00:10,  1.43s/it]Loading checkpoint shards:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:02<00:04,  1.13it/s]Loading checkpoint shards:  25%|‚ñà‚ñà‚ñå       | 2/8 [00:02<00:08,  1.44s/it]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:03<00:03,  1.28it/s]Loading checkpoint shards:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:03<00:02,  1.39it/s]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:04<00:01,  1.45it/s]Loading checkpoint shards:  38%|‚ñà‚ñà‚ñà‚ñä      | 3/8 [00:04<00:07,  1.45s/it]Loading checkpoint shards:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:04<00:00,  1.50it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:05<00:00,  1.81it/s]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:05<00:00,  1.53it/s]
[INFO|modeling_utils.py:4800] 2025-01-06 20:11:19,352 >> All model checkpoint weights were used when initializing Qwen2ForCausalLM.

[INFO|modeling_utils.py:4808] 2025-01-06 20:11:19,352 >> All the weights of Qwen2ForCausalLM were initialized from the model checkpoint at /mnt/sda/zzh/Qwen2.5-14B-Instruct.
If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.
[INFO|configuration_utils.py:1049] 2025-01-06 20:11:19,353 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/generation_config.json
[INFO|configuration_utils.py:1096] 2025-01-06 20:11:19,354 >> Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 0.7,
  "top_k": 20,
  "top_p": 0.8
}

[INFO|2025-01-06 20:11:19] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.
[INFO|2025-01-06 20:11:19] llamafactory.model.model_utils.attention:157 >> Using torch SDPA for faster training and inference.
[INFO|2025-01-06 20:11:19] llamafactory.model.adapter:157 >> Upcasting trainable params to float32.
[INFO|2025-01-06 20:11:19] llamafactory.model.adapter:157 >> Fine-tuning method: LoRA
[INFO|2025-01-06 20:11:19] llamafactory.model.model_utils.misc:157 >> Found linear modules: gate_proj,k_proj,q_proj,down_proj,v_proj,o_proj,up_proj
[INFO|2025-01-06 20:11:19] llamafactory.model.loader:157 >> trainable params: 34,406,400 || all params: 14,804,440,064 || trainable%: 0.2324
[INFO|trainer.py:698] 2025-01-06 20:11:19,683 >> Using auto half precision backend
Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 4/8 [00:05<00:05,  1.45s/it]Loading checkpoint shards:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 5/8 [00:07<00:04,  1.44s/it]Loading checkpoint shards:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 6/8 [00:08<00:02,  1.44s/it]Loading checkpoint shards:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 7/8 [00:10<00:01,  1.43s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:10<00:00,  1.18s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8/8 [00:10<00:00,  1.34s/it]
[INFO|trainer.py:2313] 2025-01-06 20:11:26,023 >> ***** Running training *****
[INFO|trainer.py:2314] 2025-01-06 20:11:26,023 >>   Num examples = 1,075
[INFO|trainer.py:2315] 2025-01-06 20:11:26,023 >>   Num Epochs = 5
[INFO|trainer.py:2316] 2025-01-06 20:11:26,023 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2319] 2025-01-06 20:11:26,023 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2320] 2025-01-06 20:11:26,023 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2321] 2025-01-06 20:11:26,023 >>   Total optimization steps = 335
[INFO|trainer.py:2322] 2025-01-06 20:11:26,025 >>   Number of trainable parameters = 34,406,400
  0%|          | 0/335 [00:00<?, ?it/s]  0%|          | 1/335 [00:06<34:38,  6.22s/it]  1%|          | 2/335 [00:11<30:08,  5.43s/it]  1%|          | 3/335 [00:17<32:03,  5.79s/it]  1%|          | 4/335 [00:23<32:16,  5.85s/it]  1%|‚ñè         | 5/335 [00:28<31:46,  5.78s/it]  2%|‚ñè         | 6/335 [00:34<31:37,  5.77s/it]  2%|‚ñè         | 7/335 [00:40<31:28,  5.76s/it]  2%|‚ñè         | 8/335 [00:46<31:45,  5.83s/it]  3%|‚ñé         | 9/335 [00:52<32:34,  5.99s/it]  3%|‚ñé         | 10/335 [00:58<32:12,  5.95s/it]  3%|‚ñé         | 11/335 [01:04<31:25,  5.82s/it]  4%|‚ñé         | 12/335 [01:10<32:35,  6.06s/it]  4%|‚ñç         | 13/335 [01:16<32:03,  5.97s/it]  4%|‚ñç         | 14/335 [01:22<31:32,  5.90s/it]  4%|‚ñç         | 15/335 [01:27<30:49,  5.78s/it]  5%|‚ñç         | 16/335 [01:33<30:38,  5.76s/it]  5%|‚ñå         | 17/335 [01:39<30:36,  5.78s/it]  5%|‚ñå         | 18/335 [01:45<31:30,  5.96s/it]  6%|‚ñå         | 19/335 [01:51<31:27,  5.97s/it]  6%|‚ñå         | 20/335 [01:57<31:07,  5.93s/it]  6%|‚ñã         | 21/335 [02:04<32:21,  6.18s/it]  7%|‚ñã         | 22/335 [02:09<31:08,  5.97s/it]  7%|‚ñã         | 23/335 [02:15<31:06,  5.98s/it]  7%|‚ñã         | 24/335 [02:22<32:02,  6.18s/it]  7%|‚ñã         | 25/335 [02:27<30:16,  5.86s/it]  8%|‚ñä         | 26/335 [02:33<30:27,  5.91s/it]  8%|‚ñä         | 27/335 [02:38<29:09,  5.68s/it]  8%|‚ñä         | 28/335 [02:44<30:01,  5.87s/it]  9%|‚ñä         | 29/335 [02:50<30:03,  5.89s/it]  9%|‚ñâ         | 30/335 [02:57<31:30,  6.20s/it]  9%|‚ñâ         | 31/335 [03:04<31:55,  6.30s/it] 10%|‚ñâ         | 32/335 [03:10<30:53,  6.12s/it] 10%|‚ñâ         | 33/335 [03:16<31:09,  6.19s/it] 10%|‚ñà         | 34/335 [03:21<29:29,  5.88s/it] 10%|‚ñà         | 35/335 [03:28<30:14,  6.05s/it] 11%|‚ñà         | 36/335 [03:34<30:42,  6.16s/it] 11%|‚ñà         | 37/335 [03:40<30:05,  6.06s/it] 11%|‚ñà‚ñè        | 38/335 [03:46<30:32,  6.17s/it] 12%|‚ñà‚ñè        | 39/335 [03:52<29:37,  6.01s/it] 12%|‚ñà‚ñè        | 40/335 [03:58<29:26,  5.99s/it] 12%|‚ñà‚ñè        | 41/335 [04:03<28:43,  5.86s/it] 13%|‚ñà‚ñé        | 42/335 [04:09<28:03,  5.75s/it] 13%|‚ñà‚ñé        | 43/335 [04:14<27:46,  5.71s/it] 13%|‚ñà‚ñé        | 44/335 [04:20<27:42,  5.71s/it] 13%|‚ñà‚ñé        | 45/335 [04:26<28:25,  5.88s/it] 14%|‚ñà‚ñé        | 46/335 [04:32<28:16,  5.87s/it] 14%|‚ñà‚ñç        | 47/335 [04:37<27:10,  5.66s/it] 14%|‚ñà‚ñç        | 48/335 [04:43<27:04,  5.66s/it] 15%|‚ñà‚ñç        | 49/335 [04:48<26:04,  5.47s/it] 15%|‚ñà‚ñç        | 50/335 [04:54<26:07,  5.50s/it] 15%|‚ñà‚ñå        | 51/335 [04:59<25:50,  5.46s/it] 16%|‚ñà‚ñå        | 52/335 [05:05<26:36,  5.64s/it] 16%|‚ñà‚ñå        | 53/335 [05:11<27:24,  5.83s/it] 16%|‚ñà‚ñå        | 54/335 [05:18<28:21,  6.05s/it] 16%|‚ñà‚ñã        | 55/335 [05:24<27:58,  6.00s/it] 17%|‚ñà‚ñã        | 56/335 [05:30<27:36,  5.94s/it] 17%|‚ñà‚ñã        | 57/335 [05:36<27:30,  5.94s/it] 17%|‚ñà‚ñã        | 58/335 [05:41<27:23,  5.93s/it] 18%|‚ñà‚ñä        | 59/335 [05:48<27:48,  6.04s/it] 18%|‚ñà‚ñä        | 60/335 [05:54<27:27,  5.99s/it] 18%|‚ñà‚ñä        | 61/335 [06:00<27:30,  6.02s/it] 19%|‚ñà‚ñä        | 62/335 [06:05<26:10,  5.75s/it] 19%|‚ñà‚ñâ        | 63/335 [06:11<26:15,  5.79s/it] 19%|‚ñà‚ñâ        | 64/335 [06:18<27:27,  6.08s/it] 19%|‚ñà‚ñâ        | 65/335 [06:23<26:48,  5.96s/it] 20%|‚ñà‚ñâ        | 66/335 [06:29<26:59,  6.02s/it] 20%|‚ñà‚ñà        | 67/335 [06:36<27:14,  6.10s/it][INFO|trainer.py:4117] 2025-01-06 20:18:05,627 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:18:05,627 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:18:05,627 >>   Batch size = 1

  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|‚ñé         | 2/60 [00:00<00:05,  9.87it/s][A
  5%|‚ñå         | 3/60 [00:00<00:10,  5.67it/s][A
  7%|‚ñã         | 4/60 [00:00<00:11,  4.94it/s][A
  8%|‚ñä         | 5/60 [00:00<00:11,  4.59it/s][A
 10%|‚ñà         | 6/60 [00:01<00:12,  4.16it/s][A
 12%|‚ñà‚ñè        | 7/60 [00:01<00:12,  4.40it/s][A
 13%|‚ñà‚ñé        | 8/60 [00:01<00:12,  4.11it/s][A
 15%|‚ñà‚ñå        | 9/60 [00:02<00:13,  3.70it/s][A
 17%|‚ñà‚ñã        | 10/60 [00:02<00:12,  4.08it/s][A
 18%|‚ñà‚ñä        | 11/60 [00:02<00:10,  4.47it/s][A
 20%|‚ñà‚ñà        | 12/60 [00:02<00:11,  4.29it/s][A
 22%|‚ñà‚ñà‚ñè       | 13/60 [00:02<00:10,  4.70it/s][A
 23%|‚ñà‚ñà‚ñé       | 14/60 [00:03<00:11,  4.09it/s][A
 25%|‚ñà‚ñà‚ñå       | 15/60 [00:03<00:11,  3.83it/s][A
 27%|‚ñà‚ñà‚ñã       | 16/60 [00:03<00:10,  4.36it/s][A
 28%|‚ñà‚ñà‚ñä       | 17/60 [00:03<00:09,  4.43it/s][A
 30%|‚ñà‚ñà‚ñà       | 18/60 [00:04<00:09,  4.42it/s][A
 32%|‚ñà‚ñà‚ñà‚ñè      | 19/60 [00:04<00:09,  4.40it/s][A
 33%|‚ñà‚ñà‚ñà‚ñé      | 20/60 [00:04<00:08,  4.88it/s][A
 35%|‚ñà‚ñà‚ñà‚ñå      | 21/60 [00:04<00:07,  4.93it/s][A
 37%|‚ñà‚ñà‚ñà‚ñã      | 22/60 [00:04<00:08,  4.56it/s][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 23/60 [00:05<00:08,  4.31it/s][A
 40%|‚ñà‚ñà‚ñà‚ñà      | 24/60 [00:05<00:07,  4.56it/s][A
 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 25/60 [00:05<00:08,  4.22it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 26/60 [00:05<00:08,  4.10it/s][A
 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 27/60 [00:06<00:08,  4.05it/s][A
 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 28/60 [00:06<00:08,  3.68it/s][A
 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 29/60 [00:06<00:07,  3.88it/s][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 30/60 [00:06<00:06,  4.38it/s][A
 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 31/60 [00:07<00:07,  3.74it/s][A
 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 32/60 [00:07<00:07,  3.86it/s][A
 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 33/60 [00:07<00:06,  4.06it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 34/60 [00:07<00:06,  4.18it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 35/60 [00:08<00:06,  4.13it/s][A
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 36/60 [00:08<00:06,  3.67it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 37/60 [00:08<00:05,  3.87it/s][A
 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 38/60 [00:08<00:05,  3.91it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 39/60 [00:09<00:05,  3.95it/s][A
 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 40/60 [00:09<00:04,  4.25it/s][A
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 41/60 [00:09<00:04,  4.22it/s][A
 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 42/60 [00:09<00:04,  4.16it/s][A
 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 43/60 [00:10<00:04,  4.19it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 44/60 [00:10<00:03,  4.34it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 45/60 [00:10<00:03,  4.31it/s][A
 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 46/60 [00:10<00:03,  4.19it/s][A
 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 47/60 [00:11<00:03,  3.90it/s][A
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 48/60 [00:11<00:02,  4.04it/s][A
 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 49/60 [00:11<00:02,  4.24it/s][A
 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 50/60 [00:11<00:02,  4.54it/s][A
 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 51/60 [00:12<00:02,  4.22it/s][A
 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/60 [00:12<00:02,  3.84it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 53/60 [00:12<00:01,  3.73it/s][A
 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 54/60 [00:12<00:01,  3.84it/s][A
 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 55/60 [00:13<00:01,  3.90it/s][A
 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 56/60 [00:13<00:01,  3.49it/s][A
 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 57/60 [00:13<00:00,  3.89it/s][A
 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 58/60 [00:13<00:00,  4.18it/s][A
 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 59/60 [00:14<00:00,  4.41it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.42it/s][A                                                
                                               [A{'eval_loss': 0.0028568485286086798, 'eval_runtime': 14.9863, 'eval_samples_per_second': 8.007, 'eval_steps_per_second': 4.004, 'epoch': 1.0}
 20%|‚ñà‚ñà        | 67/335 [06:52<27:14,  6.10s/it]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.42it/s][A
                                               [A 20%|‚ñà‚ñà        | 68/335 [06:56<46:17, 10.40s/it] 21%|‚ñà‚ñà        | 69/335 [07:02<40:30,  9.14s/it] 21%|‚ñà‚ñà        | 70/335 [07:08<35:51,  8.12s/it] 21%|‚ñà‚ñà        | 71/335 [07:14<33:02,  7.51s/it] 21%|‚ñà‚ñà‚ñè       | 72/335 [07:22<32:57,  7.52s/it] 22%|‚ñà‚ñà‚ñè       | 73/335 [07:27<30:30,  6.99s/it] 22%|‚ñà‚ñà‚ñè       | 74/335 [07:33<29:11,  6.71s/it] 22%|‚ñà‚ñà‚ñè       | 75/335 [07:39<28:10,  6.50s/it] 23%|‚ñà‚ñà‚ñé       | 76/335 [07:45<26:41,  6.19s/it] 23%|‚ñà‚ñà‚ñé       | 77/335 [07:52<27:07,  6.31s/it] 23%|‚ñà‚ñà‚ñé       | 78/335 [07:58<26:57,  6.29s/it] 24%|‚ñà‚ñà‚ñé       | 79/335 [08:03<24:55,  5.84s/it] 24%|‚ñà‚ñà‚ñç       | 80/335 [08:07<23:41,  5.57s/it] 24%|‚ñà‚ñà‚ñç       | 81/335 [08:13<24:03,  5.68s/it] 24%|‚ñà‚ñà‚ñç       | 82/335 [08:19<23:48,  5.65s/it] 25%|‚ñà‚ñà‚ñç       | 83/335 [08:26<25:01,  5.96s/it] 25%|‚ñà‚ñà‚ñå       | 84/335 [08:31<24:02,  5.75s/it] 25%|‚ñà‚ñà‚ñå       | 85/335 [08:37<23:45,  5.70s/it] 26%|‚ñà‚ñà‚ñå       | 86/335 [08:43<24:42,  5.95s/it] 26%|‚ñà‚ñà‚ñå       | 87/335 [08:48<23:42,  5.73s/it] 26%|‚ñà‚ñà‚ñã       | 88/335 [08:55<24:46,  6.02s/it] 27%|‚ñà‚ñà‚ñã       | 89/335 [09:00<23:46,  5.80s/it] 27%|‚ñà‚ñà‚ñã       | 90/335 [09:06<23:26,  5.74s/it] 27%|‚ñà‚ñà‚ñã       | 91/335 [09:12<23:16,  5.72s/it] 27%|‚ñà‚ñà‚ñã       | 92/335 [09:18<24:16,  5.99s/it] 28%|‚ñà‚ñà‚ñä       | 93/335 [09:24<23:23,  5.80s/it] 28%|‚ñà‚ñà‚ñä       | 94/335 [09:29<23:01,  5.73s/it] 28%|‚ñà‚ñà‚ñä       | 95/335 [09:35<23:24,  5.85s/it] 29%|‚ñà‚ñà‚ñä       | 96/335 [09:41<23:29,  5.90s/it] 29%|‚ñà‚ñà‚ñâ       | 97/335 [09:47<23:24,  5.90s/it] 29%|‚ñà‚ñà‚ñâ       | 98/335 [09:52<22:07,  5.60s/it] 30%|‚ñà‚ñà‚ñâ       | 99/335 [09:58<22:52,  5.81s/it] 30%|‚ñà‚ñà‚ñâ       | 100/335 [10:04<22:18,  5.69s/it] 30%|‚ñà‚ñà‚ñà       | 101/335 [10:10<22:30,  5.77s/it] 30%|‚ñà‚ñà‚ñà       | 102/335 [10:17<24:12,  6.23s/it] 31%|‚ñà‚ñà‚ñà       | 103/335 [10:24<24:30,  6.34s/it] 31%|‚ñà‚ñà‚ñà       | 104/335 [10:30<24:20,  6.32s/it] 31%|‚ñà‚ñà‚ñà‚ñè      | 105/335 [10:36<23:35,  6.15s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 106/335 [10:42<23:57,  6.28s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 107/335 [10:49<24:25,  6.43s/it] 32%|‚ñà‚ñà‚ñà‚ñè      | 108/335 [10:55<24:07,  6.38s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 109/335 [11:01<23:47,  6.32s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 110/335 [11:07<23:09,  6.18s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 111/335 [11:13<22:41,  6.08s/it] 33%|‚ñà‚ñà‚ñà‚ñé      | 112/335 [11:19<21:53,  5.89s/it] 34%|‚ñà‚ñà‚ñà‚ñé      | 113/335 [11:25<21:52,  5.91s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 114/335 [11:30<21:34,  5.86s/it] 34%|‚ñà‚ñà‚ñà‚ñç      | 115/335 [11:36<21:10,  5.77s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 116/335 [11:41<20:42,  5.67s/it] 35%|‚ñà‚ñà‚ñà‚ñç      | 117/335 [11:47<20:45,  5.71s/it] 35%|‚ñà‚ñà‚ñà‚ñå      | 118/335 [11:54<21:44,  6.01s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 119/335 [11:59<21:05,  5.86s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 120/335 [12:04<20:12,  5.64s/it] 36%|‚ñà‚ñà‚ñà‚ñå      | 121/335 [12:10<20:10,  5.66s/it] 36%|‚ñà‚ñà‚ñà‚ñã      | 122/335 [12:16<20:31,  5.78s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 123/335 [12:22<20:08,  5.70s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 124/335 [12:27<19:33,  5.56s/it] 37%|‚ñà‚ñà‚ñà‚ñã      | 125/335 [12:33<19:41,  5.62s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 126/335 [12:39<19:51,  5.70s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 127/335 [12:44<19:08,  5.52s/it] 38%|‚ñà‚ñà‚ñà‚ñä      | 128/335 [12:50<20:11,  5.85s/it] 39%|‚ñà‚ñà‚ñà‚ñä      | 129/335 [12:56<20:12,  5.89s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 130/335 [13:02<19:47,  5.79s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 131/335 [13:08<19:35,  5.76s/it] 39%|‚ñà‚ñà‚ñà‚ñâ      | 132/335 [13:14<20:00,  5.91s/it] 40%|‚ñà‚ñà‚ñà‚ñâ      | 133/335 [13:20<19:41,  5.85s/it] 40%|‚ñà‚ñà‚ñà‚ñà      | 134/335 [13:26<19:55,  5.95s/it][INFO|trainer.py:4117] 2025-01-06 20:24:57,076 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:24:57,076 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:24:57,076 >>   Batch size = 1

  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|‚ñé         | 2/60 [00:00<00:05,  9.87it/s][A
  5%|‚ñå         | 3/60 [00:00<00:10,  5.67it/s][A
  7%|‚ñã         | 4/60 [00:00<00:11,  4.94it/s][A
  8%|‚ñä         | 5/60 [00:00<00:11,  4.59it/s][A
 10%|‚ñà         | 6/60 [00:01<00:12,  4.17it/s][A
 12%|‚ñà‚ñè        | 7/60 [00:01<00:12,  4.40it/s][A
 13%|‚ñà‚ñé        | 8/60 [00:01<00:12,  4.11it/s][A
 15%|‚ñà‚ñå        | 9/60 [00:02<00:13,  3.70it/s][A
 17%|‚ñà‚ñã        | 10/60 [00:02<00:12,  4.08it/s][A
 18%|‚ñà‚ñä        | 11/60 [00:02<00:10,  4.47it/s][A
 20%|‚ñà‚ñà        | 12/60 [00:02<00:11,  4.29it/s][A
 22%|‚ñà‚ñà‚ñè       | 13/60 [00:02<00:10,  4.70it/s][A
 23%|‚ñà‚ñà‚ñé       | 14/60 [00:03<00:11,  4.09it/s][A
 25%|‚ñà‚ñà‚ñå       | 15/60 [00:03<00:11,  3.83it/s][A
 27%|‚ñà‚ñà‚ñã       | 16/60 [00:03<00:10,  4.36it/s][A
 28%|‚ñà‚ñà‚ñä       | 17/60 [00:03<00:09,  4.43it/s][A
 30%|‚ñà‚ñà‚ñà       | 18/60 [00:04<00:09,  4.42it/s][A
 32%|‚ñà‚ñà‚ñà‚ñè      | 19/60 [00:04<00:09,  4.41it/s][A
 33%|‚ñà‚ñà‚ñà‚ñé      | 20/60 [00:04<00:08,  4.88it/s][A
 35%|‚ñà‚ñà‚ñà‚ñå      | 21/60 [00:04<00:07,  4.93it/s][A
 37%|‚ñà‚ñà‚ñà‚ñã      | 22/60 [00:04<00:08,  4.56it/s][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 23/60 [00:05<00:08,  4.31it/s][A
 40%|‚ñà‚ñà‚ñà‚ñà      | 24/60 [00:05<00:07,  4.56it/s][A
 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 25/60 [00:05<00:08,  4.22it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 26/60 [00:05<00:08,  4.10it/s][A
 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 27/60 [00:06<00:08,  4.05it/s][A
 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 28/60 [00:06<00:08,  3.68it/s][A
 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 29/60 [00:06<00:07,  3.88it/s][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 30/60 [00:06<00:06,  4.38it/s][A
 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 31/60 [00:07<00:07,  3.74it/s][A
 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 32/60 [00:07<00:07,  3.87it/s][A
 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 33/60 [00:07<00:06,  4.06it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 34/60 [00:07<00:06,  4.18it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 35/60 [00:08<00:06,  4.13it/s][A
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 36/60 [00:08<00:06,  3.67it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 37/60 [00:08<00:05,  3.87it/s][A
 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 38/60 [00:08<00:05,  3.91it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 39/60 [00:09<00:05,  3.95it/s][A
 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 40/60 [00:09<00:04,  4.25it/s][A
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 41/60 [00:09<00:04,  4.22it/s][A
 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 42/60 [00:09<00:04,  4.16it/s][A
 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 43/60 [00:10<00:04,  4.19it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 44/60 [00:10<00:03,  4.34it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 45/60 [00:10<00:03,  4.32it/s][A
 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 46/60 [00:10<00:03,  4.19it/s][A
 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 47/60 [00:11<00:03,  3.90it/s][A
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 48/60 [00:11<00:02,  4.05it/s][A
 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 49/60 [00:11<00:02,  4.24it/s][A
 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 50/60 [00:11<00:02,  4.54it/s][A
 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 51/60 [00:12<00:02,  4.21it/s][A
 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/60 [00:12<00:02,  3.84it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 53/60 [00:12<00:01,  3.73it/s][A
 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 54/60 [00:12<00:01,  3.84it/s][A
 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 55/60 [00:13<00:01,  3.90it/s][A
 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 56/60 [00:13<00:01,  3.49it/s][A
 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 57/60 [00:13<00:00,  3.89it/s][A
 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 58/60 [00:13<00:00,  4.18it/s][A
 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 59/60 [00:14<00:00,  4.41it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.43it/s][A                                                 
                                               [A{'eval_loss': 0.0022934062872081995, 'eval_runtime': 15.143, 'eval_samples_per_second': 7.924, 'eval_steps_per_second': 3.962, 'epoch': 1.99}
 40%|‚ñà‚ñà‚ñà‚ñà      | 134/335 [13:43<19:55,  5.95s/it]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.43it/s][A
                                               [A 40%|‚ñà‚ñà‚ñà‚ñà      | 135/335 [13:46<34:40, 10.40s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 136/335 [13:52<29:30,  8.90s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 137/335 [13:58<26:15,  7.96s/it] 41%|‚ñà‚ñà‚ñà‚ñà      | 138/335 [14:04<24:23,  7.43s/it] 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 139/335 [14:10<22:50,  6.99s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 140/335 [14:15<21:25,  6.59s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 141/335 [14:21<19:55,  6.16s/it] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 142/335 [14:27<19:32,  6.07s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 143/335 [14:33<19:52,  6.21s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 144/335 [14:39<19:15,  6.05s/it] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 145/335 [14:45<19:24,  6.13s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 146/335 [14:50<18:29,  5.87s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 147/335 [14:57<18:52,  6.02s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 148/335 [15:02<18:05,  5.80s/it] 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 149/335 [15:08<17:56,  5.79s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 150/335 [15:14<17:56,  5.82s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 151/335 [15:20<18:15,  5.95s/it] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 152/335 [15:26<17:54,  5.87s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 153/335 [15:31<17:50,  5.88s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 154/335 [15:37<17:31,  5.81s/it] 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 155/335 [15:43<17:21,  5.79s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 156/335 [15:48<16:48,  5.63s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 157/335 [15:54<17:02,  5.75s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 158/335 [16:01<17:39,  5.98s/it] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 159/335 [16:06<17:24,  5.93s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 160/335 [16:12<17:01,  5.84s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 161/335 [16:18<16:48,  5.80s/it] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 162/335 [16:24<17:03,  5.92s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 163/335 [16:30<16:53,  5.89s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 164/335 [16:36<17:23,  6.10s/it] 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 165/335 [16:42<17:12,  6.07s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 166/335 [16:48<17:06,  6.08s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 167/335 [16:54<16:53,  6.03s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 168/335 [17:00<16:48,  6.04s/it] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 169/335 [17:06<16:24,  5.93s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 170/335 [17:13<16:53,  6.14s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 171/335 [17:19<16:39,  6.09s/it] 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 172/335 [17:24<16:08,  5.94s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 173/335 [17:30<15:31,  5.75s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 174/335 [17:36<15:40,  5.84s/it] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 175/335 [17:41<15:26,  5.79s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 176/335 [17:46<14:21,  5.42s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 177/335 [17:51<13:45,  5.22s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 178/335 [17:56<13:44,  5.25s/it] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 179/335 [18:03<15:07,  5.82s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 180/335 [18:10<15:34,  6.03s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 181/335 [18:15<15:14,  5.94s/it] 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 182/335 [18:21<15:13,  5.97s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 183/335 [18:27<14:50,  5.86s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 184/335 [18:34<15:22,  6.11s/it] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 185/335 [18:39<14:53,  5.96s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 186/335 [18:45<14:31,  5.85s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 187/335 [18:51<14:32,  5.89s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 188/335 [18:56<14:07,  5.76s/it] 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 189/335 [19:02<14:10,  5.82s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 190/335 [19:08<13:57,  5.77s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 191/335 [19:14<14:06,  5.88s/it] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 192/335 [19:20<14:17,  6.00s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 193/335 [19:27<14:45,  6.24s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 194/335 [19:33<14:33,  6.19s/it] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 195/335 [19:40<14:49,  6.35s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 196/335 [19:46<14:14,  6.15s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 197/335 [19:51<13:31,  5.88s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 198/335 [19:56<13:02,  5.71s/it] 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 199/335 [20:02<13:03,  5.76s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 200/335 [20:09<13:30,  6.00s/it]                                                 {'loss': 0.178, 'grad_norm': 0.05876597389578819, 'learning_rate': 4.194641844029227e-05, 'epoch': 2.97}
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 200/335 [20:09<13:30,  6.00s/it] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 201/335 [20:14<13:14,  5.93s/it][INFO|trainer.py:4117] 2025-01-06 20:31:47,894 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:31:47,894 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:31:47,894 >>   Batch size = 1

  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|‚ñé         | 2/60 [00:00<00:05,  9.88it/s][A
  5%|‚ñå         | 3/60 [00:00<00:10,  5.68it/s][A
  7%|‚ñã         | 4/60 [00:00<00:11,  4.95it/s][A
  8%|‚ñä         | 5/60 [00:00<00:11,  4.59it/s][A
 10%|‚ñà         | 6/60 [00:01<00:12,  4.17it/s][A
 12%|‚ñà‚ñè        | 7/60 [00:01<00:12,  4.40it/s][A
 13%|‚ñà‚ñé        | 8/60 [00:01<00:12,  4.12it/s][A
 15%|‚ñà‚ñå        | 9/60 [00:02<00:13,  3.70it/s][A
 17%|‚ñà‚ñã        | 10/60 [00:02<00:12,  4.09it/s][A
 18%|‚ñà‚ñä        | 11/60 [00:02<00:10,  4.47it/s][A
 20%|‚ñà‚ñà        | 12/60 [00:02<00:11,  4.29it/s][A
 22%|‚ñà‚ñà‚ñè       | 13/60 [00:02<00:09,  4.70it/s][A
 23%|‚ñà‚ñà‚ñé       | 14/60 [00:03<00:11,  4.09it/s][A
 25%|‚ñà‚ñà‚ñå       | 15/60 [00:03<00:11,  3.83it/s][A
 27%|‚ñà‚ñà‚ñã       | 16/60 [00:03<00:10,  4.37it/s][A
 28%|‚ñà‚ñà‚ñä       | 17/60 [00:03<00:09,  4.44it/s][A
 30%|‚ñà‚ñà‚ñà       | 18/60 [00:04<00:09,  4.42it/s][A
 32%|‚ñà‚ñà‚ñà‚ñè      | 19/60 [00:04<00:09,  4.41it/s][A
 33%|‚ñà‚ñà‚ñà‚ñé      | 20/60 [00:04<00:08,  4.89it/s][A
 35%|‚ñà‚ñà‚ñà‚ñå      | 21/60 [00:04<00:07,  4.94it/s][A
 37%|‚ñà‚ñà‚ñà‚ñã      | 22/60 [00:04<00:08,  4.56it/s][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 23/60 [00:05<00:08,  4.31it/s][A
 40%|‚ñà‚ñà‚ñà‚ñà      | 24/60 [00:05<00:07,  4.55it/s][A
 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 25/60 [00:05<00:08,  4.22it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 26/60 [00:05<00:08,  4.10it/s][A
 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 27/60 [00:06<00:08,  4.05it/s][A
 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 28/60 [00:06<00:08,  3.68it/s][A
 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 29/60 [00:06<00:08,  3.87it/s][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 30/60 [00:06<00:06,  4.38it/s][A
 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 31/60 [00:07<00:07,  3.74it/s][A
 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 32/60 [00:07<00:07,  3.86it/s][A
 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 33/60 [00:07<00:06,  4.06it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 34/60 [00:07<00:06,  4.18it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 35/60 [00:08<00:06,  4.13it/s][A
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 36/60 [00:08<00:06,  3.67it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 37/60 [00:08<00:05,  3.87it/s][A
 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 38/60 [00:08<00:05,  3.92it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 39/60 [00:09<00:05,  3.95it/s][A
 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 40/60 [00:09<00:04,  4.25it/s][A
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 41/60 [00:09<00:04,  4.22it/s][A
 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 42/60 [00:09<00:04,  4.16it/s][A
 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 43/60 [00:10<00:04,  4.19it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 44/60 [00:10<00:03,  4.34it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 45/60 [00:10<00:03,  4.31it/s][A
 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 46/60 [00:10<00:03,  4.19it/s][A
 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 47/60 [00:11<00:03,  3.90it/s][A
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 48/60 [00:11<00:02,  4.04it/s][A
 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 49/60 [00:11<00:02,  4.24it/s][A
 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 50/60 [00:11<00:02,  4.54it/s][A
 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 51/60 [00:12<00:02,  4.21it/s][A
 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/60 [00:12<00:02,  3.84it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 53/60 [00:12<00:01,  3.73it/s][A
 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 54/60 [00:12<00:01,  3.84it/s][A
 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 55/60 [00:13<00:01,  3.90it/s][A
 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 56/60 [00:13<00:01,  3.49it/s][A
 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 57/60 [00:13<00:00,  3.89it/s][A
 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 58/60 [00:13<00:00,  4.18it/s][A
 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 59/60 [00:14<00:00,  4.42it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.43it/s][A                                                 
                                               [A{'eval_loss': 0.002161339158192277, 'eval_runtime': 15.0163, 'eval_samples_per_second': 7.991, 'eval_steps_per_second': 3.996, 'epoch': 2.99}
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 201/335 [20:34<13:14,  5.93s/it]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.43it/s][A
                                               [A 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 202/335 [20:36<23:19, 10.52s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 203/335 [20:43<20:57,  9.53s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 204/335 [20:48<18:08,  8.31s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 205/335 [20:55<16:57,  7.83s/it] 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 206/335 [21:01<15:42,  7.31s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 207/335 [21:07<14:23,  6.75s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 208/335 [21:13<14:02,  6.63s/it] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 209/335 [21:18<13:08,  6.26s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 210/335 [21:24<12:43,  6.11s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 211/335 [21:30<12:30,  6.05s/it] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 212/335 [21:36<12:18,  6.01s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 213/335 [21:41<11:54,  5.86s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 214/335 [21:47<11:41,  5.80s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 215/335 [21:54<11:58,  5.99s/it] 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 216/335 [22:00<11:52,  5.98s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 217/335 [22:05<11:38,  5.92s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 218/335 [22:11<11:37,  5.96s/it] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 219/335 [22:17<11:35,  5.99s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 220/335 [22:23<11:04,  5.78s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 221/335 [22:28<10:36,  5.58s/it] 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 222/335 [22:34<10:59,  5.84s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 223/335 [22:40<10:59,  5.89s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 224/335 [22:46<10:37,  5.74s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 225/335 [22:52<10:39,  5.81s/it] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 226/335 [22:59<11:08,  6.14s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 227/335 [23:04<10:46,  5.99s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 228/335 [23:11<10:57,  6.14s/it] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 229/335 [23:17<10:42,  6.06s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 230/335 [23:22<10:02,  5.74s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 231/335 [23:27<09:55,  5.73s/it] 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 232/335 [23:33<09:46,  5.69s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 233/335 [23:39<09:40,  5.69s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 234/335 [23:44<09:31,  5.66s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 235/335 [23:50<09:36,  5.77s/it] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 236/335 [23:56<09:44,  5.91s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 237/335 [24:02<09:25,  5.77s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 238/335 [24:08<09:21,  5.79s/it] 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 239/335 [24:14<09:28,  5.93s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 240/335 [24:20<09:38,  6.09s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 241/335 [24:26<09:14,  5.90s/it] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 242/335 [24:31<08:47,  5.67s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 243/335 [24:37<08:40,  5.66s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 244/335 [24:42<08:17,  5.47s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 245/335 [24:48<08:29,  5.66s/it] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 246/335 [24:53<08:22,  5.64s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 247/335 [25:00<08:31,  5.81s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 248/335 [25:06<08:42,  6.01s/it] 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 249/335 [25:11<08:20,  5.82s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 250/335 [25:17<08:13,  5.81s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 251/335 [25:24<08:26,  6.02s/it] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 252/335 [25:30<08:39,  6.25s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 253/335 [25:36<08:20,  6.10s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 254/335 [25:42<08:16,  6.13s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 255/335 [25:49<08:25,  6.32s/it] 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 256/335 [25:55<08:09,  6.20s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 257/335 [26:01<08:00,  6.15s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 258/335 [26:07<07:41,  6.00s/it] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 259/335 [26:13<07:42,  6.09s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 260/335 [26:19<07:25,  5.94s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 261/335 [26:24<07:09,  5.80s/it] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 262/335 [26:30<07:01,  5.77s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 263/335 [26:36<06:53,  5.75s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 264/335 [26:41<06:46,  5.72s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 265/335 [26:47<06:51,  5.88s/it] 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 266/335 [26:53<06:44,  5.87s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 267/335 [27:00<06:49,  6.02s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 268/335 [27:05<06:27,  5.78s/it] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 269/335 [27:11<06:34,  5.98s/it][INFO|trainer.py:4117] 2025-01-06 20:38:40,238 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:38:40,238 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:38:40,238 >>   Batch size = 1

  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|‚ñé         | 2/60 [00:00<00:05,  9.83it/s][A
  5%|‚ñå         | 3/60 [00:00<00:10,  5.66it/s][A
  7%|‚ñã         | 4/60 [00:00<00:11,  4.94it/s][A
  8%|‚ñä         | 5/60 [00:00<00:11,  4.59it/s][A
 10%|‚ñà         | 6/60 [00:01<00:12,  4.17it/s][A
 12%|‚ñà‚ñè        | 7/60 [00:01<00:12,  4.40it/s][A
 13%|‚ñà‚ñé        | 8/60 [00:01<00:12,  4.11it/s][A
 15%|‚ñà‚ñå        | 9/60 [00:02<00:13,  3.70it/s][A
 17%|‚ñà‚ñã        | 10/60 [00:02<00:12,  4.09it/s][A
 18%|‚ñà‚ñä        | 11/60 [00:02<00:10,  4.48it/s][A
 20%|‚ñà‚ñà        | 12/60 [00:02<00:11,  4.29it/s][A
 22%|‚ñà‚ñà‚ñè       | 13/60 [00:02<00:09,  4.70it/s][A
 23%|‚ñà‚ñà‚ñé       | 14/60 [00:03<00:11,  4.09it/s][A
 25%|‚ñà‚ñà‚ñå       | 15/60 [00:03<00:11,  3.83it/s][A
 27%|‚ñà‚ñà‚ñã       | 16/60 [00:03<00:10,  4.37it/s][A
 28%|‚ñà‚ñà‚ñä       | 17/60 [00:03<00:09,  4.43it/s][A
 30%|‚ñà‚ñà‚ñà       | 18/60 [00:04<00:09,  4.42it/s][A
 32%|‚ñà‚ñà‚ñà‚ñè      | 19/60 [00:04<00:09,  4.41it/s][A
 33%|‚ñà‚ñà‚ñà‚ñé      | 20/60 [00:04<00:08,  4.89it/s][A
 35%|‚ñà‚ñà‚ñà‚ñå      | 21/60 [00:04<00:07,  4.94it/s][A
 37%|‚ñà‚ñà‚ñà‚ñã      | 22/60 [00:04<00:08,  4.56it/s][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 23/60 [00:05<00:08,  4.31it/s][A
 40%|‚ñà‚ñà‚ñà‚ñà      | 24/60 [00:05<00:07,  4.56it/s][A
 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 25/60 [00:05<00:08,  4.22it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 26/60 [00:05<00:08,  4.10it/s][A
 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 27/60 [00:06<00:08,  4.05it/s][A
 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 28/60 [00:06<00:08,  3.68it/s][A
 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 29/60 [00:06<00:07,  3.88it/s][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 30/60 [00:06<00:06,  4.38it/s][A
 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 31/60 [00:07<00:07,  3.74it/s][A
 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 32/60 [00:07<00:07,  3.86it/s][A
 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 33/60 [00:07<00:06,  4.06it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 34/60 [00:07<00:06,  4.18it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 35/60 [00:08<00:06,  4.13it/s][A
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 36/60 [00:08<00:06,  3.67it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 37/60 [00:08<00:05,  3.87it/s][A
 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 38/60 [00:08<00:05,  3.92it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 39/60 [00:09<00:05,  3.95it/s][A
 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 40/60 [00:09<00:04,  4.25it/s][A
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 41/60 [00:09<00:04,  4.22it/s][A
 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 42/60 [00:09<00:04,  4.16it/s][A
 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 43/60 [00:10<00:04,  4.19it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 44/60 [00:10<00:03,  4.34it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 45/60 [00:10<00:03,  4.32it/s][A
 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 46/60 [00:10<00:03,  4.19it/s][A
 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 47/60 [00:11<00:03,  3.90it/s][A
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 48/60 [00:11<00:02,  4.04it/s][A
 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 49/60 [00:11<00:02,  4.24it/s][A
 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 50/60 [00:11<00:02,  4.54it/s][A
 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 51/60 [00:12<00:02,  4.21it/s][A
 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/60 [00:12<00:02,  3.84it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 53/60 [00:12<00:01,  3.72it/s][A
 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 54/60 [00:12<00:01,  3.84it/s][A
 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 55/60 [00:13<00:01,  3.90it/s][A
 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 56/60 [00:13<00:01,  3.49it/s][A
 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 57/60 [00:13<00:00,  3.89it/s][A
 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 58/60 [00:13<00:00,  4.18it/s][A
 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 59/60 [00:14<00:00,  4.41it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.43it/s][A                                                 
                                               [A{'eval_loss': 0.002379727317020297, 'eval_runtime': 14.5383, 'eval_samples_per_second': 8.254, 'eval_steps_per_second': 4.127, 'epoch': 4.0}
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 269/335 [27:26<06:34,  5.98s/it]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.43it/s][A
                                               [A 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 270/335 [27:32<11:18, 10.43s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 271/335 [27:38<09:47,  9.18s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 272/335 [27:45<08:44,  8.32s/it] 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 273/335 [27:51<07:50,  7.59s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 274/335 [27:57<07:26,  7.32s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 275/335 [28:03<06:47,  6.78s/it] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 276/335 [28:08<06:18,  6.41s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 277/335 [28:14<05:53,  6.10s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 278/335 [28:20<05:42,  6.01s/it] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 279/335 [28:26<05:42,  6.11s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 280/335 [28:32<05:27,  5.96s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 281/335 [28:38<05:24,  6.00s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 282/335 [28:43<05:10,  5.85s/it] 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 283/335 [28:49<05:03,  5.83s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 284/335 [28:56<05:11,  6.10s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 285/335 [29:02<05:02,  6.05s/it] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 286/335 [29:08<04:56,  6.04s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 287/335 [29:14<04:54,  6.13s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 288/335 [29:20<04:51,  6.21s/it] 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 289/335 [29:26<04:39,  6.08s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 290/335 [29:32<04:24,  5.88s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 291/335 [29:38<04:30,  6.15s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 292/335 [29:44<04:18,  6.02s/it] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 293/335 [29:50<04:07,  5.90s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 294/335 [29:55<03:54,  5.71s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 295/335 [30:00<03:37,  5.43s/it] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 296/335 [30:06<03:39,  5.64s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 297/335 [30:11<03:27,  5.47s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 298/335 [30:16<03:19,  5.39s/it] 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 299/335 [30:21<03:13,  5.36s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 300/335 [30:27<03:12,  5.49s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 301/335 [30:34<03:16,  5.77s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 302/335 [30:39<03:08,  5.72s/it] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 303/335 [30:45<03:01,  5.66s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 304/335 [30:50<02:56,  5.69s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 305/335 [30:56<02:46,  5.56s/it] 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 306/335 [31:03<02:52,  5.96s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 307/335 [31:09<02:50,  6.11s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 308/335 [31:15<02:41,  5.99s/it] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 309/335 [31:21<02:35,  5.98s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 310/335 [31:26<02:20,  5.64s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 311/335 [31:31<02:11,  5.49s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 312/335 [31:37<02:09,  5.64s/it] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 313/335 [31:42<02:04,  5.64s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 314/335 [31:49<02:02,  5.84s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 315/335 [31:55<01:57,  5.88s/it] 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 316/335 [32:00<01:48,  5.72s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 317/335 [32:06<01:44,  5.83s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 318/335 [32:11<01:35,  5.61s/it] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 319/335 [32:17<01:32,  5.77s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 320/335 [32:23<01:26,  5.79s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 321/335 [32:29<01:22,  5.86s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 322/335 [32:34<01:13,  5.65s/it] 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 323/335 [32:40<01:08,  5.68s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 324/335 [32:45<01:01,  5.58s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 325/335 [32:51<00:55,  5.57s/it] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 326/335 [32:57<00:50,  5.66s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 327/335 [33:03<00:45,  5.69s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 328/335 [33:09<00:41,  5.87s/it] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 329/335 [33:15<00:35,  5.95s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 330/335 [33:22<00:30,  6.10s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 331/335 [33:27<00:24,  6.06s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 332/335 [33:34<00:18,  6.25s/it] 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 333/335 [33:40<00:12,  6.27s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 334/335 [33:46<00:06,  6.09s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 335/335 [33:52<00:00,  6.04s/it][INFO|trainer.py:3801] 2025-01-06 20:45:20,961 >> Saving model checkpoint to saves/qwen-14b-hi-e5/lora/sft/checkpoint-335
[INFO|configuration_utils.py:677] 2025-01-06 20:45:20,981 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-06 20:45:20,982 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2025-01-06 20:45:21,106 >> tokenizer config file saved in saves/qwen-14b-hi-e5/lora/sft/checkpoint-335/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-06 20:45:21,106 >> Special tokens file saved in saves/qwen-14b-hi-e5/lora/sft/checkpoint-335/special_tokens_map.json
[INFO|trainer.py:4117] 2025-01-06 20:45:21,346 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:45:21,346 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:45:21,346 >>   Batch size = 1

  0%|          | 0/60 [00:00<?, ?it/s][A
  3%|‚ñé         | 2/60 [00:00<00:05,  9.86it/s][A
  5%|‚ñå         | 3/60 [00:00<00:10,  5.67it/s][A
  7%|‚ñã         | 4/60 [00:00<00:11,  4.94it/s][A
  8%|‚ñä         | 5/60 [00:00<00:11,  4.59it/s][A
 10%|‚ñà         | 6/60 [00:01<00:12,  4.17it/s][A
 12%|‚ñà‚ñè        | 7/60 [00:01<00:12,  4.40it/s][A
 13%|‚ñà‚ñé        | 8/60 [00:01<00:12,  4.11it/s][A
 15%|‚ñà‚ñå        | 9/60 [00:02<00:13,  3.70it/s][A
 17%|‚ñà‚ñã        | 10/60 [00:02<00:12,  4.09it/s][A
 18%|‚ñà‚ñä        | 11/60 [00:02<00:10,  4.48it/s][A
 20%|‚ñà‚ñà        | 12/60 [00:02<00:11,  4.29it/s][A
 22%|‚ñà‚ñà‚ñè       | 13/60 [00:02<00:09,  4.71it/s][A
 23%|‚ñà‚ñà‚ñé       | 14/60 [00:03<00:11,  4.09it/s][A
 25%|‚ñà‚ñà‚ñå       | 15/60 [00:03<00:11,  3.83it/s][A
 27%|‚ñà‚ñà‚ñã       | 16/60 [00:03<00:10,  4.37it/s][A
 28%|‚ñà‚ñà‚ñä       | 17/60 [00:03<00:09,  4.43it/s][A
 30%|‚ñà‚ñà‚ñà       | 18/60 [00:04<00:09,  4.42it/s][A
 32%|‚ñà‚ñà‚ñà‚ñè      | 19/60 [00:04<00:09,  4.41it/s][A
 33%|‚ñà‚ñà‚ñà‚ñé      | 20/60 [00:04<00:08,  4.88it/s][A
 35%|‚ñà‚ñà‚ñà‚ñå      | 21/60 [00:04<00:07,  4.93it/s][A
 37%|‚ñà‚ñà‚ñà‚ñã      | 22/60 [00:04<00:08,  4.56it/s][A
 38%|‚ñà‚ñà‚ñà‚ñä      | 23/60 [00:05<00:08,  4.31it/s][A
 40%|‚ñà‚ñà‚ñà‚ñà      | 24/60 [00:05<00:07,  4.55it/s][A
 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 25/60 [00:05<00:08,  4.22it/s][A
 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 26/60 [00:05<00:08,  4.10it/s][A
 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 27/60 [00:06<00:08,  4.05it/s][A
 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 28/60 [00:06<00:08,  3.68it/s][A
 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 29/60 [00:06<00:07,  3.88it/s][A
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 30/60 [00:06<00:06,  4.38it/s][A
 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 31/60 [00:07<00:07,  3.74it/s][A
 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 32/60 [00:07<00:07,  3.86it/s][A
 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 33/60 [00:07<00:06,  4.06it/s][A
 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 34/60 [00:07<00:06,  4.18it/s][A
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 35/60 [00:08<00:06,  4.13it/s][A
 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 36/60 [00:08<00:06,  3.67it/s][A
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 37/60 [00:08<00:05,  3.87it/s][A
 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 38/60 [00:08<00:05,  3.91it/s][A
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 39/60 [00:09<00:05,  3.95it/s][A
 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 40/60 [00:09<00:04,  4.25it/s][A
 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 41/60 [00:09<00:04,  4.22it/s][A
 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 42/60 [00:09<00:04,  4.16it/s][A
 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 43/60 [00:10<00:04,  4.19it/s][A
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 44/60 [00:10<00:03,  4.34it/s][A
 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 45/60 [00:10<00:03,  4.32it/s][A
 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 46/60 [00:10<00:03,  4.19it/s][A
 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 47/60 [00:11<00:03,  3.90it/s][A
 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 48/60 [00:11<00:02,  4.04it/s][A
 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 49/60 [00:11<00:02,  4.24it/s][A
 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 50/60 [00:11<00:02,  4.53it/s][A
 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 51/60 [00:12<00:02,  4.21it/s][A
 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/60 [00:12<00:02,  3.84it/s][A
 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 53/60 [00:12<00:01,  3.73it/s][A
 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 54/60 [00:12<00:01,  3.84it/s][A
 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 55/60 [00:13<00:01,  3.90it/s][A
 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 56/60 [00:13<00:01,  3.49it/s][A
 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 57/60 [00:13<00:00,  3.89it/s][A
 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 58/60 [00:13<00:00,  4.18it/s][A
 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 59/60 [00:14<00:00,  4.42it/s][A
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.43it/s][A                                                 
                                               [A{'eval_loss': 0.002399302087724209, 'eval_runtime': 14.5403, 'eval_samples_per_second': 8.253, 'eval_steps_per_second': 4.126, 'epoch': 4.98}
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 335/335 [34:07<00:00,  6.04s/it]
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.43it/s][A
                                               [A[INFO|trainer.py:2584] 2025-01-06 20:45:35,886 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                 {'train_runtime': 2049.8609, 'train_samples_per_second': 2.622, 'train_steps_per_second': 0.163, 'train_loss': 0.10667623343752392, 'epoch': 4.98}
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 335/335 [34:07<00:00,  6.04s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 335/335 [34:07<00:00,  6.11s/it]
[INFO|trainer.py:3801] 2025-01-06 20:45:35,887 >> Saving model checkpoint to saves/qwen-14b-hi-e5/lora/sft
[INFO|configuration_utils.py:677] 2025-01-06 20:45:35,906 >> loading configuration file /mnt/sda/zzh/Qwen2.5-14B-Instruct/config.json
[INFO|configuration_utils.py:746] 2025-01-06 20:45:35,907 >> Model config Qwen2Config {
  "architectures": [
    "Qwen2ForCausalLM"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 5120,
  "initializer_range": 0.02,
  "intermediate_size": 13824,
  "max_position_embeddings": 32768,
  "max_window_layers": 70,
  "model_type": "qwen2",
  "num_attention_heads": 40,
  "num_hidden_layers": 48,
  "num_key_value_heads": 8,
  "rms_norm_eps": 1e-06,
  "rope_scaling": null,
  "rope_theta": 1000000.0,
  "sliding_window": null,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.46.1",
  "use_cache": true,
  "use_sliding_window": false,
  "vocab_size": 152064
}

[INFO|tokenization_utils_base.py:2646] 2025-01-06 20:45:36,029 >> tokenizer config file saved in saves/qwen-14b-hi-e5/lora/sft/tokenizer_config.json
[INFO|tokenization_utils_base.py:2655] 2025-01-06 20:45:36,030 >> Special tokens file saved in saves/qwen-14b-hi-e5/lora/sft/special_tokens_map.json
***** train metrics *****
  epoch                    =      4.9814
  total_flos               = 291033312GF
  train_loss               =      0.1067
  train_runtime            =  0:34:09.86
  train_samples_per_second =       2.622
  train_steps_per_second   =       0.163
Figure saved at: saves/qwen-14b-hi-e5/lora/sft/training_loss.png
Figure saved at: saves/qwen-14b-hi-e5/lora/sft/training_eval_loss.png
[WARNING|2025-01-06 20:45:36] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.
[INFO|trainer.py:4117] 2025-01-06 20:45:36,177 >> 
***** Running Evaluation *****
[INFO|trainer.py:4119] 2025-01-06 20:45:36,177 >>   Num examples = 120
[INFO|trainer.py:4122] 2025-01-06 20:45:36,178 >>   Batch size = 1
  0%|          | 0/60 [00:00<?, ?it/s]  3%|‚ñé         | 2/60 [00:00<00:05,  9.85it/s]  5%|‚ñå         | 3/60 [00:00<00:10,  5.67it/s]  7%|‚ñã         | 4/60 [00:00<00:11,  4.94it/s]  8%|‚ñä         | 5/60 [00:00<00:11,  4.59it/s] 10%|‚ñà         | 6/60 [00:01<00:12,  4.17it/s] 12%|‚ñà‚ñè        | 7/60 [00:01<00:12,  4.40it/s] 13%|‚ñà‚ñé        | 8/60 [00:01<00:12,  4.12it/s] 15%|‚ñà‚ñå        | 9/60 [00:02<00:13,  3.70it/s] 17%|‚ñà‚ñã        | 10/60 [00:02<00:12,  4.09it/s] 18%|‚ñà‚ñä        | 11/60 [00:02<00:10,  4.48it/s] 20%|‚ñà‚ñà        | 12/60 [00:02<00:11,  4.29it/s] 22%|‚ñà‚ñà‚ñè       | 13/60 [00:02<00:09,  4.71it/s] 23%|‚ñà‚ñà‚ñé       | 14/60 [00:03<00:11,  4.09it/s] 25%|‚ñà‚ñà‚ñå       | 15/60 [00:03<00:11,  3.83it/s] 27%|‚ñà‚ñà‚ñã       | 16/60 [00:03<00:10,  4.37it/s] 28%|‚ñà‚ñà‚ñä       | 17/60 [00:03<00:09,  4.43it/s] 30%|‚ñà‚ñà‚ñà       | 18/60 [00:04<00:09,  4.42it/s] 32%|‚ñà‚ñà‚ñà‚ñè      | 19/60 [00:04<00:09,  4.41it/s] 33%|‚ñà‚ñà‚ñà‚ñé      | 20/60 [00:04<00:08,  4.88it/s] 35%|‚ñà‚ñà‚ñà‚ñå      | 21/60 [00:04<00:07,  4.93it/s] 37%|‚ñà‚ñà‚ñà‚ñã      | 22/60 [00:04<00:08,  4.56it/s] 38%|‚ñà‚ñà‚ñà‚ñä      | 23/60 [00:05<00:08,  4.31it/s] 40%|‚ñà‚ñà‚ñà‚ñà      | 24/60 [00:05<00:07,  4.55it/s] 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 25/60 [00:05<00:08,  4.22it/s] 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 26/60 [00:05<00:08,  4.10it/s] 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 27/60 [00:06<00:08,  4.05it/s] 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 28/60 [00:06<00:08,  3.68it/s] 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 29/60 [00:06<00:07,  3.88it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 30/60 [00:06<00:06,  4.38it/s] 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 31/60 [00:07<00:07,  3.74it/s] 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 32/60 [00:07<00:07,  3.87it/s] 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 33/60 [00:07<00:06,  4.06it/s] 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 34/60 [00:07<00:06,  4.18it/s] 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 35/60 [00:08<00:06,  4.13it/s] 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 36/60 [00:08<00:06,  3.67it/s] 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 37/60 [00:08<00:05,  3.87it/s] 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 38/60 [00:08<00:05,  3.92it/s] 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 39/60 [00:09<00:05,  3.96it/s] 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 40/60 [00:09<00:04,  4.25it/s] 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 41/60 [00:09<00:04,  4.22it/s] 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 42/60 [00:09<00:04,  4.16it/s] 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 43/60 [00:10<00:04,  4.19it/s] 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 44/60 [00:10<00:03,  4.34it/s] 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 45/60 [00:10<00:03,  4.32it/s] 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 46/60 [00:10<00:03,  4.19it/s] 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 47/60 [00:11<00:03,  3.90it/s] 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 48/60 [00:11<00:02,  4.05it/s] 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 49/60 [00:11<00:02,  4.24it/s] 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 50/60 [00:11<00:02,  4.54it/s] 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 51/60 [00:12<00:02,  4.21it/s] 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 52/60 [00:12<00:02,  3.84it/s] 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 53/60 [00:12<00:01,  3.73it/s] 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 54/60 [00:12<00:01,  3.85it/s] 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 55/60 [00:13<00:01,  3.90it/s] 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 56/60 [00:13<00:01,  3.49it/s] 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 57/60 [00:13<00:00,  3.89it/s] 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 58/60 [00:13<00:00,  4.18it/s] 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 59/60 [00:14<00:00,  4.42it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.43it/s]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:14<00:00,  4.20it/s]
***** eval metrics *****
  epoch                   =     4.9814
  eval_loss               =     0.0024
  eval_runtime            = 0:00:14.53
  eval_samples_per_second =      8.256
  eval_steps_per_second   =      4.128
[INFO|modelcard.py:449] 2025-01-06 20:45:50,713 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}
[rank0]:[W106 20:45:51.263996954 ProcessGroupNCCL.cpp:1250] Warning: WARNING: process group has NOT been destroyed before we destruct ProcessGroupNCCL. On normal program exit, the application should call destroy_process_group to ensure that any pending NCCL operations have finished in this process. In rare cases this process can exit before this point and block the progress of another member of the process group. This constraint has always been present,  but this warning has only been added since PyTorch 2.4 (function operator())
